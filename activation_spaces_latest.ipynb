{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "device: cpu\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.optimizers import DARTSOptimizer\n",
    "from naslib.search_spaces import DartsSearchSpace\n",
    "from naslib.utils import utils, setup_logger, get_config_from_args, set_seed, log_args\n",
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from fvcore.common.config import CfgNode\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from naslib.search_spaces.core.primitives import AbstractPrimitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config_from_args(config_type='nas')\n",
    "config.optimizer = 'darts'\n",
    "config.search.batch_size = 32\n",
    "utils.set_seed(config.seed)\n",
    "clear_output(wait=True)\n",
    "utils.log_args(config)\n",
    "config.search.epochs=10\n",
    "config.search.learning_rate=0.001\n",
    "logger = setup_logger(config.save + '/log.log')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power(AbstractPrimitive):\n",
    "    def __init__(self,power):\n",
    "        super().__init__(locals())\n",
    "        self.power=power\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.pow(x,self.power)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sqrt(AbstractPrimitive):\n",
    "    def __init__(self,eps=1e-10):\n",
    "        super().__init__(locals())\n",
    "        self.eps = eps\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.pow(torch.maximum(x,torch.tensor(self.eps).repeat(x.shape).cuda()),.5)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sin(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.sin(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Cos(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.cos(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Abs_op(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.abs(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sign(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return x*-1\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Beta_mul(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return x * self.beta\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Beta_add(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return x + self.beta\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Log(AbstractPrimitive):\n",
    "    def __init__(self,eps=1e-10):\n",
    "        super().__init__(locals())\n",
    "        self.eps = eps\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.log(torch.maximum(x,torch.tensor(self.eps).repeat(x.shape).cuda()))\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Exp(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        x=torch.exp(x)\n",
    "        return x\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sinh(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.sinh(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Cosh(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.cosh(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Tanh(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.tanh(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Asinh(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.asinh(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Atan(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.atan(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sinc(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.sinc(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Maximum0(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.maximum(x,torch.zeros(x.shape).cuda())\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Minimum0(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.minimum(x,torch.zeros(x.shape).cuda())\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Sigmoid(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.sigmoid(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class LogExp(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        x=torch.log(1+torch.exp(x))\n",
    "        return x\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Exp2(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.exp(-torch.pow(x,2))\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Erf(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.erf(x)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Beta(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return self.beta\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Add(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.add(x[0],x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Sub(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.sub(x[0],x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Mul(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.mul(x[0],x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Div(AbstractPrimitive):\n",
    "    def __init__(self,eps=1e-10):\n",
    "        super().__init__(locals())\n",
    "        self.eps=eps\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.div(x[0],x[1] + self.eps)\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Maximum(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.maximum(x[0],x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "class Minimum(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.minimum(x[0],x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class SigMul(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.mul(torch.sigmoid(x[0]),x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class ExpBetaSub2(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.exp(-self.beta*torch.pow(torch.sub(x[0],x[1]),2))\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class ExpBetaSubAbs(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.exp(-self.beta*torch.abs(torch.sub(x[0],x[1])))\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class BetaMix(AbstractPrimitive):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__(locals())\n",
    "        self.beta = torch.nn.Parameter(torch.ones(channels))\n",
    "    def forward(self,x, edge_data=None):\n",
    "        x=x.clamp(-10,10)\n",
    "        return torch.add(-self.beta*x[0],(1-self.beta)*x[1])\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "class Stack():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, tensors, edges_data=None):\n",
    "        return torch.stack(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        stages = ['a_stage_1', 'a_stage_2']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2) # intermediate node\n",
    "        activation_cell.add_node(3) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())]) # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable output edge\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        for i, scope in zip(range(3, 5), stages):\n",
    "            self.add_node(i, subgraph=deepcopy(activation_cell).set_scope(scope).set_input([i-1])) # activation cell i\n",
    "            self.nodes[i]['subgraph'].name = scope\n",
    "        self.add_node(5) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 5)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[4, 5].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "    def _set_ops(self, edge):\n",
    "        edge.data.set('op', [\n",
    "            ops.Sequential(nn.ReLU()),\n",
    "            ops.Sequential(nn.Hardswish()),\n",
    "            ops.Sequential(nn.LeakyReLU()),\n",
    "            ops.Sequential(nn.Identity())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'u_stage_1',\n",
    "        'u_stage_2',\n",
    "        'b_stage_1'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        u_stages = ['u_stage_1', 'u_stage_2']\n",
    "        \n",
    "        # unary cell definition\n",
    "        unary_cell = Graph()\n",
    "        unary_cell.name = 'u_cell'\n",
    "        unary_cell.add_node(1) # input node\n",
    "        unary_cell.add_node(2) # intermediate node\n",
    "        unary_cell.add_node(3) # output node\n",
    "        unary_cell.add_edges_from([(1, 2, EdgeData())]) # mutable edge\n",
    "        unary_cell.edges[1, 2].set('cell_name', 'u_cell')\n",
    "        unary_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable edge\n",
    "        \n",
    "        # binary cell definition\n",
    "        binary_cell = Graph()\n",
    "        binary_cell.name = 'b_cell'\n",
    "        binary_cell.add_node(1) # input node\n",
    "        binary_cell.add_node(2) # input node\n",
    "        binary_cell.add_node(3) # concatination node\n",
    "        binary_cell.nodes[3]['comb_op'] = stack()\n",
    "        binary_cell.add_node(4) # intermediate node\n",
    "        binary_cell.add_node(5) # output node\n",
    "        binary_cell.add_edges_from([(3, 4, EdgeData())]) # mutable edge\n",
    "        binary_cell.edges[3, 4].set('cell_name', 'b_cell') \n",
    "        binary_cell.add_edges_from([(1, 3, EdgeData().finalize()),\n",
    "                                    (2, 3, EdgeData().finalize()),\n",
    "                                    (4, 5, EdgeData().finalize())]) # immutable edges\n",
    "        \n",
    "        # activation cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'a_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2, subgraph=deepcopy(unary_cell).set_scope('u_stage_1').set_input([1])) # unary node\n",
    "        activation_cell.nodes[2]['subgraph'].name = 'u_stage_1'\n",
    "        activation_cell.add_node(3, subgraph=deepcopy(unary_cell).set_scope('u_stage_2').set_input([1])) # unary node\n",
    "        activation_cell.nodes[3]['subgraph'].name = 'u_stage_2'\n",
    "        activation_cell.add_node(4, subgraph=deepcopy(binary_cell).set_scope('b_stage_1').set_input([2, 3])) # binary node\n",
    "        activation_cell.nodes[4]['subgraph'].name = 'b_stage_1'\n",
    "        activation_cell.add_node(5) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData().finalize()), \n",
    "                                        (1, 3, EdgeData().finalize()),\n",
    "                                        (2, 4, EdgeData().finalize()),\n",
    "                                        (3, 4, EdgeData().finalize()), \n",
    "                                        (4, 5, EdgeData().finalize())])\n",
    "        \n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3, subgraph=deepcopy(activation_cell).set_input([2])) # activation cell\n",
    "        self.nodes[3]['subgraph'].name = 'a_stage_1'\n",
    "        self.add_node(4) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 4)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[3, 4].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in u_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_unary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            ) # set unary cell ops\n",
    "        \n",
    "        self.update_edges(\n",
    "            update_func=lambda edge: self._set_binary_ops(edge),\n",
    "            scope='b_stage_1',\n",
    "            private_edge_data=True\n",
    "        ) # set binary cell ops\n",
    "        \n",
    "\n",
    "    def _set_unary_ops(self, edge):\n",
    "        edge.data.set('op', [ops.Identity(), ops.Zero(stride=1)]) \n",
    "        \n",
    "        \n",
    "    def _set_binary_ops(self, edge):\n",
    "        edge.data.set('op', [Minimum(), Maximum()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'u_stage_1',\n",
    "        'u_stage_2',\n",
    "        'u_stage_3',\n",
    "        'u_stage_4',\n",
    "        'b_stage_1',\n",
    "        'b_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        u_stages = ['u_stage_1', 'u_stage_2', 'u_stage_3', 'u_stage_4']\n",
    "        b_stages = ['b_stage_1', 'b_stage_2']\n",
    "        \n",
    "        # unary cell definition\n",
    "        unary_cell = Graph()\n",
    "        unary_cell.name = 'u_cell'\n",
    "        unary_cell.add_node(1) # input node\n",
    "        unary_cell.add_node(2) # intermediate node\n",
    "        unary_cell.add_node(3) # output node\n",
    "        unary_cell.add_edges_from([(1, 2, EdgeData())]) # mutable edge\n",
    "        unary_cell.edges[1, 2].set('cell_name', 'u_cell')\n",
    "        unary_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable edge\n",
    "        \n",
    "        # binary cell definition\n",
    "        binary_cell = Graph()\n",
    "        binary_cell.name = 'b_cell'\n",
    "        binary_cell.add_node(1) # input node\n",
    "        binary_cell.add_node(2) # input node\n",
    "        binary_cell.add_node(3) # concatination node\n",
    "        binary_cell.nodes[3]['comb_op'] = Stack()\n",
    "        binary_cell.add_node(4) # intermediate node\n",
    "        binary_cell.add_node(5) # output node\n",
    "        binary_cell.add_edges_from([(3, 4, EdgeData())]) # mutable edge\n",
    "        binary_cell.edges[3, 4].set('cell_name', 'b_cell') \n",
    "        binary_cell.add_edges_from([(1, 3, EdgeData().finalize()),\n",
    "                                    (2, 3, EdgeData().finalize()),\n",
    "                                    (4, 5, EdgeData().finalize())]) # immutable edges\n",
    "        \n",
    "        # activation cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'a_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2, subgraph=deepcopy(unary_cell).set_scope('u_stage_1').set_input([1])) # unary cell 1\n",
    "        activation_cell.nodes[2]['subgraph'].name = 'u_stage_1'\n",
    "        activation_cell.add_node(3, subgraph=deepcopy(unary_cell).set_scope('u_stage_2').set_input([1])) # unary cell 2\n",
    "        activation_cell.nodes[3]['subgraph'].name = 'u_stage_2'\n",
    "        activation_cell.add_node(4, subgraph=deepcopy(unary_cell).set_scope('u_stage_3').set_input([1])) # unary cell 3\n",
    "        activation_cell.nodes[4]['subgraph'].name = 'u_stage_3'\n",
    "        activation_cell.add_node(5, subgraph=deepcopy(binary_cell).set_scope('b_stage_1').set_input([2, 3])) # binary cell 1\n",
    "        activation_cell.nodes[5]['subgraph'].name = 'b_stage_1'\n",
    "        activation_cell.add_node(6, subgraph=deepcopy(unary_cell).set_scope('u_stage_4').set_input([5])) # unary cell 4\n",
    "        activation_cell.nodes[6]['subgraph'].name = 'u_stage_4'\n",
    "        activation_cell.add_node(7, subgraph=deepcopy(binary_cell).set_scope('b_stage_2').set_input([4, 6])) # binary cell 2\n",
    "        activation_cell.nodes[7]['subgraph'].name = 'b_stage_2'\n",
    "        activation_cell.add_node(8) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData().finalize()), \n",
    "                                        (1, 3, EdgeData().finalize()),\n",
    "                                        (1, 4, EdgeData().finalize()),\n",
    "                                        (2, 5, EdgeData().finalize()),\n",
    "                                        (3, 5, EdgeData().finalize()), \n",
    "                                        (4, 7, EdgeData().finalize()),\n",
    "                                        (5, 6, EdgeData().finalize()),\n",
    "                                        (6, 7, EdgeData().finalize()),\n",
    "                                        (7, 8, EdgeData().finalize())])\n",
    "        \n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3, subgraph=deepcopy(activation_cell).set_input([2])) # activation cell\n",
    "        self.add_node(4) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 4)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[3, 4].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in u_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_unary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            ) # set unary cell ops\n",
    "        \n",
    "        for scope in b_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_binary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            ) # set binary cell ops\n",
    "        \n",
    "\n",
    "    def _set_unary_ops(self, edge, channels=None):\n",
    "        edge.data.set('op', [\n",
    "            ops.Identity(), \n",
    "            ops.Zero(stride=1)\n",
    "        ]) \n",
    "        \n",
    "        \n",
    "    def _set_binary_ops(self, edge, channels=None):\n",
    "        edge.data.set('op', [\n",
    "            Minimum(),\n",
    "            Maximum(),\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack(AbstractPrimitive):\n",
    "    def __init__(self):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.stack(x)\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "class UnStack(AbstractPrimitive):\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__(locals())\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return x[self.dim]\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "class RNNResNet20SearchSpace(Graph):\n",
    "    \"\"\"\n",
    "    https://www.researchgate.net/figure/ResNet-20-architecture_fig3_351046093\n",
    "    \"\"\"\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        f\"activation_{i}\" for i in range(1, 20)\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1)  # input node\n",
    "        activation_cell.add_node(2)  # unary node / intermediate node\n",
    "        activation_cell.add_node(3)  # unary node / intermediate node\n",
    "        activation_cell.add_node(4)  # binary node / output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(1, 3, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_edges_from([(2, 4, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(3, 4, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.nodes[4]['comb_op'] = Stack()\n",
    "\n",
    "        activation_cell.add_node(5)  # binary node / output node\n",
    "        activation_cell.add_edges_from([(4, 5, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(6)\n",
    "        activation_cell.add_edges_from([(5, 6, EdgeData().finalize())])  # unary node / intermediate node\n",
    "        activation_cell.add_node(7)\n",
    "        activation_cell.add_edges_from([(6, 7, EdgeData())])  # mutable intermediate edge\n",
    "        activation_cell.add_node(8)\n",
    "        activation_cell.add_edges_from([(1, 8, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(9)\n",
    "        activation_cell.add_edges_from([(8, 9, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(7, 9, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.nodes[9]['comb_op'] = Stack()\n",
    "\n",
    "        activation_cell.add_node(10)\n",
    "        activation_cell.add_edges_from([(9, 10, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(11)\n",
    "        activation_cell.add_edges_from([(10, 11, EdgeData().finalize())])  # mutable intermediate edge\n",
    "\n",
    "        for tup in [(1, 2), (1, 3), (1, 8), (6, 7)]:  # unary operations\n",
    "            activation_cell.edges[tup[0], tup[1]].set(\"op\", [\n",
    "                ops.Identity(),\n",
    "                ops.Zero(stride=1),\n",
    "                ops.Sequential(Power(2)),\n",
    "                ops.Sequential(Power(3)),\n",
    "                ops.Sequential(Sqrt()),\n",
    "                ops.Sequential(Sin()),\n",
    "                ops.Sequential(Cos()),\n",
    "                ops.Sequential(Abs_op()),\n",
    "                ops.Sequential(Sign()),\n",
    "#                 ops.Sequential(Beta_mul(channels=32)),\n",
    "#                 ops.Sequential(Beta_add(channels=32)),\n",
    "                ops.Sequential(Log()),\n",
    "                ops.Sequential(Exp()),\n",
    "                ops.Sequential(Sinh()),\n",
    "                ops.Sequential(Cosh()),\n",
    "                ops.Sequential(Tanh()),\n",
    "                ops.Sequential(Asinh()),\n",
    "                ops.Sequential(Atan()),\n",
    "                ops.Sequential(Sinc()),\n",
    "                ops.Sequential(Maximum0()),\n",
    "                ops.Sequential(Minimum0()),\n",
    "                ops.Sequential(Sigmoid()),\n",
    "                ops.Sequential(LogExp()),\n",
    "                ops.Sequential(Exp2()),\n",
    "                ops.Sequential(Erf()),\n",
    "#                 ops.Sequential(Beta(channels=16)),    \n",
    "            ])\n",
    "\n",
    "        for tup in [(4, 5), (9, 10)]:\n",
    "            activation_cell.edges[tup[0], tup[1]].set(\"op\", [\n",
    "                ops.Sequential(Add()),\n",
    "                ops.Sequential(Sub()),\n",
    "                ops.Sequential(Mul()),\n",
    "                ops.Sequential(Div()),\n",
    "                ops.Sequential(Maximum()),\n",
    "                ops.Sequential(Minimum()),\n",
    "                ops.Sequential(SigMul()),\n",
    "#                 ops.Sequential(ExpBetaSub2(channels=32)),\n",
    "#                 ops.Sequential(ExpBetaSubAbs(channels=32)),\n",
    "#                 ops.Sequential(BetaMix(channels=32)),\n",
    "            ])\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1)  # input\n",
    "        self.add_node(2)  # intermediate\n",
    "        self.add_node(3,\n",
    "                      subgraph=activation_cell.copy().set_scope(\"activation_1\").set_input([2]))  # activation cell 3\n",
    "        #self.nodes[3]['subgraph'].name = \"activation_1\"\n",
    "\n",
    "        self.add_node(4)\n",
    "        self.add_node(5,\n",
    "                      subgraph=activation_cell.copy().set_scope(\"activation_2\").set_input([4]))  # activation cell 3\n",
    "        #self.nodes[5]['subgraph'].name = \"activation_2\"\n",
    "\n",
    "        self.add_node(6)\n",
    "        self.add_node(7,\n",
    "                      subgraph=activation_cell.copy().set_scope(\"activation_3\").set_input([6]))  # activation cell 3\n",
    "        #self.nodes[7]['subgraph'].name = \"activation_3\"\n",
    "\n",
    "        self.add_edges_from([\n",
    "            (1, 2, EdgeData()),\n",
    "            (2, 3, EdgeData()),\n",
    "            (3, 4, EdgeData()),\n",
    "            (4, 5, EdgeData()),\n",
    "            (5, 6, EdgeData()),\n",
    "            (3, 6, EdgeData()),\n",
    "            (6, 7, EdgeData())\n",
    "        ])\n",
    "\n",
    "        self.edges[1, 2].set('op',\n",
    "                             ops.Sequential(nn.Conv2d(3, 16, 3, padding=1), ))  # convolutional edge\n",
    "        self.edges[3, 4].set('op',\n",
    "                             ops.Sequential(nn.Conv2d(16, 16, 3, padding=1), ))  # convolutional edge\n",
    "        self.edges[5, 6].set('op',\n",
    "                             ops.Sequential(nn.Conv2d(16, 16, 3, padding=1), ))  # convolutional edge\n",
    "\n",
    "        conv_option = {\n",
    "            \"in_channels\": 16,\n",
    "            \"out_channels\": 16,\n",
    "            \"kernel_size\": 3,\n",
    "            \"padding\": 1\n",
    "        }\n",
    "        self._create_base_block(7, 4, activation_cell, conv_option)\n",
    "        self._create_base_block(11, 6, activation_cell, conv_option)\n",
    "\n",
    "        conv_option_a = {\n",
    "            \"in_channels\": 16,\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 2\n",
    "        }\n",
    "        conv_option_b = {\n",
    "            \"in_channels\": 16,\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 1,\n",
    "            \"padding\": 0,\n",
    "            \"stride\": 2\n",
    "        }\n",
    "        self._create_reduction_block(15, 8, activation_cell, conv_option_a, conv_option_b)\n",
    "\n",
    "        conv_option = {\n",
    "            \"in_channels\": 32,\n",
    "            \"out_channels\": 32,\n",
    "            \"kernel_size\": 3,\n",
    "            \"padding\": 1\n",
    "        }\n",
    "        self._create_base_block(19, 10, activation_cell, conv_option)\n",
    "        self._create_base_block(23, 12, activation_cell, conv_option)\n",
    "\n",
    "        conv_option_a = {\n",
    "            \"in_channels\": 32,\n",
    "            \"out_channels\": 64,\n",
    "            \"kernel_size\": 3,\n",
    "            \"padding\": 1,\n",
    "            \"stride\": 2\n",
    "        }\n",
    "        conv_option_b = {\n",
    "            \"in_channels\": 32,\n",
    "            \"out_channels\": 64,\n",
    "            \"kernel_size\": 1,\n",
    "            \"padding\": 0,\n",
    "            \"stride\": 2\n",
    "        }\n",
    "        self._create_reduction_block(27, 14, activation_cell, conv_option_a, conv_option_b)\n",
    "\n",
    "        conv_option = {\n",
    "            \"in_channels\": 64,\n",
    "            \"out_channels\": 64,\n",
    "            \"kernel_size\": 3,\n",
    "            \"padding\": 1\n",
    "        }\n",
    "        self._create_base_block(31, 16, activation_cell, conv_option)\n",
    "        self._create_base_block(34, 18, activation_cell, conv_option)\n",
    "\n",
    "        # add head\n",
    "        self.add_node(39)\n",
    "        self.add_edges_from([\n",
    "            (38, 39, EdgeData())\n",
    "        ])\n",
    "        self.edges[38, 39].set('op',\n",
    "                               ops.Sequential(\n",
    "                                   nn.AvgPool2d(8),\n",
    "                                   nn.Flatten(),\n",
    "                                   nn.Linear(64, 10),\n",
    "                                   nn.Softmax()\n",
    "                               ))  # convolutional edge\n",
    "        self.add_node(40)\n",
    "        self.add_edges_from([\n",
    "            (39, 40, EdgeData().finalize())\n",
    "        ])\n",
    "\n",
    "    def _create_base_block(self, start: int, stage: int, cell, conv_option: dict):\n",
    "        self.add_node(start + 1)\n",
    "\n",
    "        self.add_node(start + 2, subgraph=cell.copy().set_scope(f\"activation_{stage}\").set_input(\n",
    "            [start + 1]))  # activation cell 3\n",
    "        #self.nodes[start + 2]['subgraph'].name = f\"activation_{stage}\"\n",
    "\n",
    "        self.add_node(start + 3)\n",
    "\n",
    "        self.add_node(start + 4, subgraph=cell.copy().set_scope(f\"activation_{stage + 1}\").set_input(\n",
    "            [start + 3]))  # activation cell 3\n",
    "        #self.nodes[start + 4]['subgraph'].name = f\"activation_{stage + 1}\"\n",
    "\n",
    "        self.add_edges_from([\n",
    "            (start, start + 1, EdgeData()),\n",
    "            (start, start + 3, EdgeData()),\n",
    "            (start + 1, start + 2, EdgeData()),\n",
    "            (start + 2, start + 3, EdgeData()),\n",
    "            (start + 3, start + 4, EdgeData()),\n",
    "        ])\n",
    "\n",
    "        self.edges[start, start + 1].set('op',\n",
    "                                         ops.Sequential(nn.Conv2d(**conv_option), ))  # convolutional edge\n",
    "        self.edges[start + 2, start + 3].set('op',\n",
    "                                             ops.Sequential(nn.Conv2d(**conv_option), ))  # convolutional edge\n",
    "\n",
    "    def _create_reduction_block(self, start: int, stage: int, cell, conv_option_a: dict, conv_option_b: dict):\n",
    "        self.add_node(start + 1)\n",
    "\n",
    "        self.add_node(start + 2, subgraph=cell.copy().set_scope(f\"activation_{stage}\").set_input(\n",
    "            [start + 1]))  # activation cell 3\n",
    "        #self.nodes[start + 2]['subgraph'].name = f\"activation_{stage}\"\n",
    "\n",
    "        self.add_node(start + 3)\n",
    "\n",
    "        self.add_node(start + 4, subgraph=cell.copy().set_scope(f\"activation_{stage + 1}\").set_input(\n",
    "            [start + 3]))  # activation cell 3\n",
    "        #self.nodes[start + 4]['subgraph'].name = f\"activation_{stage + 1}\"\n",
    "\n",
    "        self.add_edges_from([\n",
    "            (start, start + 1, EdgeData()),\n",
    "            (start, start + 3, EdgeData()),  # add conv\n",
    "            (start + 1, start + 2, EdgeData()),\n",
    "            (start + 2, start + 3, EdgeData()),\n",
    "            (start + 3, start + 4, EdgeData()),\n",
    "        ])\n",
    "\n",
    "        self.edges[start, start + 1].set('op',\n",
    "                                         ops.Sequential(nn.Conv2d(**conv_option_a), ))  # convolutional edge\n",
    "        conv_option_a[\"in_channels\"] = conv_option_a[\"out_channels\"]\n",
    "        conv_option_a[\"stride\"] = 1\n",
    "\n",
    "        self.edges[start, start + 3].set('op',\n",
    "                                         ops.Sequential(nn.Conv2d(**conv_option_b), ))  # convolutional edge\n",
    "        self.edges[start + 2, start + 3].set('op',\n",
    "                                             ops.Sequential(nn.Conv2d(**conv_option_a), ))  # convolutional edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSearchSpaceComplexActivation(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        stages = ['a_stage_1', 'a_stage_2']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1)  # input node\n",
    "        activation_cell.add_node(2)  # unary node / intermediate node\n",
    "        activation_cell.add_node(3)  # unary node / intermediate node\n",
    "        activation_cell.add_node(4)  # binary node / output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(1, 3, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_edges_from([(2, 4, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(3, 4, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.nodes[4]['comb_op'] = Stack()\n",
    "\n",
    "        activation_cell.add_node(5)  # binary node / output node\n",
    "        activation_cell.add_edges_from([(4, 5, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(6)\n",
    "        activation_cell.add_edges_from([(5, 6, EdgeData().finalize())])  # unary node / intermediate node\n",
    "        activation_cell.add_node(7)\n",
    "        activation_cell.add_edges_from([(6, 7, EdgeData())])  # mutable intermediate edge\n",
    "        activation_cell.add_node(8)\n",
    "        activation_cell.add_edges_from([(1, 8, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(9)\n",
    "        activation_cell.add_edges_from([(8, 9, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(7, 9, EdgeData().finalize())])  # mutable intermediate edge\n",
    "        activation_cell.nodes[9]['comb_op'] = Stack()\n",
    "\n",
    "        activation_cell.add_node(10)\n",
    "        activation_cell.add_edges_from([(9, 10, EdgeData())])  # mutable intermediate edge\n",
    "\n",
    "        activation_cell.add_node(11)\n",
    "        activation_cell.add_edges_from([(10, 11, EdgeData().finalize())])  # mutable intermediate edge\n",
    "\n",
    "        for tup in [(1, 2), (1, 3), (1, 8), (6, 7)]:  # unary operations\n",
    "            activation_cell.edges[tup[0], tup[1]].set(\"op\", [i for i in [\n",
    "                ops.Identity(),\n",
    "                ops.Zero(stride=1),\n",
    "                ops.Sequential(Power(2)),\n",
    "                ops.Sequential(Power(3)),\n",
    "                ops.Sequential(Sqrt()),\n",
    "                ops.Sequential(Sin()),\n",
    "                ops.Sequential(Cos()),\n",
    "                ops.Sequential(Abs_op()),\n",
    "                ops.Sequential(Sign()),\n",
    "#                 ops.Sequential(Beta_mul(channels=32)),\n",
    "#                 ops.Sequential(Beta_add(channels=32)),\n",
    "                ops.Sequential(Log()),\n",
    "                ops.Sequential(Exp()),\n",
    "                ops.Sequential(Sinh()),\n",
    "                ops.Sequential(Cosh()),\n",
    "                ops.Sequential(Tanh()),\n",
    "                ops.Sequential(Asinh()),\n",
    "                ops.Sequential(Atan()),\n",
    "                ops.Sequential(Sinc()),\n",
    "                ops.Sequential(Maximum0()),\n",
    "                ops.Sequential(Minimum0()),\n",
    "                ops.Sequential(Sigmoid()),\n",
    "                ops.Sequential(LogExp()),\n",
    "                ops.Sequential(Exp2()),\n",
    "                ops.Sequential(Erf()),\n",
    "#                 ops.Sequential(Beta(channels=16)),    \n",
    "            ][:]])\n",
    "\n",
    "        for tup in [(4, 5), (9, 10)]:\n",
    "            activation_cell.edges[tup[0], tup[1]].set(\"op\", [i for i in [\n",
    "                ops.Sequential(Add()),\n",
    "                ops.Sequential(Sub()),\n",
    "                ops.Sequential(Mul()),\n",
    "                ops.Sequential(Div()),\n",
    "                ops.Sequential(Maximum()),\n",
    "                ops.Sequential(Minimum()),\n",
    "                ops.Sequential(SigMul()),\n",
    "#               ops.Sequential(ExpBetaSub2(channels=32)),\n",
    "#               ops.Sequential(ExpBetaSubAbs(channels=32)),\n",
    "#               ops.Sequential(BetaMix(channels=32)),\n",
    "            ][:]])\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3,subgraph=activation_cell.copy().set_scope(\"a_stage_1\").set_input([2]))  # activation cell 3\n",
    "        self.add_node(5) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 5)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[4, 5].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = RNNResNet20SearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = SimpleSearchSpaceComplexActivation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/13 11:31:17 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/13 11:31:17 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"
     ]
    }
   ],
   "source": [
    "optimizer = DARTSOptimizer(config)\n",
    "optimizer.adapt_search_space(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:31:18 nl.defaults.trainer]: \u001b[0mparam size = 0.006882MB\n",
      "\u001b[32m[07/13 11:31:18 nl.defaults.trainer]: \u001b[0mStart training\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[07/13 11:31:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.001529, -0.000555, -0.000095, -0.000816, +0.001150, -0.000755, -0.000470, -0.002380, +0.000877, -0.000549, -0.000377, +0.000611, +0.001464, +0.000401, -0.001118, +0.000092, +0.001110, +0.001769, +0.001139, -0.001464, -0.002292, +0.000135, -0.000023, 17\n",
      "-0.001366, +0.001118, -0.000766, +0.001588, -0.000654, +0.000865, +0.000068, +0.000319, -0.000595, +0.001064, +0.000822, +0.000472, +0.000078, -0.001169, +0.001535, +0.000990, -0.000028, +0.000868, -0.000395, +0.001506, -0.001716, +0.000950, -0.001593, 3\n",
      "-0.000397, +0.000516, +0.001723, +0.000910, -0.000226, +0.000921, -0.000947, -0.000704, +0.001299, -0.000040, +0.001560, -0.000360, -0.000715, +0.000691, -0.001885, +0.000120, -0.000407, +0.001689, -0.000380, -0.002143, +0.002399, -0.000198, +0.001380, 20\n",
      "-0.000262, +0.001170, -0.000366, +0.001566, -0.000944, +0.001050, +0.001452, 3\n",
      "-0.000047, +0.001868, -0.000806, -0.001903, +0.000575, -0.000194, +0.001276, -0.000954, +0.000102, +0.001029, -0.002628, -0.001193, +0.000336, -0.000304, +0.000431, -0.001706, -0.000572, +0.001210, +0.002527, +0.000228, +0.000672, -0.000744, -0.000411, 18\n",
      "-0.001294, -0.000849, +0.000120, -0.001110, -0.000678, +0.000475, -0.000669, 5\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/13 11:31:21 nl.search_spaces.core.graph]: \u001b[0mComb_op is ignored if subgraph is defined!\n",
      "\u001b[32m[07/13 11:31:22 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.29706, validation loss: 2.31518, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:22 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     872 KB |   15122 KB |  188632 KB |  187760 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15122 KB |  188632 KB |  187760 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     872 KB |   15122 KB |  188632 KB |  187760 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15122 KB |  188632 KB |  187760 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    3224 KB |    8482 KB |  214641 KB |  211417 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |    3224 KB |    8482 KB |  214641 KB |  211417 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      50    |     305    |    5450    |    5400    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     305    |    5450    |    5400    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      50    |     305    |    5450    |    5400    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     305    |    5450    |    5400    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |       8    |       8    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       8    |       8    |       8    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       6    |      20    |    1747    |    1741    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       6    |      20    |    1747    |    1741    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[07/13 11:31:22 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     872 KB |   15155 KB |  377197 KB |  376325 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15155 KB |  377197 KB |  376325 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     872 KB |   15155 KB |  377197 KB |  376325 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15155 KB |  377197 KB |  376325 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    3224 KB |    8482 KB |  426532 KB |  423308 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |    3224 KB |    8482 KB |  426532 KB |  423308 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      50    |     315    |   10870    |   10820    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     315    |   10870    |   10820    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      50    |     315    |   10870    |   10820    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     315    |   10870    |   10820    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |       8    |       8    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       8    |       8    |       8    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       7    |      21    |    3423    |    3416    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       7    |      21    |    3423    |    3416    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/13 11:31:22 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     872 KB |   15155 KB |  565762 KB |  564890 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15155 KB |  565762 KB |  564890 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     872 KB |   15155 KB |  565762 KB |  564890 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |     872 KB |   15155 KB |  565762 KB |  564890 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 B  |\n",
      "|       from small pool |   16384 KB |   16384 KB |   16384 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    3224 KB |    8482 KB |  638423 KB |  635199 KB |\n",
      "|       from large pool |       0 KB |       0 KB |       0 KB |       0 KB |\n",
      "|       from small pool |    3224 KB |    8482 KB |  638423 KB |  635199 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      50    |     315    |   16290    |   16240    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     315    |   16290    |   16240    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      50    |     315    |   16290    |   16240    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |      50    |     315    |   16290    |   16240    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |       8    |       8    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       8    |       8    |       8    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       6    |      21    |    5244    |    5238    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       6    |      21    |    5244    |    5238    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[07/13 11:31:27 nl.defaults.trainer]: \u001b[0mEpoch 0-66, Train loss: 2.29946, validation loss: 2.30342, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:32 nl.defaults.trainer]: \u001b[0mEpoch 0-131, Train loss: 2.30946, validation loss: 2.30863, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:37 nl.defaults.trainer]: \u001b[0mEpoch 0-194, Train loss: 2.29749, validation loss: 2.29683, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:42 nl.defaults.trainer]: \u001b[0mEpoch 0-253, Train loss: 2.30068, validation loss: 2.30108, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:47 nl.defaults.trainer]: \u001b[0mEpoch 0-314, Train loss: 2.28107, validation loss: 2.29988, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:52 nl.defaults.trainer]: \u001b[0mEpoch 0-370, Train loss: 2.30148, validation loss: 2.29135, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:31:57 nl.defaults.trainer]: \u001b[0mEpoch 0-425, Train loss: 2.30389, validation loss: 2.29871, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:02 nl.defaults.trainer]: \u001b[0mEpoch 0-479, Train loss: 2.32277, validation loss: 2.29424, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:07 nl.defaults.trainer]: \u001b[0mEpoch 0-534, Train loss: 2.28088, validation loss: 2.29181, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:12 nl.defaults.trainer]: \u001b[0mEpoch 0-589, Train loss: 2.34742, validation loss: 2.39407, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:17 nl.defaults.trainer]: \u001b[0mEpoch 0-639, Train loss: 2.30848, validation loss: 2.28189, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:22 nl.defaults.trainer]: \u001b[0mEpoch 0-690, Train loss: 2.30119, validation loss: 2.30207, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:27 nl.defaults.trainer]: \u001b[0mEpoch 0-743, Train loss: 2.30299, validation loss: 2.30467, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:33 nl.defaults.trainer]: \u001b[0mEpoch 0-798, Train loss: 2.30362, validation loss: 2.29626, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:38 nl.defaults.trainer]: \u001b[0mEpoch 0-854, Train loss: 2.29763, validation loss: 2.30330, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:43 nl.defaults.trainer]: \u001b[0mEpoch 0-915, Train loss: 2.30929, validation loss: 2.30288, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:48 nl.defaults.trainer]: \u001b[0mEpoch 0-979, Train loss: 2.27308, validation loss: 2.30046, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:53 nl.defaults.trainer]: \u001b[0mEpoch 0-1041, Train loss: 2.30319, validation loss: 2.27868, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:32:58 nl.defaults.trainer]: \u001b[0mEpoch 0-1097, Train loss: 2.30067, validation loss: 2.30409, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:03 nl.defaults.trainer]: \u001b[0mEpoch 0-1152, Train loss: 2.26448, validation loss: 2.31082, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:08 nl.defaults.trainer]: \u001b[0mEpoch 0-1207, Train loss: 2.28930, validation loss: 2.29351, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:13 nl.defaults.trainer]: \u001b[0mEpoch 0-1266, Train loss: 2.30490, validation loss: 2.39865, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:18 nl.defaults.trainer]: \u001b[0mEpoch 0-1326, Train loss: 2.31246, validation loss: 2.30034, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:23 nl.defaults.trainer]: \u001b[0mEpoch 0-1386, Train loss: 2.30272, validation loss: 2.30382, learning rate: [0.001]\n",
      "\u001b[32m[07/13 11:33:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0008, -0.0029, -0.0053,  0.0226, -0.0022, -0.0032, -0.0035, -0.0036,\n",
      "        -0.0042,  0.0023, -0.0026,  0.0214, -0.0013, -0.0026, -0.0021, -0.0025,\n",
      "        -0.0019, -0.0027, -0.0011, -0.0044, -0.0051, -0.0029, -0.0027],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0047, -0.0036, -0.0073,  0.0222,  0.0102,  0.0049,  0.0114,  0.0051,\n",
      "        -0.0093, -0.0103,  0.0123,  0.0196,  0.0116,  0.0042,  0.0055,  0.0051,\n",
      "         0.0113,  0.0058, -0.0010,  0.0129,  0.0097,  0.0123,  0.0044],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0029, -0.0060, -0.0221,  0.0510,  0.0034,  0.0045,  0.0025,  0.0037,\n",
      "        -0.0035, -0.0035,  0.0051,  0.0053,  0.0031,  0.0042,  0.0017,  0.0037,\n",
      "         0.0030,  0.0057, -0.0014,  0.0013,  0.0059,  0.0032,  0.0049],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0050, -0.0061,  0.0223, -0.0143, -0.0039, -0.0038, -0.0058],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-1.5034e-03, -1.4655e-03,  2.0749e-03,  3.3147e-03, -1.2464e-03,\n",
      "        -1.8489e-03, -1.0111e-03, -9.3422e-05, -1.3599e-03, -4.0312e-03,\n",
      "         4.3896e-03, -3.9151e-04,  4.5730e-03, -1.7595e-03, -1.6548e-03,\n",
      "        -1.7285e-03, -1.1050e-03, -8.5800e-04, -1.9967e-03, -7.5390e-04,\n",
      "        -5.6231e-05, -9.6839e-04, -1.8033e-03], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0742, -0.0866,  0.0788, -0.0535, -0.0863, -0.0239, -0.0287],\n",
      "       device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/13 11:33:25 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[32m[07/13 11:33:25 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 12.32889, 54.55778, Validation accuracy: 12.35341, 54.76190\n",
      "\u001b[32m[07/13 11:33:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000792, -0.002944, -0.005308, +0.022618, -0.002210, -0.003208, -0.003502, -0.003620, -0.004159, +0.002299, -0.002644, +0.021380, -0.001287, -0.002647, -0.002059, -0.002486, -0.001924, -0.002680, -0.001128, -0.004395, -0.005099, -0.002879, -0.002654, 3\n",
      "+0.004731, -0.003555, -0.007343, +0.022213, +0.010245, +0.004874, +0.011434, +0.005140, -0.009271, -0.010344, +0.012323, +0.019612, +0.011629, +0.004214, +0.005464, +0.005098, +0.011347, +0.005835, -0.001042, +0.012860, +0.009688, +0.012322, +0.004433, 3\n",
      "+0.002853, -0.006004, -0.022094, +0.050983, +0.003416, +0.004491, +0.002504, +0.003736, -0.003481, -0.003484, +0.005087, +0.005299, +0.003059, +0.004242, +0.001687, +0.003677, +0.003039, +0.005728, -0.001399, +0.001313, +0.005856, +0.003250, +0.004908, 3\n",
      "-0.004994, -0.006072, +0.022342, -0.014255, -0.003926, -0.003775, -0.005847, 2\n",
      "-0.001503, -0.001465, +0.002075, +0.003315, -0.001246, -0.001849, -0.001011, -0.000093, -0.001360, -0.004031, +0.004390, -0.000392, +0.004573, -0.001760, -0.001655, -0.001729, -0.001105, -0.000858, -0.001997, -0.000754, -0.000056, -0.000968, -0.001803, 12\n",
      "-0.074246, -0.086633, +0.078820, -0.053464, -0.086330, -0.023861, -0.028719, 2\n",
      "\u001b[32m[07/13 11:33:28 nl.defaults.trainer]: \u001b[0mEpoch 1-35, Train loss: 2.29461, validation loss: 2.30382, learning rate: [0.0009779754323328191]\n",
      "\u001b[32m[07/13 11:33:33 nl.defaults.trainer]: \u001b[0mEpoch 1-89, Train loss: 2.30648, validation loss: 2.30754, learning rate: [0.0009779754323328191]\n",
      "\u001b[32m[07/13 11:33:38 nl.defaults.trainer]: \u001b[0mEpoch 1-143, Train loss: 2.30245, validation loss: 2.30266, learning rate: [0.0009779754323328191]\n",
      "\u001b[32m[07/13 11:33:43 nl.defaults.trainer]: \u001b[0mEpoch 1-201, Train loss: 2.29647, validation loss: 2.30543, learning rate: [0.0009779754323328191]\n",
      "\u001b[32m[07/13 11:33:48 nl.defaults.trainer]: \u001b[0mEpoch 1-255, Train loss: 2.30162, validation loss: 2.30170, learning rate: [0.0009779754323328191]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_234309/2031152707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/NASLib/naslib/defaults/trainer.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, resume_from, summary_writer, after_epoch, report_incumbent)\u001b[0m\n\u001b[1;32m    124\u001b[0m                     )\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                     \u001b[0mlogits_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/optimizers/oneshot/darts/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, data_train, data_val)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Update architecture weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mlogits_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/search_spaces/core/graph.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# TODO: merge 'subgraph' and 'comb_op'. It is basicallly the same thing. Also in parse()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"subgraph\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subgraph\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/search_spaces/core/graph.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         )\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                         \u001b[0medge_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                         raise ValueError(\n",
      "\u001b[0;32m~/Work/NASLib/naslib/search_spaces/core/primitives.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_data)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedded_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/optimizers/oneshot/darts/optimizer.py\u001b[0m in \u001b[0;36mapply_weights\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/optimizers/oneshot/darts/optimizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NASLib/naslib/search_spaces/core/primitives.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_data)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedded_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/naslib/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_234309/2741753646.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_data)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedded_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer, config)\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate_oneshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.cuda.FloatTensor of size 23 (GPU 0)]\n",
       "    (1): Parameter containing: [torch.cuda.FloatTensor of size 23 (GPU 0)]\n",
       "    (2): Parameter containing: [torch.cuda.FloatTensor of size 23 (GPU 0)]\n",
       "    (3): Parameter containing: [torch.cuda.FloatTensor of size 7 (GPU 0)]\n",
       "    (4): Parameter containing: [torch.cuda.FloatTensor of size 23 (GPU 0)]\n",
       "    (5): Parameter containing: [torch.cuda.FloatTensor of size 7 (GPU 0)]\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.architectural_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
