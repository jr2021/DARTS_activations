{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.optimizers import DARTSOptimizer\n",
    "from naslib.search_spaces import DartsSearchSpace\n",
    "from naslib.utils import utils, setup_logger, get_config_from_args, set_seed, log_args\n",
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from fvcore.common.config import CfgNode\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='/project/dl2022s/robertsj/NASLib/naslib/benchmarks/nas_predictors/discrete_config.yaml', dist_backend='nccl', dist_url='tcp://127.0.0.1:8888', eval_only=False, gpu=None, model_path=None, multiprocessing_distributed=False, opts=[], rank=0, resume=False, seed=0, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "config = utils.get_config_from_args(config_type='nas')\n",
    "# config.search.epochs = 1 # for testing\n",
    "config.optimizer = 'darts'\n",
    "config.save = '{}/{}/{}/{}'.format(config.out_dir, config.dataset, config.optimizer, config.seed)\n",
    "utils.set_seed(config.seed)\n",
    "utils.log_args(config)\n",
    "\n",
    "\n",
    "logger = setup_logger(config.save + '/log.log')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class DartsSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2', \n",
    "        'a_stage_3'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [(16 * 5 * 5, 120), (120, 84), (84, 10)]\n",
    "        stages = ['a_stage_1', 'a_stage_2', 'a_stage_3']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2) # intermediate node\n",
    "        activation_cell.add_node(3) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())]) # mutable intermediate edge\n",
    "        activation_cell.edges[1, 2].set('cell_name', 'activation_cell') \n",
    "        activation_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable output edge\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        for i, scope in zip(range(3, 6), stages):\n",
    "            self.add_node(i, subgraph=deepcopy(activation_cell).set_scope(scope).set_input([i-1])) # activation node i\n",
    "            self.nodes[i]['subgraph'].name = scope\n",
    "        self.add_node(6) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 6)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        \n",
    "        for scope, (in_dim, out_dim) in zip(stages, channels):\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_ops(edge, in_dim, out_dim),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "    def _set_ops(self, edge, in_dim, out_dim):\n",
    "        if out_dim != 10:\n",
    "            edge.data.set('op', [\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Hardswish()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.LeakyReLU()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Identity())\n",
    "            ])\n",
    "        else:\n",
    "            edge.data.set('op', [\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Softmax(dim=1))\n",
    "            ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 09:31:39 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 09:31:39 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 09:31:39 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"
     ]
    }
   ],
   "source": [
    "search_space = DartsSearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 09:31:42 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/27 09:31:42 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be veryfied. Be cautious with the setting of `private_edge_data` in `update_edges()`\n",
      "\u001b[32m[06/27 09:31:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mParsed graph:\n",
      "Graph normal_cell:\n",
      " Graph(\n",
      "  (normal_cell-edge(1,3)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(1,4)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(1,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(1,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(2,3)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(2,4)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(2,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(2,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(3,4)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(3,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(3,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(3,7)): Identity()\n",
      "  (normal_cell-edge(4,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(4,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(4,7)): Identity()\n",
      "  (normal_cell-edge(5,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=16, bias=False)\n",
      "        (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=16, bias=False)\n",
      "        (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (normal_cell-edge(5,7)): Identity()\n",
      "  (normal_cell-edge(6,7)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph reduction_cell:\n",
      " Graph(\n",
      "  (reduction_cell-edge(1,3)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(1,4)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(1,5)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(1,6)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(2,3)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(2,4)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(2,5)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(2,6)): MixedOp(\n",
      "    (primitive-0): FactorizedReduce(\n",
      "      (relu): ReLU()\n",
      "      (conv_1): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (conv_2): Conv2d(32, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "    (primitive-1): Zero (stride=2)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(3,4)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(3,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(3,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(3,7)): Identity()\n",
      "  (reduction_cell-edge(4,5)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(4,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(4,7)): Identity()\n",
      "  (reduction_cell-edge(5,6)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "    (primitive-2): MaxPool(\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (primitive-3): AvgPool(\n",
      "      (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      "    )\n",
      "    (primitive-4): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-5): SepConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=32, bias=False)\n",
      "        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-6): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (primitive-7): DilConv(\n",
      "      (op): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=32, bias=False)\n",
      "        (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (reduction_cell-edge(5,7)): Identity()\n",
      "  (reduction_cell-edge(6,7)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph makrograph:\n",
      " DartsSearchSpace(\n",
      "  (makrograph-edge(1,2)): Stem(\n",
      "    (seq): Sequential(\n",
      "      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(2,3)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(2,4)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(3)): Graph normal_cell-0.7579544, scope n_stage_1, 7 nodes\n",
      "  (makrograph-edge(3,4)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(3,5)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(4)): Graph normal_cell-0.4205716, scope n_stage_1, 7 nodes\n",
      "  (makrograph-edge(4,5)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(4,6)): FactorizedReduce(\n",
      "    (relu): ReLU()\n",
      "    (conv_1): Conv2d(64, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (conv_2): Conv2d(64, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (makrograph-subgraph_at(5)): Graph reduction_cell-0.7579544, scope r_stage_1, 7 nodes\n",
      "  (makrograph-edge(5,6)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(5,7)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(6)): Graph normal_cell-0.2589168, scope n_stage_2, 7 nodes\n",
      "  (makrograph-edge(6,7)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(6,8)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(7)): Graph normal_cell-0.5112747, scope n_stage_2, 7 nodes\n",
      "  (makrograph-edge(7,8)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(7,9)): FactorizedReduce(\n",
      "    (relu): ReLU()\n",
      "    (conv_1): Conv2d(128, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (conv_2): Conv2d(128, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  )\n",
      "  (makrograph-subgraph_at(8)): Graph reduction_cell-0.4049341, scope r_stage_2, 7 nodes\n",
      "  (makrograph-edge(8,9)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(8,10)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(9)): Graph normal_cell-0.7837986, scope n_stage_3, 7 nodes\n",
      "  (makrograph-edge(9,10)): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-subgraph_at(10)): Graph normal_cell-0.3033127, scope n_stage_3, 7 nodes\n",
      "  (makrograph-edge(10,11)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): AdaptiveAvgPool2d(output_size=1)\n",
      "      (1): Flatten(start_dim=1, end_dim=-1)\n",
      "      (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = DARTSOptimizer(config)\n",
    "optimizer.adapt_search_space(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:24:40 nl.defaults.trainer]: \u001b[0mparam size = 0.236858MB\n",
      "\u001b[32m[06/25 15:24:40 nl.defaults.trainer]: \u001b[0mStart training\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/25 15:24:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000529, +0.001313, +0.000432, +0.000184, 1\n",
      "+0.000824, -0.001669, +0.000387, +0.000960, 3\n",
      "+0.001344, 0\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/25 15:24:44 nl.search_spaces.core.graph]: \u001b[0mComb_op is ignored if subgraph is defined!\n",
      "\u001b[32m[06/25 15:24:44 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.30296, validation loss: 2.30231, learning rate: [0.025]\n",
      "\u001b[32m[06/25 15:24:44 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    8960 KB |   21392 KB |   67056 KB |   58095 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |   41616 KB |   35472 KB |\n",
      "|       from small pool |    2816 KB |    4958 KB |   25440 KB |   22623 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    8960 KB |   21392 KB |   67056 KB |   58095 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |   41616 KB |   35472 KB |\n",
      "|       from small pool |    2816 KB |    4958 KB |   25440 KB |   22623 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   47104 KB |   47104 KB |   47104 KB |       0 B  |\n",
      "|       from large pool |   40960 KB |   40960 KB |   40960 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   17663 KB |   34383 KB |  104050 KB |   86386 KB |\n",
      "|       from large pool |   14336 KB |   30112 KB |   75024 KB |   60688 KB |\n",
      "|       from small pool |    3327 KB |    4281 KB |   29026 KB |   25698 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      86    |      93    |     409    |     323    |\n",
      "|       from large pool |       2    |       7    |      16    |      14    |\n",
      "|       from small pool |      84    |      87    |     393    |     309    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      86    |      93    |     409    |     323    |\n",
      "|       from large pool |       2    |       7    |      16    |      14    |\n",
      "|       from small pool |      84    |      87    |     393    |     309    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       5    |       5    |       5    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      14    |      14    |     143    |     129    |\n",
      "|       from large pool |       1    |       3    |       7    |       6    |\n",
      "|       from small pool |      13    |      13    |     136    |     123    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[06/25 15:24:44 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    8960 KB |   22342 KB |  132250 KB |  123290 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |   83232 KB |   77088 KB |\n",
      "|       from small pool |    2816 KB |    5907 KB |   49018 KB |   46202 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    8960 KB |   22342 KB |  132250 KB |  123290 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |   83232 KB |   77088 KB |\n",
      "|       from small pool |    2816 KB |    5907 KB |   49018 KB |   46202 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   49152 KB |   49152 KB |   49152 KB |       0 B  |\n",
      "|       from large pool |   40960 KB |   40960 KB |   40960 KB |       0 B  |\n",
      "|       from small pool |    8192 KB |    8192 KB |    8192 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   17663 KB |   34383 KB |  193884 KB |  176221 KB |\n",
      "|       from large pool |   14336 KB |   30112 KB |  138784 KB |  124448 KB |\n",
      "|       from small pool |    3327 KB |    4281 KB |   55100 KB |   51773 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      86    |     119    |     765    |     679    |\n",
      "|       from large pool |       2    |       7    |      32    |      30    |\n",
      "|       from small pool |      84    |     113    |     733    |     649    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      86    |     119    |     765    |     679    |\n",
      "|       from large pool |       2    |       7    |      32    |      30    |\n",
      "|       from small pool |      84    |     113    |     733    |     649    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       6    |       6    |       6    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       4    |       4    |       4    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      18    |     317    |     302    |\n",
      "|       from large pool |       1    |       3    |      14    |      13    |\n",
      "|       from small pool |      14    |      15    |     303    |     289    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[06/25 15:24:44 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    8960 KB |   22342 KB |  197445 KB |  188484 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |  124848 KB |  118704 KB |\n",
      "|       from small pool |    2816 KB |    5907 KB |   72597 KB |   69780 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    8960 KB |   22342 KB |  197445 KB |  188484 KB |\n",
      "|       from large pool |    6144 KB |   19504 KB |  124848 KB |  118704 KB |\n",
      "|       from small pool |    2816 KB |    5907 KB |   72597 KB |   69780 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   49152 KB |   49152 KB |   49152 KB |       0 B  |\n",
      "|       from large pool |   40960 KB |   40960 KB |   40960 KB |       0 B  |\n",
      "|       from small pool |    8192 KB |    8192 KB |    8192 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   17663 KB |   34383 KB |  283719 KB |  266055 KB |\n",
      "|       from large pool |   14336 KB |   30112 KB |  202544 KB |  188208 KB |\n",
      "|       from small pool |    3327 KB |    4281 KB |   81175 KB |   77847 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      86    |     119    |    1121    |    1035    |\n",
      "|       from large pool |       2    |       7    |      48    |      46    |\n",
      "|       from small pool |      84    |     113    |    1073    |     989    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      86    |     119    |    1121    |    1035    |\n",
      "|       from large pool |       2    |       7    |      48    |      46    |\n",
      "|       from small pool |      84    |     113    |    1073    |     989    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       6    |       6    |       6    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       4    |       4    |       4    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      18    |      18    |     490    |     472    |\n",
      "|       from large pool |       1    |       3    |      21    |      20    |\n",
      "|       from small pool |      17    |      17    |     469    |     452    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:24:49 nl.defaults.trainer]: \u001b[0mEpoch 0-30, Train loss: 2.30282, validation loss: 2.30232, learning rate: [0.025]\n",
      "\u001b[32m[06/25 15:24:54 nl.defaults.trainer]: \u001b[0mEpoch 0-60, Train loss: 2.30240, validation loss: 2.30287, learning rate: [0.025]\n",
      "\u001b[32m[06/25 15:24:59 nl.defaults.trainer]: \u001b[0mEpoch 0-90, Train loss: 2.30253, validation loss: 2.30225, learning rate: [0.025]\n",
      "\u001b[32m[06/25 15:25:04 nl.defaults.trainer]: \u001b[0mEpoch 0-120, Train loss: 2.30295, validation loss: 2.30237, learning rate: [0.025]\n",
      "\u001b[32m[06/25 15:25:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0119,  0.0039, -0.0163,  0.0211], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005,  0.0131, -0.0039, -0.0062], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-5.3323e-24], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:25:07 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 9.88286, 49.89429, Validation accuracy: 10.05646, 50.82687\n",
      "\u001b[32m[06/25 15:25:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.011851, +0.003920, -0.016251, +0.021122, 3\n",
      "+0.000490, +0.013142, -0.003860, -0.006205, 1\n",
      "-0.000000, 0\n",
      "\u001b[32m[06/25 15:25:09 nl.defaults.trainer]: \u001b[0mEpoch 1-12, Train loss: 2.30236, validation loss: 2.30231, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/25 15:25:14 nl.defaults.trainer]: \u001b[0mEpoch 1-42, Train loss: 2.30232, validation loss: 2.30229, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/25 15:25:19 nl.defaults.trainer]: \u001b[0mEpoch 1-72, Train loss: 2.30209, validation loss: 2.30282, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/25 15:25:24 nl.defaults.trainer]: \u001b[0mEpoch 1-101, Train loss: 2.30261, validation loss: 2.30283, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/25 15:25:29 nl.defaults.trainer]: \u001b[0mEpoch 1-131, Train loss: 2.30216, validation loss: 2.30254, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/25 15:25:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0163, -0.0113, -0.0320,  0.0508], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0006,  0.0063, -0.0080,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:25:30 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 11.07143, 52.37429, Validation accuracy: 11.21978, 52.42929\n",
      "\u001b[32m[06/25 15:25:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.016346, -0.011326, -0.032040, +0.050827, 3\n",
      "+0.000574, +0.006306, -0.007962, +0.000389, 1\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:25:34 nl.defaults.trainer]: \u001b[0mEpoch 2-23, Train loss: 2.30256, validation loss: 2.30188, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/25 15:25:39 nl.defaults.trainer]: \u001b[0mEpoch 2-53, Train loss: 2.30254, validation loss: 2.30222, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/25 15:25:45 nl.defaults.trainer]: \u001b[0mEpoch 2-83, Train loss: 2.30276, validation loss: 2.30211, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/25 15:25:50 nl.defaults.trainer]: \u001b[0mEpoch 2-113, Train loss: 2.30243, validation loss: 2.30233, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/25 15:25:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0428, -0.0483, -0.0667,  0.1181], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0187, -0.0045, -0.0212,  0.0348], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:25:54 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 11.78857, 55.12286, Validation accuracy: 11.73301, 55.12089\n",
      "\u001b[32m[06/25 15:25:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.042780, -0.048282, -0.066694, +0.118094, 3\n",
      "-0.018674, -0.004489, -0.021224, +0.034816, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:25:55 nl.defaults.trainer]: \u001b[0mEpoch 3-5, Train loss: 2.30192, validation loss: 2.30167, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/25 15:26:00 nl.defaults.trainer]: \u001b[0mEpoch 3-35, Train loss: 2.30076, validation loss: 2.30124, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/25 15:26:05 nl.defaults.trainer]: \u001b[0mEpoch 3-65, Train loss: 2.30035, validation loss: 2.30098, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/25 15:26:10 nl.defaults.trainer]: \u001b[0mEpoch 3-95, Train loss: 2.30015, validation loss: 2.29936, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/25 15:26:15 nl.defaults.trainer]: \u001b[0mEpoch 3-125, Train loss: 2.29639, validation loss: 2.29681, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/25 15:26:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1426, -0.1660, -0.1893,  0.2539], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1136, -0.0835, -0.0971,  0.1584], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:26:17 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 13.48286, 62.09143, Validation accuracy: 13.62625, 62.36029\n",
      "\u001b[32m[06/25 15:26:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.142617, -0.166025, -0.189254, +0.253943, 3\n",
      "-0.113573, -0.083479, -0.097080, +0.158429, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:26:20 nl.defaults.trainer]: \u001b[0mEpoch 4-17, Train loss: 2.29306, validation loss: 2.29463, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/25 15:26:25 nl.defaults.trainer]: \u001b[0mEpoch 4-47, Train loss: 2.26917, validation loss: 2.26054, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/25 15:26:30 nl.defaults.trainer]: \u001b[0mEpoch 4-77, Train loss: 2.26822, validation loss: 2.23538, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/25 15:26:35 nl.defaults.trainer]: \u001b[0mEpoch 4-107, Train loss: 2.23905, validation loss: 2.23149, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/25 15:26:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2663, -0.2845, -0.3037,  0.3735], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2188, -0.2137, -0.2146,  0.2812], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:26:41 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 19.35714, 69.02286, Validation accuracy: 19.13207, 68.76141\n",
      "\u001b[32m[06/25 15:26:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.266253, -0.284482, -0.303697, +0.373518, 3\n",
      "-0.218803, -0.213659, -0.214622, +0.281172, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:26:41 nl.defaults.trainer]: \u001b[0mEpoch 5-0, Train loss: 2.19612, validation loss: 2.23067, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/25 15:26:46 nl.defaults.trainer]: \u001b[0mEpoch 5-30, Train loss: 2.22912, validation loss: 2.16722, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/25 15:26:51 nl.defaults.trainer]: \u001b[0mEpoch 5-60, Train loss: 2.21832, validation loss: 2.18139, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/25 15:26:56 nl.defaults.trainer]: \u001b[0mEpoch 5-90, Train loss: 2.18325, validation loss: 2.18208, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/25 15:27:01 nl.defaults.trainer]: \u001b[0mEpoch 5-120, Train loss: 2.15109, validation loss: 2.16758, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/25 15:27:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3235, -0.3281, -0.3506,  0.4281], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2555, -0.2628, -0.2551,  0.3309], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:27:04 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 25.34000, 72.82000, Validation accuracy: 26.10059, 72.66195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:27:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.323537, -0.328100, -0.350614, +0.428123, 3\n",
      "-0.255544, -0.262764, -0.255105, +0.330919, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:27:06 nl.defaults.trainer]: \u001b[0mEpoch 6-13, Train loss: 2.16771, validation loss: 2.16863, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/25 15:27:11 nl.defaults.trainer]: \u001b[0mEpoch 6-43, Train loss: 2.16364, validation loss: 2.16331, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/25 15:27:16 nl.defaults.trainer]: \u001b[0mEpoch 6-73, Train loss: 2.15727, validation loss: 2.13947, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/25 15:27:21 nl.defaults.trainer]: \u001b[0mEpoch 6-103, Train loss: 2.17639, validation loss: 2.13943, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/25 15:27:26 nl.defaults.trainer]: \u001b[0mEpoch 6-133, Train loss: 2.13354, validation loss: 2.15798, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/25 15:27:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3274, -0.3312, -0.3461,  0.4349], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2630, -0.2761, -0.2460,  0.3402], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:27:27 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 28.38000, 74.49143, Validation accuracy: 28.07653, 74.12751\n",
      "\u001b[32m[06/25 15:27:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.327448, -0.331193, -0.346078, +0.434917, 3\n",
      "-0.262969, -0.276067, -0.245978, +0.340197, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:27:32 nl.defaults.trainer]: \u001b[0mEpoch 7-26, Train loss: 2.15563, validation loss: 2.16997, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/25 15:27:37 nl.defaults.trainer]: \u001b[0mEpoch 7-55, Train loss: 2.15377, validation loss: 2.16714, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/25 15:27:42 nl.defaults.trainer]: \u001b[0mEpoch 7-85, Train loss: 2.13837, validation loss: 2.17953, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/25 15:27:47 nl.defaults.trainer]: \u001b[0mEpoch 7-115, Train loss: 2.18604, validation loss: 2.13045, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/25 15:27:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3141, -0.3140, -0.3275,  0.4229], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2562, -0.2744, -0.2309,  0.3356], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:27:50 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 29.58571, 75.96571, Validation accuracy: 29.41093, 75.76414\n",
      "\u001b[32m[06/25 15:27:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.314095, -0.313965, -0.327500, +0.422855, 3\n",
      "-0.256219, -0.274427, -0.230877, +0.335565, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:27:52 nl.defaults.trainer]: \u001b[0mEpoch 8-8, Train loss: 2.11181, validation loss: 2.10729, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/25 15:27:57 nl.defaults.trainer]: \u001b[0mEpoch 8-37, Train loss: 2.15523, validation loss: 2.19350, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/25 15:28:02 nl.defaults.trainer]: \u001b[0mEpoch 8-67, Train loss: 2.13552, validation loss: 2.14872, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/25 15:28:07 nl.defaults.trainer]: \u001b[0mEpoch 8-97, Train loss: 2.15002, validation loss: 2.14238, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/25 15:28:12 nl.defaults.trainer]: \u001b[0mEpoch 8-127, Train loss: 2.16082, validation loss: 2.10699, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/25 15:28:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3104, -0.3095, -0.3250,  0.4238], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2678, -0.2808, -0.2361,  0.3493], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:28:14 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 31.37714, 77.09429, Validation accuracy: 31.13310, 76.79630\n",
      "\u001b[32m[06/25 15:28:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.310408, -0.309541, -0.324951, +0.423788, 3\n",
      "-0.267807, -0.280778, -0.236067, +0.349268, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:28:17 nl.defaults.trainer]: \u001b[0mEpoch 9-19, Train loss: 2.14793, validation loss: 2.12928, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/25 15:28:22 nl.defaults.trainer]: \u001b[0mEpoch 9-49, Train loss: 2.12407, validation loss: 2.11777, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/25 15:28:27 nl.defaults.trainer]: \u001b[0mEpoch 9-79, Train loss: 2.11730, validation loss: 2.10882, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/25 15:28:32 nl.defaults.trainer]: \u001b[0mEpoch 9-108, Train loss: 2.11146, validation loss: 2.08397, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/25 15:28:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3023, -0.2954, -0.3226,  0.4186], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2718, -0.2814, -0.2380,  0.3563], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:28:37 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 33.05714, 78.30000, Validation accuracy: 32.68704, 78.02521\n",
      "\u001b[32m[06/25 15:28:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.302309, -0.295353, -0.322637, +0.418639, 3\n",
      "-0.271821, -0.281436, -0.238046, +0.356293, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:28:37 nl.defaults.trainer]: \u001b[0mEpoch 10-1, Train loss: 2.14871, validation loss: 2.13772, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/25 15:28:42 nl.defaults.trainer]: \u001b[0mEpoch 10-31, Train loss: 2.04544, validation loss: 2.09336, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/25 15:28:47 nl.defaults.trainer]: \u001b[0mEpoch 10-61, Train loss: 2.12154, validation loss: 2.12659, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/25 15:28:53 nl.defaults.trainer]: \u001b[0mEpoch 10-91, Train loss: 2.07957, validation loss: 2.12478, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/25 15:28:58 nl.defaults.trainer]: \u001b[0mEpoch 10-121, Train loss: 2.12782, validation loss: 2.13029, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/25 15:29:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2962, -0.2816, -0.3181,  0.4127], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2824, -0.2831, -0.2363,  0.3648], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:29:00 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 34.14286, 79.67143, Validation accuracy: 34.07562, 79.68750\n",
      "\u001b[32m[06/25 15:29:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.296244, -0.281603, -0.318130, +0.412746, 3\n",
      "-0.282449, -0.283141, -0.236336, +0.364763, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:29:03 nl.defaults.trainer]: \u001b[0mEpoch 11-14, Train loss: 2.14070, validation loss: 2.11021, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/25 15:29:08 nl.defaults.trainer]: \u001b[0mEpoch 11-43, Train loss: 2.11859, validation loss: 2.10756, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/25 15:29:13 nl.defaults.trainer]: \u001b[0mEpoch 11-73, Train loss: 2.07731, validation loss: 2.10988, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/25 15:29:18 nl.defaults.trainer]: \u001b[0mEpoch 11-103, Train loss: 2.08745, validation loss: 2.12629, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/25 15:29:23 nl.defaults.trainer]: \u001b[0mEpoch 11-133, Train loss: 2.06607, validation loss: 2.10962, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/25 15:29:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2908, -0.2723, -0.3173,  0.4096], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2758, -0.2630, -0.2219,  0.3516], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:29:24 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 35.25429, 81.03429, Validation accuracy: 34.34934, 80.16081\n",
      "\u001b[32m[06/25 15:29:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.290776, -0.272266, -0.317259, +0.409648, 3\n",
      "-0.275819, -0.262987, -0.221904, +0.351612, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:29:28 nl.defaults.trainer]: \u001b[0mEpoch 12-26, Train loss: 2.12649, validation loss: 2.14147, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/25 15:29:33 nl.defaults.trainer]: \u001b[0mEpoch 12-56, Train loss: 2.04177, validation loss: 2.09675, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/25 15:29:38 nl.defaults.trainer]: \u001b[0mEpoch 12-85, Train loss: 2.15215, validation loss: 2.08459, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/25 15:29:44 nl.defaults.trainer]: \u001b[0mEpoch 12-115, Train loss: 2.09759, validation loss: 2.07379, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/25 15:29:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2692, -0.2522, -0.2984,  0.3888], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2790, -0.2542, -0.2140,  0.3493], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:29:47 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 35.58571, 81.72571, Validation accuracy: 35.18191, 81.13880\n",
      "\u001b[32m[06/25 15:29:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.269184, -0.252244, -0.298355, +0.388785, 3\n",
      "-0.278963, -0.254194, -0.214033, +0.349298, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:29:49 nl.defaults.trainer]: \u001b[0mEpoch 13-7, Train loss: 2.09273, validation loss: 2.08858, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/25 15:29:54 nl.defaults.trainer]: \u001b[0mEpoch 13-37, Train loss: 2.10573, validation loss: 2.10030, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/25 15:29:59 nl.defaults.trainer]: \u001b[0mEpoch 13-67, Train loss: 2.04977, validation loss: 2.11072, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/25 15:30:04 nl.defaults.trainer]: \u001b[0mEpoch 13-96, Train loss: 2.12644, validation loss: 2.07127, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/25 15:30:09 nl.defaults.trainer]: \u001b[0mEpoch 13-126, Train loss: 2.12379, validation loss: 2.07874, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/25 15:30:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2618, -0.2425, -0.2962,  0.3834], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2867, -0.2528, -0.2146,  0.3554], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:30:11 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 36.68000, 82.32286, Validation accuracy: 36.92404, 82.29357\n",
      "\u001b[32m[06/25 15:30:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.261783, -0.242539, -0.296187, +0.383419, 3\n",
      "-0.286702, -0.252820, -0.214564, +0.355430, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:30:14 nl.defaults.trainer]: \u001b[0mEpoch 14-18, Train loss: 2.09045, validation loss: 2.07042, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/25 15:30:19 nl.defaults.trainer]: \u001b[0mEpoch 14-48, Train loss: 2.05929, validation loss: 2.13460, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/25 15:30:24 nl.defaults.trainer]: \u001b[0mEpoch 14-77, Train loss: 2.09126, validation loss: 2.08454, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/25 15:30:30 nl.defaults.trainer]: \u001b[0mEpoch 14-107, Train loss: 2.10602, validation loss: 2.07504, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/25 15:30:35 nl.defaults.trainer]: \u001b[0mEpoch 14-136, Train loss: 2.06893, validation loss: 2.06680, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/25 15:30:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2580, -0.2331, -0.2889,  0.3774], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2816, -0.2413, -0.2196,  0.3533], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:30:35 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 37.47429, 83.35714, Validation accuracy: 37.45153, 83.26015\n",
      "\u001b[32m[06/25 15:30:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.258020, -0.233052, -0.288931, +0.377398, 3\n",
      "-0.281552, -0.241269, -0.219563, +0.353331, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:30:40 nl.defaults.trainer]: \u001b[0mEpoch 15-29, Train loss: 2.09173, validation loss: 2.15726, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/25 15:30:45 nl.defaults.trainer]: \u001b[0mEpoch 15-59, Train loss: 2.03494, validation loss: 2.04090, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/25 15:30:50 nl.defaults.trainer]: \u001b[0mEpoch 15-89, Train loss: 2.06481, validation loss: 2.05451, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/25 15:30:55 nl.defaults.trainer]: \u001b[0mEpoch 15-119, Train loss: 2.07183, validation loss: 2.03055, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/25 15:30:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2584, -0.2334, -0.2885,  0.3792], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2760, -0.2279, -0.2130,  0.3451], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:30:58 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 38.71143, 84.32000, Validation accuracy: 38.41811, 84.00433\n",
      "\u001b[32m[06/25 15:30:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.258439, -0.233403, -0.288511, +0.379182, 3\n",
      "-0.276019, -0.227950, -0.212967, +0.345118, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:31:00 nl.defaults.trainer]: \u001b[0mEpoch 16-11, Train loss: 2.08137, validation loss: 2.07388, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/25 15:31:05 nl.defaults.trainer]: \u001b[0mEpoch 16-41, Train loss: 2.07839, validation loss: 2.08753, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/25 15:31:10 nl.defaults.trainer]: \u001b[0mEpoch 16-70, Train loss: 2.05598, validation loss: 2.08377, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/25 15:31:15 nl.defaults.trainer]: \u001b[0mEpoch 16-100, Train loss: 2.04179, validation loss: 2.09005, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/25 15:31:20 nl.defaults.trainer]: \u001b[0mEpoch 16-129, Train loss: 1.99364, validation loss: 2.06745, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/25 15:31:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2401, -0.2213, -0.2856,  0.3676], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2807, -0.2235, -0.2147,  0.3485], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:31:22 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 39.70571, 85.50286, Validation accuracy: 38.78593, 84.73426\n",
      "\u001b[32m[06/25 15:31:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.240116, -0.221261, -0.285590, +0.367627, 3\n",
      "-0.280740, -0.223478, -0.214697, +0.348491, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:31:26 nl.defaults.trainer]: \u001b[0mEpoch 17-22, Train loss: 2.10526, validation loss: 2.04523, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/25 15:31:31 nl.defaults.trainer]: \u001b[0mEpoch 17-51, Train loss: 2.09957, validation loss: 2.10881, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/25 15:31:36 nl.defaults.trainer]: \u001b[0mEpoch 17-81, Train loss: 2.04934, validation loss: 2.07012, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/25 15:31:41 nl.defaults.trainer]: \u001b[0mEpoch 17-111, Train loss: 2.07891, validation loss: 2.07257, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/25 15:31:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2299, -0.2131, -0.2790,  0.3594], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2752, -0.2143, -0.2151,  0.3447], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:31:45 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 40.64571, 86.45714, Validation accuracy: 40.61930, 85.94035\n",
      "\u001b[32m[06/25 15:31:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.229880, -0.213105, -0.278955, +0.359372, 3\n",
      "-0.275161, -0.214319, -0.215124, +0.344731, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:31:46 nl.defaults.trainer]: \u001b[0mEpoch 18-3, Train loss: 2.03039, validation loss: 2.01571, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/25 15:31:51 nl.defaults.trainer]: \u001b[0mEpoch 18-33, Train loss: 2.01263, validation loss: 2.04525, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/25 15:31:56 nl.defaults.trainer]: \u001b[0mEpoch 18-62, Train loss: 2.04031, validation loss: 2.05108, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/25 15:32:01 nl.defaults.trainer]: \u001b[0mEpoch 18-92, Train loss: 2.02848, validation loss: 2.03425, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/25 15:32:06 nl.defaults.trainer]: \u001b[0mEpoch 18-122, Train loss: 2.02320, validation loss: 2.02833, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/25 15:32:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2220, -0.2079, -0.2765,  0.3545], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2730, -0.2119, -0.2185,  0.3464], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:32:09 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 41.42000, 86.97714, Validation accuracy: 41.40340, 86.84991\n",
      "\u001b[32m[06/25 15:32:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.222031, -0.207915, -0.276496, +0.354470, 3\n",
      "-0.273028, -0.211945, -0.218486, +0.346369, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:32:12 nl.defaults.trainer]: \u001b[0mEpoch 19-15, Train loss: 2.02163, validation loss: 2.07239, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/25 15:32:17 nl.defaults.trainer]: \u001b[0mEpoch 19-45, Train loss: 2.10785, validation loss: 2.03088, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/25 15:32:22 nl.defaults.trainer]: \u001b[0mEpoch 19-75, Train loss: 2.01297, validation loss: 2.00584, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/25 15:32:27 nl.defaults.trainer]: \u001b[0mEpoch 19-105, Train loss: 2.01320, validation loss: 2.01523, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/25 15:32:32 nl.defaults.trainer]: \u001b[0mEpoch 19-134, Train loss: 2.03431, validation loss: 2.03620, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/25 15:32:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2199, -0.2057, -0.2712,  0.3520], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2736, -0.2159, -0.2245,  0.3524], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:32:32 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 41.99143, 87.41714, Validation accuracy: 41.87671, 87.33177\n",
      "\u001b[32m[06/25 15:32:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.219910, -0.205702, -0.271168, +0.351969, 3\n",
      "-0.273563, -0.215868, -0.224483, +0.352414, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:32:37 nl.defaults.trainer]: \u001b[0mEpoch 20-27, Train loss: 2.05207, validation loss: 2.04979, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/25 15:32:42 nl.defaults.trainer]: \u001b[0mEpoch 20-57, Train loss: 1.95462, validation loss: 2.03160, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/25 15:32:47 nl.defaults.trainer]: \u001b[0mEpoch 20-86, Train loss: 2.05002, validation loss: 2.06773, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/25 15:32:52 nl.defaults.trainer]: \u001b[0mEpoch 20-116, Train loss: 2.02149, validation loss: 2.04599, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/25 15:32:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2151, -0.2001, -0.2592,  0.3449], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2667, -0.2146, -0.2312,  0.3536], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:32:56 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 42.90000, 88.09714, Validation accuracy: 42.72639, 87.99042\n",
      "\u001b[32m[06/25 15:32:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.215147, -0.200131, -0.259183, +0.344880, 3\n",
      "-0.266737, -0.214557, -0.231205, +0.353601, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:32:57 nl.defaults.trainer]: \u001b[0mEpoch 21-9, Train loss: 2.02894, validation loss: 2.07429, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/25 15:33:03 nl.defaults.trainer]: \u001b[0mEpoch 21-39, Train loss: 1.97205, validation loss: 2.03388, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/25 15:33:08 nl.defaults.trainer]: \u001b[0mEpoch 21-69, Train loss: 1.99095, validation loss: 1.98847, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/25 15:33:13 nl.defaults.trainer]: \u001b[0mEpoch 21-99, Train loss: 2.04990, validation loss: 2.01518, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/25 15:33:18 nl.defaults.trainer]: \u001b[0mEpoch 21-129, Train loss: 1.97991, validation loss: 2.00331, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/25 15:33:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2063, -0.1942, -0.2557,  0.3387], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2692, -0.2219, -0.2352,  0.3608], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:33:19 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy (top1, top5): 43.34000, 88.67714, Validation accuracy: 43.30520, 88.38675\n",
      "\u001b[32m[06/25 15:33:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.206297, -0.194172, -0.255670, +0.338707, 3\n",
      "-0.269229, -0.221931, -0.235217, +0.360780, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:33:23 nl.defaults.trainer]: \u001b[0mEpoch 22-22, Train loss: 2.02529, validation loss: 2.01858, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/25 15:33:28 nl.defaults.trainer]: \u001b[0mEpoch 22-51, Train loss: 2.00830, validation loss: 2.01626, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/25 15:33:33 nl.defaults.trainer]: \u001b[0mEpoch 22-81, Train loss: 2.06516, validation loss: 2.01174, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/25 15:33:38 nl.defaults.trainer]: \u001b[0mEpoch 22-111, Train loss: 2.02970, validation loss: 2.02696, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/25 15:33:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1918, -0.1816, -0.2466,  0.3258], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2655, -0.2153, -0.2397,  0.3602], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:33:43 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy (top1, top5): 44.21714, 89.14571, Validation accuracy: 43.45347, 88.74031\n",
      "\u001b[32m[06/25 15:33:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.191816, -0.181581, -0.246612, +0.325780, 3\n",
      "-0.265507, -0.215335, -0.239729, +0.360228, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:33:43 nl.defaults.trainer]: \u001b[0mEpoch 23-3, Train loss: 1.98873, validation loss: 2.00763, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/25 15:33:49 nl.defaults.trainer]: \u001b[0mEpoch 23-33, Train loss: 1.97276, validation loss: 2.00508, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/25 15:33:54 nl.defaults.trainer]: \u001b[0mEpoch 23-62, Train loss: 2.02952, validation loss: 2.03021, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/25 15:33:59 nl.defaults.trainer]: \u001b[0mEpoch 23-92, Train loss: 1.97464, validation loss: 2.05167, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/25 15:34:04 nl.defaults.trainer]: \u001b[0mEpoch 23-122, Train loss: 2.04171, validation loss: 2.03158, learning rate: [0.022001332835565518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:34:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1884, -0.1786, -0.2441,  0.3231], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2639, -0.2093, -0.2401,  0.3590], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:34:06 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy (top1, top5): 44.80571, 89.56857, Validation accuracy: 43.89256, 89.20506\n",
      "\u001b[32m[06/25 15:34:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.188415, -0.178568, -0.244143, +0.323110, 3\n",
      "-0.263902, -0.209335, -0.240081, +0.359000, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:34:09 nl.defaults.trainer]: \u001b[0mEpoch 24-15, Train loss: 2.01538, validation loss: 1.96943, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/25 15:34:14 nl.defaults.trainer]: \u001b[0mEpoch 24-44, Train loss: 2.00838, validation loss: 1.98100, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/25 15:34:19 nl.defaults.trainer]: \u001b[0mEpoch 24-74, Train loss: 1.95197, validation loss: 2.08771, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/25 15:34:24 nl.defaults.trainer]: \u001b[0mEpoch 24-104, Train loss: 1.97416, validation loss: 2.01608, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/25 15:34:29 nl.defaults.trainer]: \u001b[0mEpoch 24-134, Train loss: 1.99685, validation loss: 1.98713, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/25 15:34:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1732, -0.1626, -0.2334,  0.3078], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2641, -0.2026, -0.2466,  0.3607], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:34:30 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy (top1, top5): 45.28857, 89.63143, Validation accuracy: 44.26038, 89.44742\n",
      "\u001b[32m[06/25 15:34:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.173190, -0.162556, -0.233360, +0.307830, 3\n",
      "-0.264112, -0.202623, -0.246553, +0.360680, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:34:34 nl.defaults.trainer]: \u001b[0mEpoch 25-26, Train loss: 1.99531, validation loss: 1.99889, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/25 15:34:39 nl.defaults.trainer]: \u001b[0mEpoch 25-55, Train loss: 2.02411, validation loss: 2.02447, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/25 15:34:45 nl.defaults.trainer]: \u001b[0mEpoch 25-85, Train loss: 2.02415, validation loss: 1.98821, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/25 15:34:50 nl.defaults.trainer]: \u001b[0mEpoch 25-115, Train loss: 1.99433, validation loss: 1.97254, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/25 15:34:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1653, -0.1550, -0.2246,  0.2994], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2667, -0.1990, -0.2516,  0.3639], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:34:53 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy (top1, top5): 45.79714, 90.08000, Validation accuracy: 45.44366, 89.56432\n",
      "\u001b[32m[06/25 15:34:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.165250, -0.154998, -0.224566, +0.299351, 3\n",
      "-0.266721, -0.198982, -0.251624, +0.363923, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:34:55 nl.defaults.trainer]: \u001b[0mEpoch 26-8, Train loss: 2.00270, validation loss: 2.07419, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/25 15:35:00 nl.defaults.trainer]: \u001b[0mEpoch 26-38, Train loss: 2.00753, validation loss: 2.01907, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/25 15:35:05 nl.defaults.trainer]: \u001b[0mEpoch 26-68, Train loss: 1.98999, validation loss: 1.97152, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/25 15:35:10 nl.defaults.trainer]: \u001b[0mEpoch 26-98, Train loss: 2.06418, validation loss: 2.02499, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/25 15:35:15 nl.defaults.trainer]: \u001b[0mEpoch 26-128, Train loss: 2.01781, validation loss: 2.07090, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/25 15:35:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1531, -0.1477, -0.2178,  0.2899], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2694, -0.2001, -0.2557,  0.3685], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:35:17 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy (top1, top5): 46.27429, 89.92857, Validation accuracy: 45.40374, 89.77247\n",
      "\u001b[32m[06/25 15:35:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.153062, -0.147732, -0.217812, +0.289911, 3\n",
      "-0.269411, -0.200108, -0.255714, +0.368507, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:35:20 nl.defaults.trainer]: \u001b[0mEpoch 27-21, Train loss: 2.03581, validation loss: 1.99822, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/25 15:35:26 nl.defaults.trainer]: \u001b[0mEpoch 27-51, Train loss: 1.95357, validation loss: 2.05959, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/25 15:35:31 nl.defaults.trainer]: \u001b[0mEpoch 27-81, Train loss: 2.01020, validation loss: 2.03147, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/25 15:35:36 nl.defaults.trainer]: \u001b[0mEpoch 27-111, Train loss: 1.97077, validation loss: 1.95103, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/25 15:35:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1380, -0.1332, -0.2112,  0.2766], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2708, -0.1982, -0.2609,  0.3719], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:35:40 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy (top1, top5): 46.68000, 90.40571, Validation accuracy: 45.91412, 90.27714\n",
      "\u001b[32m[06/25 15:35:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.138030, -0.133204, -0.211241, +0.276649, 3\n",
      "-0.270782, -0.198166, -0.260876, +0.371851, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:35:41 nl.defaults.trainer]: \u001b[0mEpoch 28-3, Train loss: 1.96316, validation loss: 1.99368, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/25 15:35:46 nl.defaults.trainer]: \u001b[0mEpoch 28-33, Train loss: 1.96954, validation loss: 1.98251, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/25 15:35:51 nl.defaults.trainer]: \u001b[0mEpoch 28-63, Train loss: 1.94014, validation loss: 1.93692, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/25 15:35:56 nl.defaults.trainer]: \u001b[0mEpoch 28-93, Train loss: 1.91580, validation loss: 1.98952, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/25 15:36:01 nl.defaults.trainer]: \u001b[0mEpoch 28-123, Train loss: 2.02502, validation loss: 1.99624, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/25 15:36:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1271, -0.1276, -0.2010,  0.2671], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2672, -0.1952, -0.2585,  0.3697], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:36:03 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy (top1, top5): 47.51143, 90.51143, Validation accuracy: 47.11736, 90.47958\n",
      "\u001b[32m[06/25 15:36:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.127149, -0.127609, -0.201032, +0.267145, 3\n",
      "-0.267184, -0.195244, -0.258498, +0.369733, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:36:06 nl.defaults.trainer]: \u001b[0mEpoch 29-16, Train loss: 2.00761, validation loss: 2.00597, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/25 15:36:11 nl.defaults.trainer]: \u001b[0mEpoch 29-46, Train loss: 1.98532, validation loss: 1.99396, learning rate: [0.020354884643835724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:36:16 nl.defaults.trainer]: \u001b[0mEpoch 29-76, Train loss: 1.95150, validation loss: 1.98625, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/25 15:36:22 nl.defaults.trainer]: \u001b[0mEpoch 29-106, Train loss: 1.97860, validation loss: 2.00076, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/25 15:36:27 nl.defaults.trainer]: \u001b[0mEpoch 29-135, Train loss: 1.98700, validation loss: 2.02219, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/25 15:36:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1121, -0.1134, -0.1867,  0.2513], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2735, -0.1992, -0.2674,  0.3788], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:36:27 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy (top1, top5): 48.14000, 90.75714, Validation accuracy: 47.44240, 90.21156\n",
      "\u001b[32m[06/25 15:36:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.112051, -0.113417, -0.186662, +0.251310, 3\n",
      "-0.273527, -0.199224, -0.267422, +0.378801, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:36:32 nl.defaults.trainer]: \u001b[0mEpoch 30-28, Train loss: 1.93125, validation loss: 2.03408, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/25 15:36:37 nl.defaults.trainer]: \u001b[0mEpoch 30-57, Train loss: 2.01936, validation loss: 1.96387, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/25 15:36:42 nl.defaults.trainer]: \u001b[0mEpoch 30-87, Train loss: 1.97828, validation loss: 1.98828, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/25 15:36:47 nl.defaults.trainer]: \u001b[0mEpoch 30-116, Train loss: 1.94298, validation loss: 2.00275, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/25 15:36:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1045, -0.0999, -0.1852,  0.2430], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2741, -0.1958, -0.2711,  0.3806], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:36:50 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy (top1, top5): 48.21429, 90.90000, Validation accuracy: 47.55360, 90.65351\n",
      "\u001b[32m[06/25 15:36:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.104471, -0.099931, -0.185175, +0.242955, 3\n",
      "-0.274112, -0.195824, -0.271071, +0.380599, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:36:52 nl.defaults.trainer]: \u001b[0mEpoch 31-9, Train loss: 1.97588, validation loss: 2.00227, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/25 15:36:57 nl.defaults.trainer]: \u001b[0mEpoch 31-39, Train loss: 1.98911, validation loss: 2.02002, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/25 15:37:02 nl.defaults.trainer]: \u001b[0mEpoch 31-69, Train loss: 1.93365, validation loss: 1.92915, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/25 15:37:08 nl.defaults.trainer]: \u001b[0mEpoch 31-99, Train loss: 1.93500, validation loss: 1.98111, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/25 15:37:13 nl.defaults.trainer]: \u001b[0mEpoch 31-128, Train loss: 1.97071, validation loss: 1.98942, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/25 15:37:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0951, -0.0920, -0.1773,  0.2338], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2688, -0.1905, -0.2709,  0.3779], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:37:14 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy (top1, top5): 48.96571, 91.10857, Validation accuracy: 48.39758, 91.08406\n",
      "\u001b[32m[06/25 15:37:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.095122, -0.091981, -0.177292, +0.233808, 3\n",
      "-0.268844, -0.190518, -0.270941, +0.377862, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:37:18 nl.defaults.trainer]: \u001b[0mEpoch 32-20, Train loss: 1.97226, validation loss: 2.02227, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/25 15:37:23 nl.defaults.trainer]: \u001b[0mEpoch 32-50, Train loss: 1.94404, validation loss: 1.94802, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/25 15:37:28 nl.defaults.trainer]: \u001b[0mEpoch 32-80, Train loss: 1.97025, validation loss: 1.95167, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/25 15:37:33 nl.defaults.trainer]: \u001b[0mEpoch 32-109, Train loss: 1.93560, validation loss: 1.98135, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/25 15:37:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1027, -0.0954, -0.1805,  0.2395], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2691, -0.1877, -0.2706,  0.3780], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:37:38 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy (top1, top5): 49.62857, 91.37714, Validation accuracy: 48.67701, 90.93294\n",
      "\u001b[32m[06/25 15:37:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.102695, -0.095442, -0.180491, +0.239483, 3\n",
      "-0.269110, -0.187679, -0.270593, +0.377993, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:37:38 nl.defaults.trainer]: \u001b[0mEpoch 33-2, Train loss: 1.99365, validation loss: 1.99596, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/25 15:37:43 nl.defaults.trainer]: \u001b[0mEpoch 33-32, Train loss: 1.95531, validation loss: 1.93314, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/25 15:37:48 nl.defaults.trainer]: \u001b[0mEpoch 33-62, Train loss: 1.96101, validation loss: 1.98867, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/25 15:37:53 nl.defaults.trainer]: \u001b[0mEpoch 33-92, Train loss: 1.95507, validation loss: 1.97351, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/25 15:37:59 nl.defaults.trainer]: \u001b[0mEpoch 33-122, Train loss: 1.95818, validation loss: 1.97310, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/25 15:38:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0903, -0.0922, -0.1705,  0.2301], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2679, -0.1861, -0.2736,  0.3794], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:38:01 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy (top1, top5): 49.76571, 91.44857, Validation accuracy: 48.66275, 91.08406\n",
      "\u001b[32m[06/25 15:38:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.090276, -0.092171, -0.170485, +0.230065, 3\n",
      "-0.267931, -0.186095, -0.273614, +0.379363, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:38:04 nl.defaults.trainer]: \u001b[0mEpoch 34-14, Train loss: 1.95029, validation loss: 1.98787, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/25 15:38:09 nl.defaults.trainer]: \u001b[0mEpoch 34-44, Train loss: 1.93312, validation loss: 1.96182, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/25 15:38:14 nl.defaults.trainer]: \u001b[0mEpoch 34-74, Train loss: 1.93262, validation loss: 1.96431, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/25 15:38:19 nl.defaults.trainer]: \u001b[0mEpoch 34-104, Train loss: 1.95047, validation loss: 1.94549, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/25 15:38:24 nl.defaults.trainer]: \u001b[0mEpoch 34-134, Train loss: 1.97804, validation loss: 1.95110, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/25 15:38:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0895, -0.0868, -0.1661,  0.2265], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2685, -0.1859, -0.2721,  0.3801], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:38:24 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy (top1, top5): 50.53143, 91.77429, Validation accuracy: 49.42119, 91.72274\n",
      "\u001b[32m[06/25 15:38:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.089456, -0.086846, -0.166087, +0.226517, 3\n",
      "-0.268539, -0.185934, -0.272110, +0.380117, 3\n",
      "+0.000000, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:38:29 nl.defaults.trainer]: \u001b[0mEpoch 35-26, Train loss: 1.93914, validation loss: 1.97736, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/25 15:38:34 nl.defaults.trainer]: \u001b[0mEpoch 35-55, Train loss: 1.95166, validation loss: 1.93465, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/25 15:38:39 nl.defaults.trainer]: \u001b[0mEpoch 35-85, Train loss: 1.96271, validation loss: 1.97201, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/25 15:38:44 nl.defaults.trainer]: \u001b[0mEpoch 35-115, Train loss: 1.86687, validation loss: 1.92503, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/25 15:38:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0832, -0.0845, -0.1633,  0.2223], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2610, -0.1811, -0.2670,  0.3745], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:38:48 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy (top1, top5): 50.80857, 91.79714, Validation accuracy: 49.73198, 91.40625\n",
      "\u001b[32m[06/25 15:38:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.083204, -0.084514, -0.163267, +0.222250, 3\n",
      "-0.260957, -0.181071, -0.267036, +0.374482, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:38:49 nl.defaults.trainer]: \u001b[0mEpoch 36-8, Train loss: 1.92004, validation loss: 1.96099, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/25 15:38:54 nl.defaults.trainer]: \u001b[0mEpoch 36-37, Train loss: 1.93625, validation loss: 1.94611, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/25 15:39:00 nl.defaults.trainer]: \u001b[0mEpoch 36-67, Train loss: 1.95276, validation loss: 1.93962, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/25 15:39:05 nl.defaults.trainer]: \u001b[0mEpoch 36-97, Train loss: 2.00695, validation loss: 1.94247, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/25 15:39:10 nl.defaults.trainer]: \u001b[0mEpoch 36-127, Train loss: 1.94717, validation loss: 1.93500, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/25 15:39:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0773, -0.0766, -0.1617,  0.2163], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2576, -0.1752, -0.2668,  0.3719], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:39:11 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy (top1, top5): 51.06857, 91.99143, Validation accuracy: 49.98859, 91.85960\n",
      "\u001b[32m[06/25 15:39:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.077251, -0.076588, -0.161673, +0.216343, 3\n",
      "-0.257631, -0.175172, -0.266823, +0.371853, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:39:15 nl.defaults.trainer]: \u001b[0mEpoch 37-19, Train loss: 1.94020, validation loss: 1.98592, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/25 15:39:20 nl.defaults.trainer]: \u001b[0mEpoch 37-48, Train loss: 1.94733, validation loss: 1.94878, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/25 15:39:25 nl.defaults.trainer]: \u001b[0mEpoch 37-78, Train loss: 1.99579, validation loss: 2.00502, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/25 15:39:30 nl.defaults.trainer]: \u001b[0mEpoch 37-108, Train loss: 1.94299, validation loss: 1.97310, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/25 15:39:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0767, -0.0654, -0.1628,  0.2123], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2600, -0.1764, -0.2703,  0.3757], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:39:35 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy (top1, top5): 51.77429, 92.11143, Validation accuracy: 50.80976, 91.98791\n",
      "\u001b[32m[06/25 15:39:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.076690, -0.065361, -0.162822, +0.212334, 3\n",
      "-0.259968, -0.176420, -0.270311, +0.375666, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:39:35 nl.defaults.trainer]: \u001b[0mEpoch 38-0, Train loss: 1.94260, validation loss: 1.91214, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/25 15:39:40 nl.defaults.trainer]: \u001b[0mEpoch 38-30, Train loss: 1.88226, validation loss: 1.93372, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/25 15:39:45 nl.defaults.trainer]: \u001b[0mEpoch 38-60, Train loss: 1.90672, validation loss: 1.97378, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/25 15:39:50 nl.defaults.trainer]: \u001b[0mEpoch 38-90, Train loss: 1.95182, validation loss: 1.95383, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/25 15:39:56 nl.defaults.trainer]: \u001b[0mEpoch 38-120, Train loss: 1.92951, validation loss: 1.97142, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/25 15:39:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0732, -0.0670, -0.1607,  0.2109], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2572, -0.1727, -0.2655,  0.3723], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:39:58 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy (top1, top5): 52.20857, 92.14286, Validation accuracy: 51.09489, 91.92803\n",
      "\u001b[32m[06/25 15:39:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.073200, -0.066983, -0.160725, +0.210907, 3\n",
      "-0.257194, -0.172672, -0.265467, +0.372271, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:40:01 nl.defaults.trainer]: \u001b[0mEpoch 39-12, Train loss: 1.99483, validation loss: 1.91479, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/25 15:40:06 nl.defaults.trainer]: \u001b[0mEpoch 39-42, Train loss: 1.95692, validation loss: 1.96122, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/25 15:40:11 nl.defaults.trainer]: \u001b[0mEpoch 39-72, Train loss: 1.93016, validation loss: 1.97615, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/25 15:40:16 nl.defaults.trainer]: \u001b[0mEpoch 39-101, Train loss: 1.89960, validation loss: 1.96283, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/25 15:40:21 nl.defaults.trainer]: \u001b[0mEpoch 39-131, Train loss: 1.91208, validation loss: 1.94531, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/25 15:40:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0630, -0.0649, -0.1526,  0.2031], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2596, -0.1735, -0.2700,  0.3763], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:40:22 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy (top1, top5): 52.32286, 92.41714, Validation accuracy: 51.59672, 92.36713\n",
      "\u001b[32m[06/25 15:40:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.063043, -0.064879, -0.152597, +0.203102, 3\n",
      "-0.259572, -0.173455, -0.270007, +0.376266, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:40:26 nl.defaults.trainer]: \u001b[0mEpoch 40-24, Train loss: 1.91471, validation loss: 1.94195, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/25 15:40:31 nl.defaults.trainer]: \u001b[0mEpoch 40-54, Train loss: 1.95159, validation loss: 1.91695, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/25 15:40:36 nl.defaults.trainer]: \u001b[0mEpoch 40-84, Train loss: 1.89920, validation loss: 1.98341, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/25 15:40:41 nl.defaults.trainer]: \u001b[0mEpoch 40-114, Train loss: 1.84807, validation loss: 1.91378, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/25 15:40:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0536, -0.0565, -0.1485,  0.1943], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2648, -0.1756, -0.2743,  0.3817], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:40:45 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy (top1, top5): 52.67143, 92.34571, Validation accuracy: 51.38572, 92.10196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:40:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.053584, -0.056476, -0.148490, +0.194349, 3\n",
      "-0.264828, -0.175606, -0.274311, +0.381728, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:40:46 nl.defaults.trainer]: \u001b[0mEpoch 41-7, Train loss: 1.94172, validation loss: 1.98317, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/25 15:40:51 nl.defaults.trainer]: \u001b[0mEpoch 41-36, Train loss: 1.93593, validation loss: 1.95087, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/25 15:40:57 nl.defaults.trainer]: \u001b[0mEpoch 41-65, Train loss: 1.95202, validation loss: 1.93426, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/25 15:41:02 nl.defaults.trainer]: \u001b[0mEpoch 41-94, Train loss: 1.89505, validation loss: 1.94322, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/25 15:41:07 nl.defaults.trainer]: \u001b[0mEpoch 41-124, Train loss: 1.92207, validation loss: 1.91342, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/25 15:41:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0614, -0.0594, -0.1464,  0.1985], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2656, -0.1727, -0.2736,  0.3816], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:41:09 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy (top1, top5): 53.27429, 92.48286, Validation accuracy: 52.49487, 92.48403\n",
      "\u001b[32m[06/25 15:41:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.061361, -0.059436, -0.146414, +0.198503, 3\n",
      "-0.265564, -0.172729, -0.273561, +0.381560, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:41:12 nl.defaults.trainer]: \u001b[0mEpoch 42-16, Train loss: 1.92960, validation loss: 2.01759, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/25 15:41:17 nl.defaults.trainer]: \u001b[0mEpoch 42-46, Train loss: 1.89277, validation loss: 1.95488, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/25 15:41:22 nl.defaults.trainer]: \u001b[0mEpoch 42-76, Train loss: 1.92923, validation loss: 1.93206, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/25 15:41:27 nl.defaults.trainer]: \u001b[0mEpoch 42-106, Train loss: 1.89732, validation loss: 1.91124, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/25 15:41:32 nl.defaults.trainer]: \u001b[0mEpoch 42-136, Train loss: 1.89687, validation loss: 1.91475, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/25 15:41:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0578, -0.0600, -0.1437,  0.1964], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2691, -0.1715, -0.2760,  0.3843], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:41:32 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy (top1, top5): 53.67714, 92.58857, Validation accuracy: 52.48061, 92.54391\n",
      "\u001b[32m[06/25 15:41:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.057820, -0.060027, -0.143729, +0.196383, 3\n",
      "-0.269141, -0.171505, -0.276016, +0.384333, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:41:37 nl.defaults.trainer]: \u001b[0mEpoch 43-29, Train loss: 1.86724, validation loss: 1.92395, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/25 15:41:43 nl.defaults.trainer]: \u001b[0mEpoch 43-59, Train loss: 1.92496, validation loss: 1.91270, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/25 15:41:48 nl.defaults.trainer]: \u001b[0mEpoch 43-89, Train loss: 1.94476, validation loss: 1.96623, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/25 15:41:53 nl.defaults.trainer]: \u001b[0mEpoch 43-119, Train loss: 1.90918, validation loss: 1.91946, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/25 15:41:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0594, -0.0622, -0.1397,  0.1968], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2732, -0.1698, -0.2814,  0.3882], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:41:56 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy (top1, top5): 53.48286, 92.54571, Validation accuracy: 52.12420, 92.60949\n",
      "\u001b[32m[06/25 15:41:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.059397, -0.062206, -0.139685, +0.196781, 3\n",
      "-0.273225, -0.169757, -0.281398, +0.388161, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:41:58 nl.defaults.trainer]: \u001b[0mEpoch 44-11, Train loss: 1.88498, validation loss: 1.97117, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/25 15:42:03 nl.defaults.trainer]: \u001b[0mEpoch 44-41, Train loss: 1.90030, validation loss: 1.90202, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/25 15:42:08 nl.defaults.trainer]: \u001b[0mEpoch 44-70, Train loss: 1.90785, validation loss: 1.98234, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/25 15:42:13 nl.defaults.trainer]: \u001b[0mEpoch 44-100, Train loss: 1.91389, validation loss: 1.88837, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/25 15:42:18 nl.defaults.trainer]: \u001b[0mEpoch 44-129, Train loss: 1.94892, validation loss: 1.97704, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/25 15:42:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0553, -0.0557, -0.1352,  0.1910], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2773, -0.1670, -0.2822,  0.3898], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:42:19 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy (top1, top5): 53.75429, 92.79143, Validation accuracy: 53.37021, 92.64656\n",
      "\u001b[32m[06/25 15:42:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.055294, -0.055693, -0.135153, +0.190998, 3\n",
      "-0.277261, -0.167024, -0.282187, +0.389791, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:42:23 nl.defaults.trainer]: \u001b[0mEpoch 45-22, Train loss: 1.97357, validation loss: 1.95487, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/25 15:42:28 nl.defaults.trainer]: \u001b[0mEpoch 45-51, Train loss: 1.91154, validation loss: 1.91415, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/25 15:42:33 nl.defaults.trainer]: \u001b[0mEpoch 45-81, Train loss: 1.87412, validation loss: 1.97903, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/25 15:42:38 nl.defaults.trainer]: \u001b[0mEpoch 45-110, Train loss: 1.92279, validation loss: 1.93900, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/25 15:42:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0537, -0.0561, -0.1308,  0.1891], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2733, -0.1627, -0.2766,  0.3855], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:42:43 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy (top1, top5): 54.40857, 92.76857, Validation accuracy: 53.29608, 92.72924\n",
      "\u001b[32m[06/25 15:42:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.053680, -0.056138, -0.130812, +0.189106, 3\n",
      "-0.273259, -0.162746, -0.276617, +0.385470, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:42:43 nl.defaults.trainer]: \u001b[0mEpoch 46-2, Train loss: 1.86425, validation loss: 1.94731, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/25 15:42:48 nl.defaults.trainer]: \u001b[0mEpoch 46-32, Train loss: 1.90427, validation loss: 1.93712, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/25 15:42:54 nl.defaults.trainer]: \u001b[0mEpoch 46-62, Train loss: 1.91625, validation loss: 1.96853, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/25 15:42:59 nl.defaults.trainer]: \u001b[0mEpoch 46-92, Train loss: 1.93319, validation loss: 1.88737, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/25 15:43:04 nl.defaults.trainer]: \u001b[0mEpoch 46-122, Train loss: 1.88723, validation loss: 1.93398, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/25 15:43:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0472, -0.0518, -0.1277,  0.1834], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2704, -0.1547, -0.2745,  0.3812], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:43:06 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy (top1, top5): 54.57143, 92.97429, Validation accuracy: 53.84352, 92.83474\n",
      "\u001b[32m[06/25 15:43:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.047159, -0.051820, -0.127711, +0.183417, 3\n",
      "-0.270356, -0.154651, -0.274472, +0.381169, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:43:09 nl.defaults.trainer]: \u001b[0mEpoch 47-15, Train loss: 1.85986, validation loss: 1.91954, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/25 15:43:14 nl.defaults.trainer]: \u001b[0mEpoch 47-45, Train loss: 1.90863, validation loss: 1.91863, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/25 15:43:19 nl.defaults.trainer]: \u001b[0mEpoch 47-75, Train loss: 1.94274, validation loss: 1.89696, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/25 15:43:24 nl.defaults.trainer]: \u001b[0mEpoch 47-105, Train loss: 1.90150, validation loss: 1.89596, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/25 15:43:30 nl.defaults.trainer]: \u001b[0mEpoch 47-134, Train loss: 1.89190, validation loss: 1.91822, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/25 15:43:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0377, -0.0488, -0.1248,  0.1769], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2663, -0.1538, -0.2725,  0.3795], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:43:30 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy (top1, top5): 55.00286, 93.12286, Validation accuracy: 53.52703, 92.85185\n",
      "\u001b[32m[06/25 15:43:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.037650, -0.048793, -0.124819, +0.176902, 3\n",
      "-0.266348, -0.153780, -0.272526, +0.379528, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:43:35 nl.defaults.trainer]: \u001b[0mEpoch 48-27, Train loss: 1.96206, validation loss: 1.93038, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/25 15:43:40 nl.defaults.trainer]: \u001b[0mEpoch 48-57, Train loss: 1.91599, validation loss: 1.92403, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/25 15:43:45 nl.defaults.trainer]: \u001b[0mEpoch 48-87, Train loss: 1.92184, validation loss: 1.88306, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/25 15:43:50 nl.defaults.trainer]: \u001b[0mEpoch 48-117, Train loss: 1.93007, validation loss: 1.90161, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/25 15:43:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0356, -0.0438, -0.1160,  0.1712], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2622, -0.1517, -0.2680,  0.3764], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:43:53 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy (top1, top5): 55.06857, 93.25714, Validation accuracy: 53.76654, 93.13127\n",
      "\u001b[32m[06/25 15:43:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.035611, -0.043824, -0.116040, +0.171214, 3\n",
      "-0.262206, -0.151725, -0.267998, +0.376383, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:43:55 nl.defaults.trainer]: \u001b[0mEpoch 49-9, Train loss: 1.90626, validation loss: 1.91622, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/25 15:44:00 nl.defaults.trainer]: \u001b[0mEpoch 49-39, Train loss: 1.93904, validation loss: 1.90483, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/25 15:44:05 nl.defaults.trainer]: \u001b[0mEpoch 49-69, Train loss: 1.92618, validation loss: 1.91390, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/25 15:44:10 nl.defaults.trainer]: \u001b[0mEpoch 49-99, Train loss: 1.92162, validation loss: 1.88980, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/25 15:44:15 nl.defaults.trainer]: \u001b[0mEpoch 49-129, Train loss: 1.91500, validation loss: 1.94235, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/25 15:44:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0338, -0.0460, -0.1153,  0.1710], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2627, -0.1552, -0.2714,  0.3801], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:44:17 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy (top1, top5): 55.39714, 93.35714, Validation accuracy: 53.98894, 92.89177\n",
      "\u001b[32m[06/25 15:44:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.033804, -0.045961, -0.115325, +0.171009, 3\n",
      "-0.262717, -0.155163, -0.271442, +0.380108, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:44:21 nl.defaults.trainer]: \u001b[0mEpoch 50-22, Train loss: 1.91753, validation loss: 1.94670, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/25 15:44:26 nl.defaults.trainer]: \u001b[0mEpoch 50-51, Train loss: 1.87991, validation loss: 1.92030, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/25 15:44:31 nl.defaults.trainer]: \u001b[0mEpoch 50-81, Train loss: 1.87620, validation loss: 1.91296, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/25 15:44:36 nl.defaults.trainer]: \u001b[0mEpoch 50-111, Train loss: 1.94129, validation loss: 1.86481, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/25 15:44:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0206, -0.0316, -0.1076,  0.1565], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2668, -0.1593, -0.2789,  0.3868], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:44:40 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy (top1, top5): 55.83429, 93.31143, Validation accuracy: 54.37101, 92.99441\n",
      "\u001b[32m[06/25 15:44:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.020561, -0.031559, -0.107562, +0.156535, 3\n",
      "-0.266827, -0.159257, -0.278866, +0.386834, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:44:41 nl.defaults.trainer]: \u001b[0mEpoch 51-4, Train loss: 1.89947, validation loss: 1.94047, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/25 15:44:46 nl.defaults.trainer]: \u001b[0mEpoch 51-34, Train loss: 1.89545, validation loss: 1.92275, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/25 15:44:51 nl.defaults.trainer]: \u001b[0mEpoch 51-64, Train loss: 1.88965, validation loss: 1.90183, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/25 15:44:56 nl.defaults.trainer]: \u001b[0mEpoch 51-94, Train loss: 1.87335, validation loss: 1.88699, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/25 15:45:01 nl.defaults.trainer]: \u001b[0mEpoch 51-123, Train loss: 1.88040, validation loss: 1.90722, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/25 15:45:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0141, -0.0181, -0.1035,  0.1467], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2589, -0.1512, -0.2776,  0.3810], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:45:03 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy (top1, top5): 56.01429, 93.39429, Validation accuracy: 54.37671, 93.04003\n",
      "\u001b[32m[06/25 15:45:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.014055, -0.018079, -0.103518, +0.146718, 3\n",
      "-0.258909, -0.151241, -0.277553, +0.380964, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:45:06 nl.defaults.trainer]: \u001b[0mEpoch 52-16, Train loss: 1.82859, validation loss: 1.93675, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/25 15:45:11 nl.defaults.trainer]: \u001b[0mEpoch 52-46, Train loss: 1.87746, validation loss: 1.93771, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/25 15:45:16 nl.defaults.trainer]: \u001b[0mEpoch 52-76, Train loss: 1.90455, validation loss: 1.85580, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/25 15:45:21 nl.defaults.trainer]: \u001b[0mEpoch 52-106, Train loss: 1.91174, validation loss: 1.89779, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/25 15:45:26 nl.defaults.trainer]: \u001b[0mEpoch 52-136, Train loss: 1.85624, validation loss: 1.90774, learning rate: [0.012246513765648233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:45:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0054, -0.0084, -0.0959,  0.1362], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2535, -0.1526, -0.2721,  0.3785], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:45:26 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy (top1, top5): 56.21143, 93.44286, Validation accuracy: 55.15511, 93.09991\n",
      "\u001b[32m[06/25 15:45:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.005351, -0.008439, -0.095880, +0.136159, 3\n",
      "-0.253488, -0.152644, -0.272052, +0.378470, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:45:32 nl.defaults.trainer]: \u001b[0mEpoch 53-29, Train loss: 1.90002, validation loss: 1.94100, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/25 15:45:37 nl.defaults.trainer]: \u001b[0mEpoch 53-59, Train loss: 1.90672, validation loss: 1.93720, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/25 15:45:42 nl.defaults.trainer]: \u001b[0mEpoch 53-89, Train loss: 1.88146, validation loss: 1.86889, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/25 15:45:47 nl.defaults.trainer]: \u001b[0mEpoch 53-119, Train loss: 1.91646, validation loss: 1.89638, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/25 15:45:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0003, -0.0096, -0.0989,  0.1352], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2499, -0.1476, -0.2730,  0.3761], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:45:50 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy (top1, top5): 56.90571, 93.52571, Validation accuracy: 55.59135, 93.31661\n",
      "\u001b[32m[06/25 15:45:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000280, -0.009560, -0.098916, +0.135202, 3\n",
      "-0.249850, -0.147580, -0.272957, +0.376137, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:45:52 nl.defaults.trainer]: \u001b[0mEpoch 54-12, Train loss: 1.84690, validation loss: 1.91660, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/25 15:45:57 nl.defaults.trainer]: \u001b[0mEpoch 54-42, Train loss: 1.86918, validation loss: 1.93333, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/25 15:46:02 nl.defaults.trainer]: \u001b[0mEpoch 54-72, Train loss: 1.87248, validation loss: 1.87977, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/25 15:46:07 nl.defaults.trainer]: \u001b[0mEpoch 54-102, Train loss: 1.83575, validation loss: 1.94717, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/25 15:46:12 nl.defaults.trainer]: \u001b[0mEpoch 54-132, Train loss: 1.91075, validation loss: 1.90481, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/25 15:46:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0076, -0.0048, -0.1002,  0.1299], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2506, -0.1488, -0.2726,  0.3776], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:46:13 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy (top1, top5): 56.88286, 93.30571, Validation accuracy: 55.53148, 93.48483\n",
      "\u001b[32m[06/25 15:46:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.007586, -0.004819, -0.100200, +0.129859, 3\n",
      "-0.250616, -0.148804, -0.272626, +0.377628, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:46:17 nl.defaults.trainer]: \u001b[0mEpoch 55-24, Train loss: 1.87176, validation loss: 1.94211, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/25 15:46:22 nl.defaults.trainer]: \u001b[0mEpoch 55-54, Train loss: 1.87945, validation loss: 1.91590, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/25 15:46:27 nl.defaults.trainer]: \u001b[0mEpoch 55-84, Train loss: 1.91355, validation loss: 1.94137, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/25 15:46:33 nl.defaults.trainer]: \u001b[0mEpoch 55-114, Train loss: 1.89394, validation loss: 1.86095, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/25 15:46:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 1.2529e-02, -1.0327e-04, -9.7473e-02,  1.2450e-01], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2553, -0.1508, -0.2730,  0.3811], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:46:36 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy (top1, top5): 56.96857, 93.68571, Validation accuracy: 55.85082, 93.41355\n",
      "\u001b[32m[06/25 15:46:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.012529, -0.000103, -0.097473, +0.124504, 3\n",
      "-0.255254, -0.150795, -0.273014, +0.381059, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:46:38 nl.defaults.trainer]: \u001b[0mEpoch 56-7, Train loss: 1.91148, validation loss: 1.92678, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/25 15:46:43 nl.defaults.trainer]: \u001b[0mEpoch 56-36, Train loss: 1.85928, validation loss: 1.91216, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/25 15:46:48 nl.defaults.trainer]: \u001b[0mEpoch 56-66, Train loss: 1.88574, validation loss: 1.86078, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/25 15:46:53 nl.defaults.trainer]: \u001b[0mEpoch 56-96, Train loss: 1.89878, validation loss: 1.94258, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/25 15:46:58 nl.defaults.trainer]: \u001b[0mEpoch 56-126, Train loss: 1.84776, validation loss: 1.87640, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/25 15:47:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0173, -0.0033, -0.0969,  0.1233], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2559, -0.1546, -0.2754,  0.3845], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:47:00 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy (top1, top5): 57.53714, 93.72571, Validation accuracy: 56.00479, 93.36223\n",
      "\u001b[32m[06/25 15:47:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.017259, -0.003283, -0.096879, +0.123312, 3\n",
      "-0.255890, -0.154638, -0.275358, +0.384525, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:47:03 nl.defaults.trainer]: \u001b[0mEpoch 57-18, Train loss: 1.89356, validation loss: 1.88323, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/25 15:47:08 nl.defaults.trainer]: \u001b[0mEpoch 57-48, Train loss: 1.85836, validation loss: 1.90349, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/25 15:47:13 nl.defaults.trainer]: \u001b[0mEpoch 57-78, Train loss: 1.86371, validation loss: 1.89667, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/25 15:47:19 nl.defaults.trainer]: \u001b[0mEpoch 57-108, Train loss: 1.88373, validation loss: 1.91084, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/25 15:47:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0166, -0.0014, -0.0983,  0.1232], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2648, -0.1562, -0.2831,  0.3919], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:47:23 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy (top1, top5): 57.67429, 93.65714, Validation accuracy: 56.21293, 93.49053\n",
      "\u001b[32m[06/25 15:47:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.016578, -0.001376, -0.098261, +0.123225, 3\n",
      "-0.264804, -0.156169, -0.283133, +0.391936, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:47:24 nl.defaults.trainer]: \u001b[0mEpoch 58-1, Train loss: 1.84842, validation loss: 1.93712, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/25 15:47:29 nl.defaults.trainer]: \u001b[0mEpoch 58-31, Train loss: 1.82343, validation loss: 1.94313, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/25 15:47:34 nl.defaults.trainer]: \u001b[0mEpoch 58-61, Train loss: 1.88587, validation loss: 1.94258, learning rate: [0.01001572135402173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:47:39 nl.defaults.trainer]: \u001b[0mEpoch 58-91, Train loss: 1.87453, validation loss: 1.92047, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/25 15:47:44 nl.defaults.trainer]: \u001b[0mEpoch 58-121, Train loss: 1.83298, validation loss: 1.92020, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/25 15:47:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0189,  0.0024, -0.0950,  0.1194], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2631, -0.1515, -0.2832,  0.3902], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:47:47 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy (top1, top5): 57.70286, 93.78857, Validation accuracy: 55.90214, 93.52190\n",
      "\u001b[32m[06/25 15:47:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.018852, +0.002408, -0.095030, +0.119388, 3\n",
      "-0.263088, -0.151510, -0.283181, +0.390163, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:47:49 nl.defaults.trainer]: \u001b[0mEpoch 59-13, Train loss: 1.86819, validation loss: 1.87657, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/25 15:47:54 nl.defaults.trainer]: \u001b[0mEpoch 59-43, Train loss: 1.91162, validation loss: 1.95520, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/25 15:47:59 nl.defaults.trainer]: \u001b[0mEpoch 59-73, Train loss: 1.88189, validation loss: 1.88439, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/25 15:48:04 nl.defaults.trainer]: \u001b[0mEpoch 59-103, Train loss: 1.85707, validation loss: 1.89336, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/25 15:48:09 nl.defaults.trainer]: \u001b[0mEpoch 59-133, Train loss: 1.89668, validation loss: 1.87912, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/25 15:48:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0227,  0.0115, -0.0892,  0.1116], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2602, -0.1526, -0.2863,  0.3915], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:48:10 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy (top1, top5): 58.18857, 93.62857, Validation accuracy: 56.11599, 93.57322\n",
      "\u001b[32m[06/25 15:48:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.022661, +0.011491, -0.089201, +0.111610, 3\n",
      "-0.260246, -0.152620, -0.286290, +0.391474, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:48:15 nl.defaults.trainer]: \u001b[0mEpoch 60-25, Train loss: 1.87947, validation loss: 1.87614, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/25 15:48:20 nl.defaults.trainer]: \u001b[0mEpoch 60-55, Train loss: 1.84630, validation loss: 1.87187, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/25 15:48:25 nl.defaults.trainer]: \u001b[0mEpoch 60-85, Train loss: 1.86504, validation loss: 1.90910, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/25 15:48:30 nl.defaults.trainer]: \u001b[0mEpoch 60-115, Train loss: 1.88340, validation loss: 1.94178, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/25 15:48:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0261,  0.0131, -0.0842,  0.1074], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2590, -0.1522, -0.2902,  0.3930], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:48:34 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy (top1, top5): 58.41714, 93.83714, Validation accuracy: 56.62922, 93.66446\n",
      "\u001b[32m[06/25 15:48:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.026129, +0.013087, -0.084215, +0.107424, 3\n",
      "-0.258975, -0.152164, -0.290247, +0.392978, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:48:35 nl.defaults.trainer]: \u001b[0mEpoch 61-7, Train loss: 1.90037, validation loss: 1.89246, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/25 15:48:40 nl.defaults.trainer]: \u001b[0mEpoch 61-36, Train loss: 1.91074, validation loss: 1.87263, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/25 15:48:45 nl.defaults.trainer]: \u001b[0mEpoch 61-66, Train loss: 1.85041, validation loss: 1.90929, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/25 15:48:50 nl.defaults.trainer]: \u001b[0mEpoch 61-96, Train loss: 1.84302, validation loss: 1.93404, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/25 15:48:55 nl.defaults.trainer]: \u001b[0mEpoch 61-126, Train loss: 1.85812, validation loss: 1.90780, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/25 15:48:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0243,  0.0144, -0.0878,  0.1089], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2552, -0.1477, -0.2865,  0.3892], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:48:57 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy (top1, top5): 58.38286, 93.93143, Validation accuracy: 56.67769, 93.77566\n",
      "\u001b[32m[06/25 15:48:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.024285, +0.014415, -0.087833, +0.108928, 3\n",
      "-0.255160, -0.147724, -0.286522, +0.389222, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:49:00 nl.defaults.trainer]: \u001b[0mEpoch 62-18, Train loss: 1.83662, validation loss: 1.92974, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/25 15:49:05 nl.defaults.trainer]: \u001b[0mEpoch 62-48, Train loss: 1.90274, validation loss: 1.91096, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/25 15:49:11 nl.defaults.trainer]: \u001b[0mEpoch 62-78, Train loss: 1.87861, validation loss: 1.92887, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/25 15:49:16 nl.defaults.trainer]: \u001b[0mEpoch 62-108, Train loss: 1.87603, validation loss: 1.88881, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/25 15:49:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0324,  0.0226, -0.0934,  0.1032], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2459, -0.1424, -0.2785,  0.3816], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:49:20 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy (top1, top5): 58.21143, 93.90286, Validation accuracy: 56.12454, 93.61884\n",
      "\u001b[32m[06/25 15:49:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.032354, +0.022567, -0.093357, +0.103209, 3\n",
      "-0.245875, -0.142355, -0.278459, +0.381613, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:49:21 nl.defaults.trainer]: \u001b[0mEpoch 63-1, Train loss: 1.85719, validation loss: 1.87043, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/25 15:49:26 nl.defaults.trainer]: \u001b[0mEpoch 63-30, Train loss: 1.85172, validation loss: 1.87922, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/25 15:49:31 nl.defaults.trainer]: \u001b[0mEpoch 63-60, Train loss: 1.87143, validation loss: 1.88090, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/25 15:49:36 nl.defaults.trainer]: \u001b[0mEpoch 63-90, Train loss: 1.86561, validation loss: 1.91929, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/25 15:49:41 nl.defaults.trainer]: \u001b[0mEpoch 63-120, Train loss: 1.86463, validation loss: 1.85290, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/25 15:49:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0346,  0.0266, -0.0933,  0.1002], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2419, -0.1399, -0.2748,  0.3786], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:49:44 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy (top1, top5): 59.15714, 93.86571, Validation accuracy: 57.59295, 93.88971\n",
      "\u001b[32m[06/25 15:49:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.034585, +0.026617, -0.093265, +0.100196, 3\n",
      "-0.241882, -0.139880, -0.274825, +0.378599, 3\n",
      "+0.000000, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:49:46 nl.defaults.trainer]: \u001b[0mEpoch 64-12, Train loss: 1.83754, validation loss: 1.85353, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/25 15:49:51 nl.defaults.trainer]: \u001b[0mEpoch 64-42, Train loss: 1.87746, validation loss: 1.90413, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/25 15:49:56 nl.defaults.trainer]: \u001b[0mEpoch 64-72, Train loss: 1.83904, validation loss: 1.86815, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/25 15:50:01 nl.defaults.trainer]: \u001b[0mEpoch 64-102, Train loss: 1.89523, validation loss: 1.84404, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/25 15:50:07 nl.defaults.trainer]: \u001b[0mEpoch 64-132, Train loss: 1.88831, validation loss: 1.91726, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/25 15:50:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0428,  0.0311, -0.0942,  0.0944], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2465, -0.1483, -0.2800,  0.3863], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:50:07 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy (top1, top5): 59.01714, 94.03714, Validation accuracy: 57.67849, 93.66446\n",
      "\u001b[32m[06/25 15:50:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.042806, +0.031099, -0.094224, +0.094408, 3\n",
      "-0.246499, -0.148256, -0.280017, +0.386326, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:50:12 nl.defaults.trainer]: \u001b[0mEpoch 65-24, Train loss: 1.90249, validation loss: 1.86634, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/25 15:50:17 nl.defaults.trainer]: \u001b[0mEpoch 65-54, Train loss: 1.88751, validation loss: 1.86822, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/25 15:50:22 nl.defaults.trainer]: \u001b[0mEpoch 65-83, Train loss: 1.86166, validation loss: 1.88342, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/25 15:50:27 nl.defaults.trainer]: \u001b[0mEpoch 65-113, Train loss: 1.84000, validation loss: 1.89288, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/25 15:50:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0473,  0.0334, -0.0906,  0.0898], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2441, -0.1483, -0.2806,  0.3864], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:50:31 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy (top1, top5): 59.44571, 93.94571, Validation accuracy: 57.79539, 93.75285\n",
      "\u001b[32m[06/25 15:50:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.047302, +0.033371, -0.090639, +0.089775, 3\n",
      "-0.244133, -0.148273, -0.280559, +0.386394, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:50:32 nl.defaults.trainer]: \u001b[0mEpoch 66-5, Train loss: 1.85903, validation loss: 1.88964, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/25 15:50:37 nl.defaults.trainer]: \u001b[0mEpoch 66-35, Train loss: 1.91120, validation loss: 1.88021, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/25 15:50:42 nl.defaults.trainer]: \u001b[0mEpoch 66-65, Train loss: 1.86053, validation loss: 1.87455, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/25 15:50:47 nl.defaults.trainer]: \u001b[0mEpoch 66-95, Train loss: 1.88303, validation loss: 1.87756, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/25 15:50:52 nl.defaults.trainer]: \u001b[0mEpoch 66-125, Train loss: 1.87611, validation loss: 1.89438, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/25 15:50:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0530,  0.0403, -0.0875,  0.0826], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2421, -0.1495, -0.2768,  0.3856], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:50:54 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy (top1, top5): 59.38857, 94.12000, Validation accuracy: 58.23449, 93.90112\n",
      "\u001b[32m[06/25 15:50:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.052982, +0.040334, -0.087472, +0.082617, 3\n",
      "-0.242105, -0.149481, -0.276803, +0.385648, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:50:57 nl.defaults.trainer]: \u001b[0mEpoch 67-17, Train loss: 1.87066, validation loss: 1.84186, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/25 15:51:02 nl.defaults.trainer]: \u001b[0mEpoch 67-47, Train loss: 1.81518, validation loss: 1.87412, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/25 15:51:08 nl.defaults.trainer]: \u001b[0mEpoch 67-77, Train loss: 1.87108, validation loss: 1.85745, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/25 15:51:13 nl.defaults.trainer]: \u001b[0mEpoch 67-107, Train loss: 1.86504, validation loss: 1.89669, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/25 15:51:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0598,  0.0430, -0.0881,  0.0781], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2402, -0.1487, -0.2778,  0.3857], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:51:18 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy (top1, top5): 59.24857, 93.95429, Validation accuracy: 57.78114, 93.63595\n",
      "\u001b[32m[06/25 15:51:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.059800, +0.042999, -0.088058, +0.078106, 3\n",
      "-0.240196, -0.148663, -0.277832, +0.385666, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:51:18 nl.defaults.trainer]: \u001b[0mEpoch 68-0, Train loss: 1.86897, validation loss: 1.94147, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/25 15:51:23 nl.defaults.trainer]: \u001b[0mEpoch 68-30, Train loss: 1.88816, validation loss: 1.90378, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/25 15:51:28 nl.defaults.trainer]: \u001b[0mEpoch 68-60, Train loss: 1.85922, validation loss: 1.86682, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/25 15:51:33 nl.defaults.trainer]: \u001b[0mEpoch 68-90, Train loss: 1.86476, validation loss: 1.88278, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/25 15:51:38 nl.defaults.trainer]: \u001b[0mEpoch 68-120, Train loss: 1.88017, validation loss: 1.85988, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/25 15:51:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0672,  0.0537, -0.0762,  0.0653], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2472, -0.1550, -0.2907,  0.3959], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:51:41 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy (top1, top5): 59.58286, 94.01143, Validation accuracy: 57.92370, 94.21476\n",
      "\u001b[32m[06/25 15:51:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.067246, +0.053708, -0.076223, +0.065347, 0\n",
      "-0.247189, -0.155020, -0.290719, +0.395925, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:51:43 nl.defaults.trainer]: \u001b[0mEpoch 69-13, Train loss: 1.84249, validation loss: 1.89421, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/25 15:51:48 nl.defaults.trainer]: \u001b[0mEpoch 69-43, Train loss: 1.87241, validation loss: 1.86097, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/25 15:51:53 nl.defaults.trainer]: \u001b[0mEpoch 69-72, Train loss: 1.88887, validation loss: 1.98670, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/25 15:51:59 nl.defaults.trainer]: \u001b[0mEpoch 69-102, Train loss: 1.84232, validation loss: 1.91848, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/25 15:52:04 nl.defaults.trainer]: \u001b[0mEpoch 69-132, Train loss: 1.83447, validation loss: 1.89681, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/25 15:52:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0796,  0.0602, -0.0739,  0.0553], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2441, -0.1530, -0.2894,  0.3942], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:52:04 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy (top1, top5): 59.74000, 94.14286, Validation accuracy: 58.22879, 93.90682\n",
      "\u001b[32m[06/25 15:52:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.079594, +0.060192, -0.073885, +0.055281, 0\n",
      "-0.244054, -0.153009, -0.289380, +0.394228, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:52:09 nl.defaults.trainer]: \u001b[0mEpoch 70-24, Train loss: 1.85701, validation loss: 1.87956, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/25 15:52:14 nl.defaults.trainer]: \u001b[0mEpoch 70-54, Train loss: 1.87242, validation loss: 1.84857, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/25 15:52:19 nl.defaults.trainer]: \u001b[0mEpoch 70-83, Train loss: 1.80018, validation loss: 1.87394, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/25 15:52:24 nl.defaults.trainer]: \u001b[0mEpoch 70-113, Train loss: 1.87469, validation loss: 1.87851, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/25 15:52:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0855,  0.0658, -0.0690,  0.0479], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2469, -0.1576, -0.2892,  0.3978], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:52:28 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy (top1, top5): 59.95143, 94.10286, Validation accuracy: 58.59090, 93.97810\n",
      "\u001b[32m[06/25 15:52:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.085541, +0.065768, -0.069036, +0.047875, 0\n",
      "-0.246912, -0.157587, -0.289198, +0.397837, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:52:29 nl.defaults.trainer]: \u001b[0mEpoch 71-5, Train loss: 1.86046, validation loss: 1.87705, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/25 15:52:34 nl.defaults.trainer]: \u001b[0mEpoch 71-35, Train loss: 1.91135, validation loss: 1.85823, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/25 15:52:39 nl.defaults.trainer]: \u001b[0mEpoch 71-65, Train loss: 1.89897, validation loss: 1.90806, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/25 15:52:44 nl.defaults.trainer]: \u001b[0mEpoch 71-95, Train loss: 1.86025, validation loss: 1.84434, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/25 15:52:50 nl.defaults.trainer]: \u001b[0mEpoch 71-125, Train loss: 1.89280, validation loss: 1.87676, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/25 15:52:51 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0919,  0.0641, -0.0724,  0.0463], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2483, -0.1607, -0.2922,  0.4014], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:52:52 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy (top1, top5): 60.26571, 94.31714, Validation accuracy: 58.44548, 93.92963\n",
      "\u001b[32m[06/25 15:52:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.091940, +0.064146, -0.072355, +0.046335, 0\n",
      "-0.248258, -0.160678, -0.292220, +0.401374, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:52:55 nl.defaults.trainer]: \u001b[0mEpoch 72-17, Train loss: 1.82921, validation loss: 1.88275, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/25 15:53:00 nl.defaults.trainer]: \u001b[0mEpoch 72-47, Train loss: 1.85741, validation loss: 1.86894, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/25 15:53:05 nl.defaults.trainer]: \u001b[0mEpoch 72-76, Train loss: 1.88989, validation loss: 1.86463, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/25 15:53:10 nl.defaults.trainer]: \u001b[0mEpoch 72-106, Train loss: 1.86776, validation loss: 1.91045, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/25 15:53:15 nl.defaults.trainer]: \u001b[0mEpoch 72-136, Train loss: 1.85305, validation loss: 1.85611, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/25 15:53:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0950,  0.0710, -0.0702,  0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2515, -0.1617, -0.2990,  0.4060], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:53:15 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy (top1, top5): 60.49143, 94.32857, Validation accuracy: 58.60230, 94.10071\n",
      "\u001b[32m[06/25 15:53:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.094987, +0.071022, -0.070225, +0.040676, 0\n",
      "-0.251476, -0.161742, -0.298951, +0.406004, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:53:20 nl.defaults.trainer]: \u001b[0mEpoch 73-29, Train loss: 1.87363, validation loss: 1.96501, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/25 15:53:25 nl.defaults.trainer]: \u001b[0mEpoch 73-58, Train loss: 1.86965, validation loss: 1.91790, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/25 15:53:30 nl.defaults.trainer]: \u001b[0mEpoch 73-88, Train loss: 1.81901, validation loss: 1.90662, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/25 15:53:35 nl.defaults.trainer]: \u001b[0mEpoch 73-118, Train loss: 1.83110, validation loss: 1.88299, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/25 15:53:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1008,  0.0786, -0.0659,  0.0325], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2483, -0.1656, -0.3011,  0.4081], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:53:39 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy (top1, top5): 60.54857, 94.22000, Validation accuracy: 58.86462, 94.05794\n",
      "\u001b[32m[06/25 15:53:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.100765, +0.078575, -0.065927, +0.032508, 0\n",
      "-0.248284, -0.165649, -0.301064, +0.408062, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:53:40 nl.defaults.trainer]: \u001b[0mEpoch 74-10, Train loss: 1.89035, validation loss: 1.89748, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/25 15:53:46 nl.defaults.trainer]: \u001b[0mEpoch 74-40, Train loss: 1.84906, validation loss: 1.87675, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/25 15:53:51 nl.defaults.trainer]: \u001b[0mEpoch 74-69, Train loss: 1.85793, validation loss: 1.85233, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/25 15:53:56 nl.defaults.trainer]: \u001b[0mEpoch 74-99, Train loss: 1.87927, validation loss: 1.85986, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/25 15:54:01 nl.defaults.trainer]: \u001b[0mEpoch 74-129, Train loss: 1.90961, validation loss: 1.83809, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/25 15:54:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1067,  0.0856, -0.0661,  0.0260], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2514, -0.1696, -0.3050,  0.4130], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:54:02 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy (top1, top5): 60.50571, 94.12857, Validation accuracy: 59.01574, 94.19195\n",
      "\u001b[32m[06/25 15:54:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.106715, +0.085631, -0.066143, +0.026042, 0\n",
      "-0.251408, -0.169595, -0.305009, +0.412960, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:54:06 nl.defaults.trainer]: \u001b[0mEpoch 75-21, Train loss: 1.88667, validation loss: 1.84723, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/25 15:54:11 nl.defaults.trainer]: \u001b[0mEpoch 75-50, Train loss: 1.87129, validation loss: 1.87438, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/25 15:54:16 nl.defaults.trainer]: \u001b[0mEpoch 75-80, Train loss: 1.94466, validation loss: 1.83690, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/25 15:54:21 nl.defaults.trainer]: \u001b[0mEpoch 75-110, Train loss: 1.80757, validation loss: 1.83993, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/25 15:54:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1114,  0.0867, -0.0623,  0.0216], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2553, -0.1780, -0.3124,  0.4212], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:54:26 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy (top1, top5): 60.62571, 94.40857, Validation accuracy: 58.67359, 94.06364\n",
      "\u001b[32m[06/25 15:54:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.111382, +0.086710, -0.062312, +0.021563, 0\n",
      "-0.255282, -0.177951, -0.312404, +0.421237, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:54:26 nl.defaults.trainer]: \u001b[0mEpoch 76-2, Train loss: 1.87498, validation loss: 1.86836, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/25 15:54:31 nl.defaults.trainer]: \u001b[0mEpoch 76-32, Train loss: 1.80951, validation loss: 1.92093, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/25 15:54:36 nl.defaults.trainer]: \u001b[0mEpoch 76-62, Train loss: 1.86040, validation loss: 1.88246, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/25 15:54:42 nl.defaults.trainer]: \u001b[0mEpoch 76-92, Train loss: 1.87222, validation loss: 1.91990, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/25 15:54:47 nl.defaults.trainer]: \u001b[0mEpoch 76-122, Train loss: 1.85886, validation loss: 1.85686, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/25 15:54:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1181,  0.0929, -0.0617,  0.0147], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2579, -0.1765, -0.3169,  0.4239], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:54:49 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy (top1, top5): 60.87143, 94.39143, Validation accuracy: 58.96156, 94.18054\n",
      "\u001b[32m[06/25 15:54:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.118147, +0.092922, -0.061702, +0.014668, 0\n",
      "-0.257927, -0.176496, -0.316856, +0.423859, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:54:52 nl.defaults.trainer]: \u001b[0mEpoch 77-15, Train loss: 1.85493, validation loss: 1.83206, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/25 15:54:57 nl.defaults.trainer]: \u001b[0mEpoch 77-45, Train loss: 1.83365, validation loss: 1.85693, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/25 15:55:02 nl.defaults.trainer]: \u001b[0mEpoch 77-75, Train loss: 1.86971, validation loss: 1.89560, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/25 15:55:07 nl.defaults.trainer]: \u001b[0mEpoch 77-104, Train loss: 1.87154, validation loss: 1.85019, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/25 15:55:12 nl.defaults.trainer]: \u001b[0mEpoch 77-134, Train loss: 1.85379, validation loss: 1.89077, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/25 15:55:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1155,  0.0976, -0.0622,  0.0137], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2654, -0.1800, -0.3254,  0.4317], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:55:13 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy (top1, top5): 60.91143, 94.36000, Validation accuracy: 59.59455, 94.44571\n",
      "\u001b[32m[06/25 15:55:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.115541, +0.097614, -0.062195, +0.013705, 0\n",
      "-0.265437, -0.179964, -0.325375, +0.431690, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:55:17 nl.defaults.trainer]: \u001b[0mEpoch 78-26, Train loss: 1.87219, validation loss: 1.88366, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/25 15:55:22 nl.defaults.trainer]: \u001b[0mEpoch 78-56, Train loss: 1.89914, validation loss: 1.87048, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/25 15:55:27 nl.defaults.trainer]: \u001b[0mEpoch 78-86, Train loss: 1.88181, validation loss: 1.88221, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/25 15:55:33 nl.defaults.trainer]: \u001b[0mEpoch 78-116, Train loss: 1.91061, validation loss: 1.89196, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/25 15:55:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1184,  0.1087, -0.0644,  0.0075], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2601, -0.1682, -0.3245,  0.4252], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:55:36 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy (top1, top5): 60.83143, 94.44286, Validation accuracy: 58.89599, 94.03228\n",
      "\u001b[32m[06/25 15:55:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.118425, +0.108692, -0.064416, +0.007511, 0\n",
      "-0.260083, -0.168225, -0.324462, +0.425224, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:55:38 nl.defaults.trainer]: \u001b[0mEpoch 79-8, Train loss: 1.81725, validation loss: 1.84040, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/25 15:55:43 nl.defaults.trainer]: \u001b[0mEpoch 79-37, Train loss: 1.83359, validation loss: 1.85608, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/25 15:55:48 nl.defaults.trainer]: \u001b[0mEpoch 79-67, Train loss: 1.85529, validation loss: 1.87867, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/25 15:55:53 nl.defaults.trainer]: \u001b[0mEpoch 79-96, Train loss: 1.91261, validation loss: 1.85810, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/25 15:55:58 nl.defaults.trainer]: \u001b[0mEpoch 79-126, Train loss: 1.84850, validation loss: 1.85393, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/25 15:56:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1146,  0.1116, -0.0606,  0.0065], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2591, -0.1669, -0.3243,  0.4251], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:56:00 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy (top1, top5): 60.96000, 94.52000, Validation accuracy: 59.13549, 94.57687\n",
      "\u001b[32m[06/25 15:56:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.114572, +0.111585, -0.060612, +0.006546, 0\n",
      "-0.259116, -0.166864, -0.324333, +0.425050, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:56:03 nl.defaults.trainer]: \u001b[0mEpoch 80-18, Train loss: 1.85660, validation loss: 1.88822, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/25 15:56:08 nl.defaults.trainer]: \u001b[0mEpoch 80-48, Train loss: 1.81696, validation loss: 1.89100, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/25 15:56:13 nl.defaults.trainer]: \u001b[0mEpoch 80-78, Train loss: 1.80716, validation loss: 1.80943, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/25 15:56:18 nl.defaults.trainer]: \u001b[0mEpoch 80-108, Train loss: 1.87535, validation loss: 1.88035, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/25 15:56:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1142,  0.1113, -0.0611,  0.0068], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2595, -0.1665, -0.3297,  0.4276], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:56:23 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy (top1, top5): 61.22286, 94.53429, Validation accuracy: 59.54893, 94.41150\n",
      "\u001b[32m[06/25 15:56:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.114203, +0.111318, -0.061072, +0.006785, 0\n",
      "-0.259472, -0.166461, -0.329678, +0.427597, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:56:23 nl.defaults.trainer]: \u001b[0mEpoch 81-1, Train loss: 1.86945, validation loss: 1.92689, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/25 15:56:28 nl.defaults.trainer]: \u001b[0mEpoch 81-31, Train loss: 1.86785, validation loss: 1.85243, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/25 15:56:34 nl.defaults.trainer]: \u001b[0mEpoch 81-61, Train loss: 1.83903, validation loss: 1.89790, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/25 15:56:39 nl.defaults.trainer]: \u001b[0mEpoch 81-91, Train loss: 1.82935, validation loss: 1.87044, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/25 15:56:44 nl.defaults.trainer]: \u001b[0mEpoch 81-121, Train loss: 1.83871, validation loss: 1.85594, learning rate: [0.0030750331087052566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:56:46 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1223,  0.1131, -0.0561, -0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2620, -0.1715, -0.3374,  0.4341], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:56:46 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy (top1, top5): 61.30571, 94.39714, Validation accuracy: 59.12409, 94.06079\n",
      "\u001b[32m[06/25 15:56:46 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.122254, +0.113092, -0.056064, -0.000362, 0\n",
      "-0.262021, -0.171534, -0.337417, +0.434138, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:56:49 nl.defaults.trainer]: \u001b[0mEpoch 82-13, Train loss: 1.83856, validation loss: 1.86192, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/25 15:56:54 nl.defaults.trainer]: \u001b[0mEpoch 82-43, Train loss: 1.82248, validation loss: 1.87725, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/25 15:56:59 nl.defaults.trainer]: \u001b[0mEpoch 82-73, Train loss: 1.90761, validation loss: 1.81618, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/25 15:57:04 nl.defaults.trainer]: \u001b[0mEpoch 82-103, Train loss: 1.91928, validation loss: 1.86405, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/25 15:57:09 nl.defaults.trainer]: \u001b[0mEpoch 82-133, Train loss: 1.86278, validation loss: 1.85186, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/25 15:57:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1241,  0.1148, -0.0534, -0.0034], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2709, -0.1730, -0.3439,  0.4408], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:57:10 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy (top1, top5): 61.32000, 94.51429, Validation accuracy: 59.52897, 94.31740\n",
      "\u001b[32m[06/25 15:57:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.124138, +0.114814, -0.053412, -0.003423, 0\n",
      "-0.270904, -0.173039, -0.343854, +0.440849, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:57:14 nl.defaults.trainer]: \u001b[0mEpoch 83-26, Train loss: 1.84884, validation loss: 1.88859, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/25 15:57:19 nl.defaults.trainer]: \u001b[0mEpoch 83-56, Train loss: 1.84225, validation loss: 1.82863, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/25 15:57:24 nl.defaults.trainer]: \u001b[0mEpoch 83-86, Train loss: 1.86435, validation loss: 1.85783, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/25 15:57:30 nl.defaults.trainer]: \u001b[0mEpoch 83-116, Train loss: 1.83020, validation loss: 1.87546, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/25 15:57:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1287,  0.1173, -0.0474, -0.0095], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2811, -0.1811, -0.3565,  0.4530], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:57:33 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy (top1, top5): 61.64286, 94.58571, Validation accuracy: 59.68294, 94.31170\n",
      "\u001b[32m[06/25 15:57:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.128746, +0.117329, -0.047386, -0.009491, 0\n",
      "-0.281099, -0.181125, -0.356506, +0.453033, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:57:35 nl.defaults.trainer]: \u001b[0mEpoch 84-9, Train loss: 1.83126, validation loss: 1.85092, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/25 15:57:40 nl.defaults.trainer]: \u001b[0mEpoch 84-39, Train loss: 1.86281, validation loss: 1.84660, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/25 15:57:45 nl.defaults.trainer]: \u001b[0mEpoch 84-69, Train loss: 1.82453, validation loss: 1.91129, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/25 15:57:50 nl.defaults.trainer]: \u001b[0mEpoch 84-99, Train loss: 1.86137, validation loss: 1.85009, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/25 15:57:55 nl.defaults.trainer]: \u001b[0mEpoch 84-129, Train loss: 1.84362, validation loss: 1.84281, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/25 15:57:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1423,  0.1219, -0.0536, -0.0168], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2794, -0.1852, -0.3587,  0.4559], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:57:56 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy (top1, top5): 61.87429, 94.56857, Validation accuracy: 59.71715, 94.23472\n",
      "\u001b[32m[06/25 15:57:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.142310, +0.121928, -0.053587, -0.016793, 0\n",
      "-0.279384, -0.185191, -0.358699, +0.455898, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:58:00 nl.defaults.trainer]: \u001b[0mEpoch 85-22, Train loss: 1.82856, validation loss: 1.90300, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/25 15:58:05 nl.defaults.trainer]: \u001b[0mEpoch 85-52, Train loss: 1.85171, validation loss: 1.85777, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/25 15:58:10 nl.defaults.trainer]: \u001b[0mEpoch 85-82, Train loss: 1.85005, validation loss: 1.84324, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/25 15:58:15 nl.defaults.trainer]: \u001b[0mEpoch 85-112, Train loss: 1.86941, validation loss: 1.88156, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/25 15:58:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1392,  0.1282, -0.0523, -0.0190], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2806, -0.1825, -0.3586,  0.4560], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:58:19 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy (top1, top5): 61.58857, 94.41429, Validation accuracy: 59.18111, 94.09786\n",
      "\u001b[32m[06/25 15:58:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.139155, +0.128201, -0.052280, -0.019041, 0\n",
      "-0.280630, -0.182469, -0.358648, +0.456012, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:58:20 nl.defaults.trainer]: \u001b[0mEpoch 86-4, Train loss: 1.82096, validation loss: 1.86751, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/25 15:58:25 nl.defaults.trainer]: \u001b[0mEpoch 86-34, Train loss: 1.84785, validation loss: 1.84291, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/25 15:58:31 nl.defaults.trainer]: \u001b[0mEpoch 86-64, Train loss: 1.82160, validation loss: 1.84350, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/25 15:58:36 nl.defaults.trainer]: \u001b[0mEpoch 86-94, Train loss: 1.86880, validation loss: 1.85634, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/25 15:58:41 nl.defaults.trainer]: \u001b[0mEpoch 86-124, Train loss: 1.86611, validation loss: 1.90239, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/25 15:58:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1447,  0.1346, -0.0508, -0.0260], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2811, -0.1852, -0.3617,  0.4593], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:58:43 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy (top1, top5): 62.05714, 94.43714, Validation accuracy: 59.60880, 94.22901\n",
      "\u001b[32m[06/25 15:58:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.144687, +0.134639, -0.050816, -0.025972, 0\n",
      "-0.281062, -0.185207, -0.361735, +0.459328, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:58:46 nl.defaults.trainer]: \u001b[0mEpoch 87-16, Train loss: 1.86497, validation loss: 1.88551, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/25 15:58:51 nl.defaults.trainer]: \u001b[0mEpoch 87-46, Train loss: 1.85505, validation loss: 1.85454, learning rate: [0.0019869444917922276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 15:58:56 nl.defaults.trainer]: \u001b[0mEpoch 87-76, Train loss: 1.84948, validation loss: 1.84514, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/25 15:59:01 nl.defaults.trainer]: \u001b[0mEpoch 87-106, Train loss: 1.82180, validation loss: 1.85563, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/25 15:59:06 nl.defaults.trainer]: \u001b[0mEpoch 87-136, Train loss: 1.81746, validation loss: 1.84363, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/25 15:59:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1461,  0.1363, -0.0527, -0.0271], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2853, -0.1909, -0.3706,  0.4672], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:59:06 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy (top1, top5): 61.47143, 94.50857, Validation accuracy: 60.23038, 94.46567\n",
      "\u001b[32m[06/25 15:59:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.146070, +0.136280, -0.052718, -0.027112, 0\n",
      "-0.285336, -0.190907, -0.370565, +0.467217, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:59:11 nl.defaults.trainer]: \u001b[0mEpoch 88-28, Train loss: 1.85034, validation loss: 1.85204, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/25 15:59:16 nl.defaults.trainer]: \u001b[0mEpoch 88-58, Train loss: 1.83397, validation loss: 1.86105, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/25 15:59:21 nl.defaults.trainer]: \u001b[0mEpoch 88-88, Train loss: 1.80567, validation loss: 1.89597, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/25 15:59:26 nl.defaults.trainer]: \u001b[0mEpoch 88-118, Train loss: 1.83064, validation loss: 1.86251, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/25 15:59:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1525,  0.1365, -0.0471, -0.0329], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2868, -0.1951, -0.3758,  0.4723], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:59:30 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy (top1, top5): 61.88286, 94.46000, Validation accuracy: 59.85401, 94.55406\n",
      "\u001b[32m[06/25 15:59:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.152462, +0.136505, -0.047135, -0.032938, 0\n",
      "-0.286757, -0.195094, -0.375816, +0.472311, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:59:32 nl.defaults.trainer]: \u001b[0mEpoch 89-10, Train loss: 1.84543, validation loss: 1.81563, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/25 15:59:37 nl.defaults.trainer]: \u001b[0mEpoch 89-40, Train loss: 1.83443, validation loss: 1.89769, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/25 15:59:42 nl.defaults.trainer]: \u001b[0mEpoch 89-70, Train loss: 1.85357, validation loss: 1.83114, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/25 15:59:47 nl.defaults.trainer]: \u001b[0mEpoch 89-100, Train loss: 1.84665, validation loss: 1.88898, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/25 15:59:52 nl.defaults.trainer]: \u001b[0mEpoch 89-130, Train loss: 1.86009, validation loss: 1.84956, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/25 15:59:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1529,  0.1325, -0.0461, -0.0318], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2867, -0.1947, -0.3781,  0.4739], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 15:59:53 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy (top1, top5): 61.71143, 94.67143, Validation accuracy: 59.66298, 94.41150\n",
      "\u001b[32m[06/25 15:59:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.152931, +0.132453, -0.046124, -0.031835, 0\n",
      "-0.286691, -0.194727, -0.378142, +0.473897, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 15:59:57 nl.defaults.trainer]: \u001b[0mEpoch 90-23, Train loss: 1.90426, validation loss: 1.82720, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/25 16:00:02 nl.defaults.trainer]: \u001b[0mEpoch 90-53, Train loss: 1.84338, validation loss: 1.88823, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/25 16:00:07 nl.defaults.trainer]: \u001b[0mEpoch 90-83, Train loss: 1.84541, validation loss: 1.83999, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/25 16:00:12 nl.defaults.trainer]: \u001b[0mEpoch 90-113, Train loss: 1.81798, validation loss: 1.88227, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/25 16:00:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1622,  0.1380, -0.0435, -0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2855, -0.1934, -0.3838,  0.4758], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:00:16 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy (top1, top5): 61.88000, 94.57429, Validation accuracy: 60.25890, 94.42005\n",
      "\u001b[32m[06/25 16:00:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.162219, +0.137976, -0.043541, -0.040710, 0\n",
      "-0.285510, -0.193434, -0.383831, +0.475828, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:00:17 nl.defaults.trainer]: \u001b[0mEpoch 91-5, Train loss: 1.89304, validation loss: 1.87425, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/25 16:00:22 nl.defaults.trainer]: \u001b[0mEpoch 91-35, Train loss: 1.84641, validation loss: 1.90361, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/25 16:00:28 nl.defaults.trainer]: \u001b[0mEpoch 91-64, Train loss: 1.86249, validation loss: 1.86527, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/25 16:00:33 nl.defaults.trainer]: \u001b[0mEpoch 91-94, Train loss: 1.88112, validation loss: 1.87640, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/25 16:00:38 nl.defaults.trainer]: \u001b[0mEpoch 91-123, Train loss: 1.77603, validation loss: 1.85268, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/25 16:00:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1596,  0.1386, -0.0530, -0.0365], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2899, -0.1961, -0.3903,  0.4817], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:00:40 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy (top1, top5): 62.00857, 94.51143, Validation accuracy: 60.15340, 94.59113\n",
      "\u001b[32m[06/25 16:00:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159632, +0.138612, -0.052976, -0.036512, 0\n",
      "-0.289923, -0.196066, -0.390342, +0.481690, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:00:43 nl.defaults.trainer]: \u001b[0mEpoch 92-16, Train loss: 1.84952, validation loss: 1.88453, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/25 16:00:48 nl.defaults.trainer]: \u001b[0mEpoch 92-46, Train loss: 1.82655, validation loss: 1.88205, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/25 16:00:53 nl.defaults.trainer]: \u001b[0mEpoch 92-76, Train loss: 1.87456, validation loss: 1.86448, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/25 16:00:58 nl.defaults.trainer]: \u001b[0mEpoch 92-106, Train loss: 1.85327, validation loss: 1.86580, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/25 16:01:03 nl.defaults.trainer]: \u001b[0mEpoch 92-135, Train loss: 1.88250, validation loss: 1.85604, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/25 16:01:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1596,  0.1393, -0.0498, -0.0383], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2874, -0.1986, -0.3936,  0.4841], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:01:03 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy (top1, top5): 62.00857, 94.49714, Validation accuracy: 60.46989, 94.54836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 16:01:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159551, +0.139306, -0.049809, -0.038293, 0\n",
      "-0.287383, -0.198598, -0.393645, +0.484081, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:01:08 nl.defaults.trainer]: \u001b[0mEpoch 93-28, Train loss: 1.82079, validation loss: 1.86226, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/25 16:01:13 nl.defaults.trainer]: \u001b[0mEpoch 93-58, Train loss: 1.83895, validation loss: 1.88437, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/25 16:01:18 nl.defaults.trainer]: \u001b[0mEpoch 93-88, Train loss: 1.85728, validation loss: 1.84760, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/25 16:01:23 nl.defaults.trainer]: \u001b[0mEpoch 93-117, Train loss: 1.81954, validation loss: 1.85884, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/25 16:01:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1611,  0.1403, -0.0495, -0.0400], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2886, -0.2017, -0.3964,  0.4878], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:01:27 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy (top1, top5): 62.25143, 94.68857, Validation accuracy: 59.91959, 94.21191\n",
      "\u001b[32m[06/25 16:01:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.161115, +0.140305, -0.049539, -0.040009, 0\n",
      "-0.288555, -0.201710, -0.396384, +0.487808, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:01:29 nl.defaults.trainer]: \u001b[0mEpoch 94-9, Train loss: 1.86112, validation loss: 1.83090, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/25 16:01:34 nl.defaults.trainer]: \u001b[0mEpoch 94-39, Train loss: 1.86082, validation loss: 1.88854, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/25 16:01:39 nl.defaults.trainer]: \u001b[0mEpoch 94-69, Train loss: 1.84085, validation loss: 1.86640, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/25 16:01:44 nl.defaults.trainer]: \u001b[0mEpoch 94-99, Train loss: 1.84766, validation loss: 1.84522, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/25 16:01:49 nl.defaults.trainer]: \u001b[0mEpoch 94-128, Train loss: 1.87648, validation loss: 1.85782, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/25 16:01:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1597,  0.1401, -0.0506, -0.0391], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2909, -0.2032, -0.3997,  0.4914], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:01:50 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy (top1, top5): 62.19143, 94.53429, Validation accuracy: 60.54688, 94.50274\n",
      "\u001b[32m[06/25 16:01:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159654, +0.140131, -0.050560, -0.039110, 0\n",
      "-0.290892, -0.203222, -0.399683, +0.491407, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:01:54 nl.defaults.trainer]: \u001b[0mEpoch 95-21, Train loss: 1.86767, validation loss: 1.82826, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/25 16:01:59 nl.defaults.trainer]: \u001b[0mEpoch 95-50, Train loss: 1.82539, validation loss: 1.87246, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/25 16:02:04 nl.defaults.trainer]: \u001b[0mEpoch 95-80, Train loss: 1.86201, validation loss: 1.85475, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/25 16:02:09 nl.defaults.trainer]: \u001b[0mEpoch 95-109, Train loss: 1.83527, validation loss: 1.84501, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/25 16:02:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1617,  0.1405, -0.0474, -0.0418], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2936, -0.2075, -0.4069,  0.4977], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:02:14 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy (top1, top5): 62.12000, 94.58286, Validation accuracy: 60.07356, 94.39724\n",
      "\u001b[32m[06/25 16:02:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.161716, +0.140455, -0.047393, -0.041805, 0\n",
      "-0.293574, -0.207505, -0.406945, +0.497719, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:02:14 nl.defaults.trainer]: \u001b[0mEpoch 96-2, Train loss: 1.83888, validation loss: 1.85566, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/25 16:02:19 nl.defaults.trainer]: \u001b[0mEpoch 96-32, Train loss: 1.84010, validation loss: 1.86539, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/25 16:02:25 nl.defaults.trainer]: \u001b[0mEpoch 96-62, Train loss: 1.83589, validation loss: 1.90638, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/25 16:02:30 nl.defaults.trainer]: \u001b[0mEpoch 96-92, Train loss: 1.85301, validation loss: 1.84521, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/25 16:02:35 nl.defaults.trainer]: \u001b[0mEpoch 96-122, Train loss: 1.87022, validation loss: 1.83761, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/25 16:02:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1628,  0.1433, -0.0504, -0.0431], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2900, -0.2019, -0.4049,  0.4944], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:02:37 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy (top1, top5): 62.56571, 94.62571, Validation accuracy: 59.70290, 94.44001\n",
      "\u001b[32m[06/25 16:02:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.162850, +0.143312, -0.050373, -0.043052, 0\n",
      "-0.290041, -0.201948, -0.404910, +0.494376, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:02:40 nl.defaults.trainer]: \u001b[0mEpoch 97-15, Train loss: 1.80908, validation loss: 1.88830, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/25 16:02:45 nl.defaults.trainer]: \u001b[0mEpoch 97-45, Train loss: 1.81243, validation loss: 1.85276, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/25 16:02:50 nl.defaults.trainer]: \u001b[0mEpoch 97-75, Train loss: 1.82522, validation loss: 1.85245, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/25 16:02:55 nl.defaults.trainer]: \u001b[0mEpoch 97-105, Train loss: 1.82205, validation loss: 1.85627, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/25 16:03:00 nl.defaults.trainer]: \u001b[0mEpoch 97-135, Train loss: 1.84946, validation loss: 1.89317, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/25 16:03:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1570,  0.1503, -0.0587, -0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2872, -0.2009, -0.4048,  0.4940], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/25 16:03:00 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy (top1, top5): 62.16000, 94.54571, Validation accuracy: 60.11633, 94.24327\n",
      "\u001b[32m[06/25 16:03:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.156956, +0.150270, -0.058707, -0.040681, 0\n",
      "-0.287225, -0.200906, -0.404808, +0.493964, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:03:05 nl.defaults.trainer]: \u001b[0mEpoch 98-28, Train loss: 1.81248, validation loss: 1.83231, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/25 16:03:11 nl.defaults.trainer]: \u001b[0mEpoch 98-58, Train loss: 1.87584, validation loss: 1.84647, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/25 16:03:16 nl.defaults.trainer]: \u001b[0mEpoch 98-88, Train loss: 1.85172, validation loss: 1.90898, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/25 16:03:21 nl.defaults.trainer]: \u001b[0mEpoch 98-117, Train loss: 1.81696, validation loss: 1.86690, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/25 16:03:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1643,  0.1561, -0.0610, -0.0469], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2866, -0.2031, -0.4085,  0.4971], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 16:03:24 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy (top1, top5): 62.53714, 94.43143, Validation accuracy: 60.39291, 94.36017\n",
      "\u001b[32m[06/25 16:03:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.164252, +0.156129, -0.061027, -0.046950, 0\n",
      "-0.286570, -0.203149, -0.408523, +0.497054, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/25 16:03:26 nl.defaults.trainer]: \u001b[0mEpoch 99-9, Train loss: 1.82262, validation loss: 1.92012, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/25 16:03:31 nl.defaults.trainer]: \u001b[0mEpoch 99-39, Train loss: 1.83167, validation loss: 1.80664, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/25 16:03:36 nl.defaults.trainer]: \u001b[0mEpoch 99-69, Train loss: 1.84602, validation loss: 1.85187, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/25 16:03:41 nl.defaults.trainer]: \u001b[0mEpoch 99-99, Train loss: 1.83823, validation loss: 1.83787, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/25 16:03:46 nl.defaults.trainer]: \u001b[0mEpoch 99-128, Train loss: 1.86664, validation loss: 1.83685, learning rate: [0.0010059212756112208]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'run/cifar10/darts/0/search/model_final.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1833793/2031152707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/project/dl2022s/robertsj/NASLib/naslib/defaults/trainer.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, resume_from)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_top1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiodic_checkpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0manytime_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/dl2022s/robertsj/miniconda3/envs/naslib_project/lib/python3.7/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, iteration, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.file_prefix}_final\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madditional_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/dl2022s/robertsj/miniconda3/envs/naslib_project/lib/python3.7/site-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving checkpoint to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/dl2022s/robertsj/miniconda3/envs/naslib_project/lib/python3.7/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, buffering, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \"\"\"\n\u001b[1;32m   1011\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_path_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0mkvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_open_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/dl2022s/robertsj/miniconda3/envs/naslib_project/lib/python3.7/site-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, buffering, encoding, errors, newline, closefd, opener, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosefd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mopener\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'run/cifar10/darts/0/search/model_final.pth'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer, config)\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/25 16:18:16 nl.defaults.trainer]: \u001b[0mStart one-shot evaluation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/25 16:18:22 nl.defaults.trainer]: \u001b[0mEvaluation finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.08148266374351"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate_oneshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NASLib Project",
   "language": "python",
   "name": "naslib_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
