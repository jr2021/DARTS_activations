{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "device: cpu\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.optimizers import DARTSOptimizer\n",
    "from naslib.search_spaces import DartsSearchSpace\n",
    "from naslib.utils import utils, setup_logger, get_config_from_args, set_seed, log_args\n",
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from fvcore.common.config import CfgNode\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mdataset....................................cifar10\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mseed.............................................0\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0msearch_space...........................nasbench201\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mout_dir........................................run\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0moptimizer....................................darts\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0msearchacq_fn_optimization: random_sampling\n",
      "acq_fn_type: its\n",
      "arch_learning_rate: 0.0003\n",
      "arch_weight_decay: 0.001\n",
      "batch_size: 256\n",
      "checkpoint_freq: 1000\n",
      "cutout: False\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 25000\n",
      "debug_predictor: False\n",
      "drop_path_prob: 0.0\n",
      "encoding_type: adjacency_one_hot\n",
      "epochs: 100\n",
      "fidelity: -1\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "k: 10\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.001\n",
      "max_mutations: 1\n",
      "momentum: 0.9\n",
      "num_arches_to_mutate: 2\n",
      "num_candidates: 20\n",
      "num_ensemble: 3\n",
      "num_init: 10\n",
      "output_weights: True\n",
      "population_size: 30\n",
      "predictor_type: var_sparse_gp\n",
      "sample_size: 10\n",
      "seed: 0\n",
      "tau_max: 10\n",
      "tau_min: 0.1\n",
      "train_portion: 0.7\n",
      "unrolled: False\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mevaluationauxiliary_weight: 0.4\n",
      "batch_size: 96\n",
      "checkpoint_freq: 30\n",
      "cutout: True\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 50000\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:8888\n",
      "drop_path_prob: 0.2\n",
      "epochs: 600\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.0\n",
      "momentum: 0.9\n",
      "multiprocessing_distributed: False\n",
      "rank: 0\n",
      "train_portion: 1.0\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "world_size: 1\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0meval_only....................................False\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mresume.......................................False\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mmodel_path....................................None\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mgpu...........................................None\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0msave.........................run/cifar10/bananas/0\n",
      "\u001b[32m[06/27 10:13:28 nl.utils.utils]: \u001b[0mdata../project/dl2022s/robertsj/NASLib/naslib/data\n"
     ]
    }
   ],
   "source": [
    "config = utils.get_config_from_args(config_type='nas')\n",
    "# config.search.epochs = 1 # for testing\n",
    "config.optimizer = 'darts'\n",
    "utils.set_seed(config.seed)\n",
    "clear_output(wait=True)\n",
    "utils.log_args(config)\n",
    "\n",
    "logger = setup_logger(config.save + '/log.log')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class DartsSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2', \n",
    "        'a_stage_3'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = [(16 * 5 * 5, 120), (120, 84), (84, 10)]\n",
    "        stages = ['a_stage_1', 'a_stage_2', 'a_stage_3']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2) # intermediate node\n",
    "        activation_cell.add_node(3) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())]) # mutable intermediate edge\n",
    "        activation_cell.edges[1, 2].set('cell_name', 'activation_cell') \n",
    "        activation_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable output edge\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        for i, scope in zip(range(3, 6), stages):\n",
    "            self.add_node(i, subgraph=deepcopy(activation_cell).set_scope(scope).set_input([i-1])) # activation node i\n",
    "            self.nodes[i]['subgraph'].name = scope # set \n",
    "        self.add_node(6) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 6)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        \n",
    "        for scope, (in_dim, out_dim) in zip(stages, channels):\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_ops(edge, in_dim, out_dim),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "    def _set_ops(self, edge, in_dim, out_dim):\n",
    "        if out_dim != 10:\n",
    "            edge.data.set('op', [\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Hardswish()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.LeakyReLU()),\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Identity())\n",
    "            ])\n",
    "        else:\n",
    "            edge.data.set('op', [\n",
    "                ops.Sequential(nn.Linear(in_dim, out_dim), nn.Softmax(dim=1))\n",
    "            ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = DartsSearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:13:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mParsed graph:\n",
      "Graph a_stage_1:\n",
      " Graph(\n",
      "  (a_stage_1-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (primitive-1): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "        (1): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (primitive-2): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (primitive-3): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (a_stage_1-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph a_stage_2:\n",
      " Graph(\n",
      "  (a_stage_2-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (primitive-1): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "        (1): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (primitive-2): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (primitive-3): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "        (1): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (a_stage_2-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph a_stage_3:\n",
      " Graph(\n",
      "  (a_stage_3-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Sequential(\n",
      "      (op): Sequential(\n",
      "        (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "        (1): Softmax(dim=1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (a_stage_3-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph makrograph:\n",
      " DartsSearchSpace(\n",
      "  (makrograph-edge(1,2)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(2,3)): Identity()\n",
      "  (makrograph-subgraph_at(3)): Graph a_stage_1-0.7579544, scope a_stage_1, 3 nodes\n",
      "  (makrograph-edge(3,4)): Identity()\n",
      "  (makrograph-subgraph_at(4)): Graph a_stage_2-0.7579544, scope a_stage_2, 3 nodes\n",
      "  (makrograph-edge(4,5)): Identity()\n",
      "  (makrograph-subgraph_at(5)): Graph a_stage_3-0.7579544, scope a_stage_3, 3 nodes\n",
      "  (makrograph-edge(5,6)): Identity()\n",
      ")\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = DARTSOptimizer(config)\n",
    "optimizer.adapt_search_space(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:13:38 nl.defaults.trainer]: \u001b[0mparam size = 0.236858MB\n",
      "\u001b[32m[06/27 10:13:38 nl.defaults.trainer]: \u001b[0mStart training\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/27 10:13:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000529, +0.001313, +0.000432, +0.000184, 1\n",
      "+0.000824, -0.001669, +0.000387, +0.000960, 3\n",
      "+0.001344, 0\n",
      "\u001b[32m[06/27 10:13:40 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.30296, validation loss: 2.30231, learning rate: [0.025]\n",
      "\u001b[32m[06/27 10:13:45 nl.defaults.trainer]: \u001b[0mEpoch 0-30, Train loss: 2.30282, validation loss: 2.30232, learning rate: [0.025]\n",
      "\u001b[32m[06/27 10:13:50 nl.defaults.trainer]: \u001b[0mEpoch 0-60, Train loss: 2.30240, validation loss: 2.30287, learning rate: [0.025]\n",
      "\u001b[32m[06/27 10:13:55 nl.defaults.trainer]: \u001b[0mEpoch 0-90, Train loss: 2.30253, validation loss: 2.30225, learning rate: [0.025]\n",
      "\u001b[32m[06/27 10:14:00 nl.defaults.trainer]: \u001b[0mEpoch 0-120, Train loss: 2.30295, validation loss: 2.30237, learning rate: [0.025]\n",
      "\u001b[32m[06/27 10:14:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0119,  0.0039, -0.0163,  0.0211], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0005,  0.0131, -0.0039, -0.0062], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-5.3323e-24], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:14:03 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 9.88286, 49.89429, Validation accuracy: 10.05646, 50.82687\n",
      "\u001b[32m[06/27 10:14:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.011851, +0.003920, -0.016251, +0.021122, 3\n",
      "+0.000490, +0.013142, -0.003860, -0.006205, 1\n",
      "-0.000000, 0\n",
      "\u001b[32m[06/27 10:14:05 nl.defaults.trainer]: \u001b[0mEpoch 1-13, Train loss: 2.30241, validation loss: 2.30234, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/27 10:14:10 nl.defaults.trainer]: \u001b[0mEpoch 1-43, Train loss: 2.30205, validation loss: 2.30265, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/27 10:14:15 nl.defaults.trainer]: \u001b[0mEpoch 1-73, Train loss: 2.30221, validation loss: 2.30235, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/27 10:14:20 nl.defaults.trainer]: \u001b[0mEpoch 1-103, Train loss: 2.30329, validation loss: 2.30191, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/27 10:14:25 nl.defaults.trainer]: \u001b[0mEpoch 1-133, Train loss: 2.30262, validation loss: 2.30256, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/27 10:14:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0163, -0.0113, -0.0320,  0.0508], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0006,  0.0063, -0.0080,  0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:14:26 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 11.07143, 52.37429, Validation accuracy: 11.21978, 52.42929\n",
      "\u001b[32m[06/27 10:14:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.016346, -0.011326, -0.032040, +0.050827, 3\n",
      "+0.000574, +0.006306, -0.007962, +0.000389, 1\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:14:30 nl.defaults.trainer]: \u001b[0mEpoch 2-26, Train loss: 2.30216, validation loss: 2.30272, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/27 10:14:35 nl.defaults.trainer]: \u001b[0mEpoch 2-56, Train loss: 2.30233, validation loss: 2.30218, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/27 10:14:40 nl.defaults.trainer]: \u001b[0mEpoch 2-86, Train loss: 2.30147, validation loss: 2.30188, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/27 10:14:45 nl.defaults.trainer]: \u001b[0mEpoch 2-116, Train loss: 2.30190, validation loss: 2.30199, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/27 10:14:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0428, -0.0483, -0.0667,  0.1181], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0187, -0.0045, -0.0212,  0.0348], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:14:49 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 11.78857, 55.12286, Validation accuracy: 11.73301, 55.12089\n",
      "\u001b[32m[06/27 10:14:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.042780, -0.048282, -0.066694, +0.118094, 3\n",
      "-0.018674, -0.004489, -0.021224, +0.034816, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:14:51 nl.defaults.trainer]: \u001b[0mEpoch 3-9, Train loss: 2.30148, validation loss: 2.30142, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/27 10:14:56 nl.defaults.trainer]: \u001b[0mEpoch 3-39, Train loss: 2.30153, validation loss: 2.30197, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/27 10:15:01 nl.defaults.trainer]: \u001b[0mEpoch 3-69, Train loss: 2.30130, validation loss: 2.30070, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/27 10:15:06 nl.defaults.trainer]: \u001b[0mEpoch 3-99, Train loss: 2.29869, validation loss: 2.29914, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/27 10:15:11 nl.defaults.trainer]: \u001b[0mEpoch 3-130, Train loss: 2.29597, validation loss: 2.29583, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/27 10:15:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1426, -0.1660, -0.1893,  0.2539], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1136, -0.0835, -0.0971,  0.1584], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:15:12 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 13.48286, 62.09143, Validation accuracy: 13.62625, 62.36029\n",
      "\u001b[32m[06/27 10:15:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.142617, -0.166025, -0.189254, +0.253943, 3\n",
      "-0.113573, -0.083479, -0.097080, +0.158429, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:15:16 nl.defaults.trainer]: \u001b[0mEpoch 4-23, Train loss: 2.28065, validation loss: 2.28759, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/27 10:15:21 nl.defaults.trainer]: \u001b[0mEpoch 4-53, Train loss: 2.25911, validation loss: 2.25468, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/27 10:15:26 nl.defaults.trainer]: \u001b[0mEpoch 4-84, Train loss: 2.24810, validation loss: 2.24480, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/27 10:15:31 nl.defaults.trainer]: \u001b[0mEpoch 4-114, Train loss: 2.21356, validation loss: 2.22395, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/27 10:15:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2663, -0.2845, -0.3037,  0.3735], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2188, -0.2137, -0.2146,  0.2812], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:15:35 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 19.35714, 69.02286, Validation accuracy: 19.13207, 68.76141\n",
      "\u001b[32m[06/27 10:15:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.266253, -0.284482, -0.303697, +0.373518, 3\n",
      "-0.218803, -0.213659, -0.214622, +0.281172, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:15:36 nl.defaults.trainer]: \u001b[0mEpoch 5-7, Train loss: 2.20061, validation loss: 2.20291, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/27 10:15:42 nl.defaults.trainer]: \u001b[0mEpoch 5-37, Train loss: 2.20907, validation loss: 2.19553, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/27 10:15:47 nl.defaults.trainer]: \u001b[0mEpoch 5-67, Train loss: 2.20527, validation loss: 2.19696, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/27 10:15:52 nl.defaults.trainer]: \u001b[0mEpoch 5-97, Train loss: 2.16700, validation loss: 2.17423, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/27 10:15:57 nl.defaults.trainer]: \u001b[0mEpoch 5-127, Train loss: 2.18393, validation loss: 2.16966, learning rate: [0.024852260087141656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:15:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3235, -0.3281, -0.3506,  0.4281], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2555, -0.2628, -0.2551,  0.3309], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:15:58 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 25.34000, 72.82000, Validation accuracy: 26.10059, 72.66195\n",
      "\u001b[32m[06/27 10:15:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.323537, -0.328100, -0.350614, +0.428123, 3\n",
      "-0.255544, -0.262764, -0.255105, +0.330919, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:16:02 nl.defaults.trainer]: \u001b[0mEpoch 6-20, Train loss: 2.18517, validation loss: 2.14826, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/27 10:16:07 nl.defaults.trainer]: \u001b[0mEpoch 6-50, Train loss: 2.20430, validation loss: 2.20526, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/27 10:16:12 nl.defaults.trainer]: \u001b[0mEpoch 6-80, Train loss: 2.22069, validation loss: 2.15825, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/27 10:16:17 nl.defaults.trainer]: \u001b[0mEpoch 6-110, Train loss: 2.15439, validation loss: 2.13900, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/27 10:16:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3274, -0.3312, -0.3461,  0.4349], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2630, -0.2761, -0.2460,  0.3402], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:16:21 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 28.38000, 74.49143, Validation accuracy: 28.07653, 74.12751\n",
      "\u001b[32m[06/27 10:16:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.327448, -0.331193, -0.346078, +0.434917, 3\n",
      "-0.262969, -0.276067, -0.245978, +0.340197, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:16:22 nl.defaults.trainer]: \u001b[0mEpoch 7-3, Train loss: 2.15673, validation loss: 2.18359, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/27 10:16:27 nl.defaults.trainer]: \u001b[0mEpoch 7-34, Train loss: 2.14753, validation loss: 2.15609, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/27 10:16:33 nl.defaults.trainer]: \u001b[0mEpoch 7-65, Train loss: 2.17127, validation loss: 2.15274, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/27 10:16:38 nl.defaults.trainer]: \u001b[0mEpoch 7-96, Train loss: 2.17958, validation loss: 2.12160, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/27 10:16:43 nl.defaults.trainer]: \u001b[0mEpoch 7-126, Train loss: 2.08243, validation loss: 2.14166, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/27 10:16:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3141, -0.3140, -0.3275,  0.4229], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2562, -0.2744, -0.2309,  0.3356], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:16:44 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 29.58571, 75.96571, Validation accuracy: 29.41093, 75.76414\n",
      "\u001b[32m[06/27 10:16:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.314095, -0.313965, -0.327500, +0.422855, 3\n",
      "-0.256219, -0.274427, -0.230877, +0.335565, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:16:48 nl.defaults.trainer]: \u001b[0mEpoch 8-19, Train loss: 2.15562, validation loss: 2.10892, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/27 10:16:53 nl.defaults.trainer]: \u001b[0mEpoch 8-49, Train loss: 2.13427, validation loss: 2.11637, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/27 10:16:58 nl.defaults.trainer]: \u001b[0mEpoch 8-79, Train loss: 2.15798, validation loss: 2.13244, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/27 10:17:03 nl.defaults.trainer]: \u001b[0mEpoch 8-109, Train loss: 2.13786, validation loss: 2.13149, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/27 10:17:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3104, -0.3095, -0.3250,  0.4238], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2678, -0.2808, -0.2361,  0.3493], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:17:08 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 31.37714, 77.09429, Validation accuracy: 31.13310, 76.79630\n",
      "\u001b[32m[06/27 10:17:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.310408, -0.309541, -0.324951, +0.423788, 3\n",
      "-0.267807, -0.280778, -0.236067, +0.349268, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:17:08 nl.defaults.trainer]: \u001b[0mEpoch 9-2, Train loss: 2.14989, validation loss: 2.13193, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/27 10:17:13 nl.defaults.trainer]: \u001b[0mEpoch 9-32, Train loss: 2.09706, validation loss: 2.11908, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/27 10:17:18 nl.defaults.trainer]: \u001b[0mEpoch 9-62, Train loss: 2.14401, validation loss: 2.13814, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/27 10:17:23 nl.defaults.trainer]: \u001b[0mEpoch 9-92, Train loss: 2.15556, validation loss: 2.11178, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/27 10:17:29 nl.defaults.trainer]: \u001b[0mEpoch 9-122, Train loss: 2.06828, validation loss: 2.13375, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/27 10:17:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3023, -0.2954, -0.3226,  0.4186], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2718, -0.2814, -0.2380,  0.3563], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:17:31 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 33.05714, 78.30000, Validation accuracy: 32.68704, 78.02521\n",
      "\u001b[32m[06/27 10:17:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.302309, -0.295353, -0.322637, +0.418639, 3\n",
      "-0.271821, -0.281436, -0.238046, +0.356293, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:17:34 nl.defaults.trainer]: \u001b[0mEpoch 10-15, Train loss: 2.08994, validation loss: 2.11230, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/27 10:17:39 nl.defaults.trainer]: \u001b[0mEpoch 10-45, Train loss: 2.13877, validation loss: 2.10707, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/27 10:17:44 nl.defaults.trainer]: \u001b[0mEpoch 10-75, Train loss: 2.11133, validation loss: 2.12578, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/27 10:17:49 nl.defaults.trainer]: \u001b[0mEpoch 10-105, Train loss: 2.11098, validation loss: 2.12155, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/27 10:17:54 nl.defaults.trainer]: \u001b[0mEpoch 10-135, Train loss: 2.13215, validation loss: 2.16036, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/27 10:17:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2962, -0.2816, -0.3181,  0.4127], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2824, -0.2831, -0.2363,  0.3648], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:17:54 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 34.14286, 79.67143, Validation accuracy: 34.07562, 79.68750\n",
      "\u001b[32m[06/27 10:17:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.296244, -0.281603, -0.318130, +0.412746, 3\n",
      "-0.282449, -0.283141, -0.236336, +0.364763, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:17:59 nl.defaults.trainer]: \u001b[0mEpoch 11-28, Train loss: 2.13985, validation loss: 2.07202, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/27 10:18:05 nl.defaults.trainer]: \u001b[0mEpoch 11-58, Train loss: 2.08869, validation loss: 2.08159, learning rate: [0.02429056922745071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:18:10 nl.defaults.trainer]: \u001b[0mEpoch 11-88, Train loss: 2.12945, validation loss: 2.11816, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/27 10:18:15 nl.defaults.trainer]: \u001b[0mEpoch 11-118, Train loss: 2.11600, validation loss: 2.10283, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/27 10:18:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2908, -0.2723, -0.3173,  0.4096], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2758, -0.2630, -0.2219,  0.3516], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:18:18 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 35.25429, 81.03429, Validation accuracy: 34.34934, 80.16081\n",
      "\u001b[32m[06/27 10:18:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.290776, -0.272266, -0.317259, +0.409648, 3\n",
      "-0.275819, -0.262987, -0.221904, +0.351612, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:18:20 nl.defaults.trainer]: \u001b[0mEpoch 12-10, Train loss: 2.13896, validation loss: 2.11856, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/27 10:18:25 nl.defaults.trainer]: \u001b[0mEpoch 12-40, Train loss: 2.09369, validation loss: 2.06976, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/27 10:18:30 nl.defaults.trainer]: \u001b[0mEpoch 12-69, Train loss: 2.08710, validation loss: 2.14064, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/27 10:18:35 nl.defaults.trainer]: \u001b[0mEpoch 12-99, Train loss: 2.10923, validation loss: 2.11849, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/27 10:18:40 nl.defaults.trainer]: \u001b[0mEpoch 12-129, Train loss: 2.05766, validation loss: 2.12024, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/27 10:18:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2692, -0.2522, -0.2984,  0.3888], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2790, -0.2542, -0.2140,  0.3493], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:18:41 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 35.58571, 81.72571, Validation accuracy: 35.18191, 81.13880\n",
      "\u001b[32m[06/27 10:18:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.269184, -0.252244, -0.298355, +0.388785, 3\n",
      "-0.278963, -0.254194, -0.214033, +0.349298, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:18:45 nl.defaults.trainer]: \u001b[0mEpoch 13-21, Train loss: 2.08160, validation loss: 2.07522, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/27 10:18:50 nl.defaults.trainer]: \u001b[0mEpoch 13-51, Train loss: 2.12646, validation loss: 2.08533, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/27 10:18:55 nl.defaults.trainer]: \u001b[0mEpoch 13-81, Train loss: 2.05330, validation loss: 2.09280, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/27 10:19:00 nl.defaults.trainer]: \u001b[0mEpoch 13-111, Train loss: 2.13190, validation loss: 2.06693, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/27 10:19:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2618, -0.2425, -0.2962,  0.3834], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2867, -0.2528, -0.2146,  0.3554], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:19:05 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 36.68000, 82.32286, Validation accuracy: 36.92404, 82.29357\n",
      "\u001b[32m[06/27 10:19:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.261783, -0.242539, -0.296187, +0.383419, 3\n",
      "-0.286702, -0.252820, -0.214564, +0.355430, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:19:06 nl.defaults.trainer]: \u001b[0mEpoch 14-3, Train loss: 2.09980, validation loss: 2.07338, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/27 10:19:11 nl.defaults.trainer]: \u001b[0mEpoch 14-33, Train loss: 2.07407, validation loss: 2.07742, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/27 10:19:16 nl.defaults.trainer]: \u001b[0mEpoch 14-63, Train loss: 2.07366, validation loss: 2.06474, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/27 10:19:21 nl.defaults.trainer]: \u001b[0mEpoch 14-93, Train loss: 2.12910, validation loss: 2.07086, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/27 10:19:26 nl.defaults.trainer]: \u001b[0mEpoch 14-122, Train loss: 2.03966, validation loss: 1.99380, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/27 10:19:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2580, -0.2331, -0.2889,  0.3774], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2816, -0.2413, -0.2196,  0.3533], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:19:28 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 37.47429, 83.35714, Validation accuracy: 37.45153, 83.26015\n",
      "\u001b[32m[06/27 10:19:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.258020, -0.233052, -0.288931, +0.377398, 3\n",
      "-0.281552, -0.241269, -0.219563, +0.353331, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:19:31 nl.defaults.trainer]: \u001b[0mEpoch 15-15, Train loss: 2.09298, validation loss: 2.13383, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/27 10:19:36 nl.defaults.trainer]: \u001b[0mEpoch 15-45, Train loss: 2.10433, validation loss: 2.07506, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/27 10:19:41 nl.defaults.trainer]: \u001b[0mEpoch 15-75, Train loss: 2.06525, validation loss: 2.06284, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/27 10:19:46 nl.defaults.trainer]: \u001b[0mEpoch 15-105, Train loss: 2.00628, validation loss: 2.04196, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/27 10:19:51 nl.defaults.trainer]: \u001b[0mEpoch 15-135, Train loss: 2.01825, validation loss: 2.12334, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/27 10:19:51 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2584, -0.2334, -0.2885,  0.3792], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2760, -0.2279, -0.2130,  0.3451], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:19:51 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 38.71143, 84.32000, Validation accuracy: 38.41811, 84.00433\n",
      "\u001b[32m[06/27 10:19:51 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.258439, -0.233403, -0.288511, +0.379182, 3\n",
      "-0.276019, -0.227950, -0.212967, +0.345118, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:19:56 nl.defaults.trainer]: \u001b[0mEpoch 16-28, Train loss: 2.03930, validation loss: 2.08310, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/27 10:20:01 nl.defaults.trainer]: \u001b[0mEpoch 16-58, Train loss: 2.07558, validation loss: 2.05924, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/27 10:20:07 nl.defaults.trainer]: \u001b[0mEpoch 16-88, Train loss: 2.08101, validation loss: 2.08487, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/27 10:20:12 nl.defaults.trainer]: \u001b[0mEpoch 16-118, Train loss: 2.09344, validation loss: 2.07295, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/27 10:20:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2401, -0.2213, -0.2856,  0.3676], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2807, -0.2235, -0.2147,  0.3485], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:20:15 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 39.70571, 85.50286, Validation accuracy: 38.78593, 84.73426\n",
      "\u001b[32m[06/27 10:20:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.240116, -0.221261, -0.285590, +0.367627, 3\n",
      "-0.280740, -0.223478, -0.214697, +0.348491, 3\n",
      "+0.000000, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:20:17 nl.defaults.trainer]: \u001b[0mEpoch 17-11, Train loss: 2.08191, validation loss: 2.02331, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/27 10:20:22 nl.defaults.trainer]: \u001b[0mEpoch 17-42, Train loss: 2.06835, validation loss: 2.09555, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/27 10:20:27 nl.defaults.trainer]: \u001b[0mEpoch 17-72, Train loss: 2.03738, validation loss: 2.03639, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/27 10:20:32 nl.defaults.trainer]: \u001b[0mEpoch 17-102, Train loss: 2.05480, validation loss: 2.05313, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/27 10:20:37 nl.defaults.trainer]: \u001b[0mEpoch 17-132, Train loss: 2.04586, validation loss: 2.03606, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/27 10:20:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2299, -0.2131, -0.2790,  0.3594], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2752, -0.2143, -0.2151,  0.3447], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:20:38 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 40.64571, 86.45714, Validation accuracy: 40.61930, 85.94035\n",
      "\u001b[32m[06/27 10:20:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.229880, -0.213105, -0.278955, +0.359372, 3\n",
      "-0.275161, -0.214319, -0.215124, +0.344731, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:20:42 nl.defaults.trainer]: \u001b[0mEpoch 18-25, Train loss: 2.08707, validation loss: 2.02496, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/27 10:20:47 nl.defaults.trainer]: \u001b[0mEpoch 18-55, Train loss: 2.03234, validation loss: 2.03779, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/27 10:20:52 nl.defaults.trainer]: \u001b[0mEpoch 18-85, Train loss: 2.03555, validation loss: 2.00940, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/27 10:20:57 nl.defaults.trainer]: \u001b[0mEpoch 18-115, Train loss: 2.02501, validation loss: 2.08680, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/27 10:21:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2220, -0.2079, -0.2765,  0.3545], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2730, -0.2119, -0.2185,  0.3464], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:21:01 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 41.42000, 86.97714, Validation accuracy: 41.40340, 86.84991\n",
      "\u001b[32m[06/27 10:21:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.222031, -0.207915, -0.276496, +0.354470, 3\n",
      "-0.273028, -0.211945, -0.218486, +0.346369, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:21:02 nl.defaults.trainer]: \u001b[0mEpoch 19-8, Train loss: 2.03385, validation loss: 2.02456, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/27 10:21:07 nl.defaults.trainer]: \u001b[0mEpoch 19-38, Train loss: 2.04400, validation loss: 2.05837, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/27 10:21:13 nl.defaults.trainer]: \u001b[0mEpoch 19-69, Train loss: 2.00235, validation loss: 2.06851, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/27 10:21:18 nl.defaults.trainer]: \u001b[0mEpoch 19-99, Train loss: 1.99883, validation loss: 2.04276, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/27 10:21:23 nl.defaults.trainer]: \u001b[0mEpoch 19-129, Train loss: 2.01365, validation loss: 2.00367, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/27 10:21:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2199, -0.2057, -0.2712,  0.3520], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2736, -0.2159, -0.2245,  0.3524], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:21:24 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 41.99143, 87.41714, Validation accuracy: 41.87671, 87.33177\n",
      "\u001b[32m[06/27 10:21:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.219910, -0.205702, -0.271168, +0.351969, 3\n",
      "-0.273563, -0.215868, -0.224483, +0.352414, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:21:28 nl.defaults.trainer]: \u001b[0mEpoch 20-22, Train loss: 2.02164, validation loss: 2.09174, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/27 10:21:33 nl.defaults.trainer]: \u001b[0mEpoch 20-52, Train loss: 1.99504, validation loss: 2.02337, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/27 10:21:38 nl.defaults.trainer]: \u001b[0mEpoch 20-82, Train loss: 1.99386, validation loss: 2.05106, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/27 10:21:43 nl.defaults.trainer]: \u001b[0mEpoch 20-112, Train loss: 1.99885, validation loss: 2.04628, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/27 10:21:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2151, -0.2001, -0.2592,  0.3449], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2667, -0.2146, -0.2312,  0.3536], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:21:47 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 42.90000, 88.09714, Validation accuracy: 42.72639, 87.99042\n",
      "\u001b[32m[06/27 10:21:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.215147, -0.200131, -0.259183, +0.344880, 3\n",
      "-0.266737, -0.214557, -0.231205, +0.353601, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:21:48 nl.defaults.trainer]: \u001b[0mEpoch 21-4, Train loss: 2.02325, validation loss: 1.99038, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/27 10:21:53 nl.defaults.trainer]: \u001b[0mEpoch 21-35, Train loss: 2.05116, validation loss: 2.03521, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/27 10:21:58 nl.defaults.trainer]: \u001b[0mEpoch 21-65, Train loss: 2.07571, validation loss: 2.03753, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/27 10:22:03 nl.defaults.trainer]: \u001b[0mEpoch 21-95, Train loss: 2.03766, validation loss: 2.02507, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/27 10:22:08 nl.defaults.trainer]: \u001b[0mEpoch 21-126, Train loss: 2.01820, validation loss: 2.06473, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/27 10:22:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2063, -0.1942, -0.2557,  0.3387], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2692, -0.2219, -0.2352,  0.3608], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:22:10 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy (top1, top5): 43.34000, 88.67714, Validation accuracy: 43.30520, 88.38675\n",
      "\u001b[32m[06/27 10:22:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.206297, -0.194172, -0.255670, +0.338707, 3\n",
      "-0.269229, -0.221931, -0.235217, +0.360780, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:22:13 nl.defaults.trainer]: \u001b[0mEpoch 22-19, Train loss: 2.03327, validation loss: 2.02163, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/27 10:22:18 nl.defaults.trainer]: \u001b[0mEpoch 22-49, Train loss: 1.98621, validation loss: 2.05763, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/27 10:22:23 nl.defaults.trainer]: \u001b[0mEpoch 22-79, Train loss: 2.02543, validation loss: 2.00990, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/27 10:22:29 nl.defaults.trainer]: \u001b[0mEpoch 22-109, Train loss: 1.99487, validation loss: 2.00236, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/27 10:22:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1918, -0.1816, -0.2466,  0.3258], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2655, -0.2153, -0.2397,  0.3602], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:22:33 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy (top1, top5): 44.21714, 89.14571, Validation accuracy: 43.45347, 88.74031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:22:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.191816, -0.181581, -0.246612, +0.325780, 3\n",
      "-0.265507, -0.215335, -0.239729, +0.360228, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:22:34 nl.defaults.trainer]: \u001b[0mEpoch 23-2, Train loss: 2.03155, validation loss: 2.00680, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/27 10:22:39 nl.defaults.trainer]: \u001b[0mEpoch 23-32, Train loss: 2.00301, validation loss: 2.04757, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/27 10:22:44 nl.defaults.trainer]: \u001b[0mEpoch 23-62, Train loss: 2.02952, validation loss: 2.03021, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/27 10:22:49 nl.defaults.trainer]: \u001b[0mEpoch 23-92, Train loss: 1.97464, validation loss: 2.05167, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/27 10:22:54 nl.defaults.trainer]: \u001b[0mEpoch 23-122, Train loss: 2.04171, validation loss: 2.03158, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/27 10:22:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1884, -0.1786, -0.2441,  0.3231], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2639, -0.2093, -0.2401,  0.3590], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:22:56 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy (top1, top5): 44.80571, 89.56857, Validation accuracy: 43.89256, 89.20506\n",
      "\u001b[32m[06/27 10:22:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.188415, -0.178568, -0.244143, +0.323110, 3\n",
      "-0.263902, -0.209335, -0.240081, +0.359000, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:22:59 nl.defaults.trainer]: \u001b[0mEpoch 24-15, Train loss: 2.01538, validation loss: 1.96943, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/27 10:23:04 nl.defaults.trainer]: \u001b[0mEpoch 24-45, Train loss: 2.00358, validation loss: 1.97214, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/27 10:23:09 nl.defaults.trainer]: \u001b[0mEpoch 24-75, Train loss: 2.03249, validation loss: 2.00898, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/27 10:23:14 nl.defaults.trainer]: \u001b[0mEpoch 24-105, Train loss: 2.02408, validation loss: 2.00869, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/27 10:23:19 nl.defaults.trainer]: \u001b[0mEpoch 24-135, Train loss: 2.01333, validation loss: 1.97191, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/27 10:23:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1732, -0.1626, -0.2334,  0.3078], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2641, -0.2026, -0.2466,  0.3607], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:23:19 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy (top1, top5): 45.28857, 89.63143, Validation accuracy: 44.26038, 89.44742\n",
      "\u001b[32m[06/27 10:23:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.173190, -0.162556, -0.233360, +0.307830, 3\n",
      "-0.264112, -0.202623, -0.246553, +0.360680, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:23:24 nl.defaults.trainer]: \u001b[0mEpoch 25-28, Train loss: 1.99620, validation loss: 1.99198, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/27 10:23:29 nl.defaults.trainer]: \u001b[0mEpoch 25-58, Train loss: 2.01158, validation loss: 1.99118, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/27 10:23:34 nl.defaults.trainer]: \u001b[0mEpoch 25-88, Train loss: 1.99247, validation loss: 2.00607, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/27 10:23:39 nl.defaults.trainer]: \u001b[0mEpoch 25-118, Train loss: 2.00019, validation loss: 2.02993, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/27 10:23:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1653, -0.1550, -0.2246,  0.2994], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2667, -0.1990, -0.2516,  0.3639], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:23:43 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy (top1, top5): 45.79714, 90.08000, Validation accuracy: 45.44366, 89.56432\n",
      "\u001b[32m[06/27 10:23:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.165250, -0.154998, -0.224566, +0.299351, 3\n",
      "-0.266721, -0.198982, -0.251624, +0.363923, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:23:44 nl.defaults.trainer]: \u001b[0mEpoch 26-10, Train loss: 1.96888, validation loss: 2.02259, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/27 10:23:50 nl.defaults.trainer]: \u001b[0mEpoch 26-40, Train loss: 2.04679, validation loss: 2.02997, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/27 10:23:55 nl.defaults.trainer]: \u001b[0mEpoch 26-70, Train loss: 1.93353, validation loss: 1.98468, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/27 10:24:00 nl.defaults.trainer]: \u001b[0mEpoch 26-101, Train loss: 1.99460, validation loss: 2.02885, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/27 10:24:05 nl.defaults.trainer]: \u001b[0mEpoch 26-131, Train loss: 1.99649, validation loss: 2.04202, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/27 10:24:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1531, -0.1477, -0.2178,  0.2899], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2694, -0.2001, -0.2557,  0.3685], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:24:06 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy (top1, top5): 46.27429, 89.92857, Validation accuracy: 45.40374, 89.77247\n",
      "\u001b[32m[06/27 10:24:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.153062, -0.147732, -0.217812, +0.289911, 3\n",
      "-0.269411, -0.200108, -0.255714, +0.368507, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:24:10 nl.defaults.trainer]: \u001b[0mEpoch 27-24, Train loss: 1.97634, validation loss: 1.99301, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/27 10:24:15 nl.defaults.trainer]: \u001b[0mEpoch 27-54, Train loss: 1.99686, validation loss: 2.03338, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/27 10:24:20 nl.defaults.trainer]: \u001b[0mEpoch 27-85, Train loss: 1.97624, validation loss: 2.00982, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/27 10:24:25 nl.defaults.trainer]: \u001b[0mEpoch 27-115, Train loss: 2.01593, validation loss: 1.94496, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/27 10:24:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1380, -0.1332, -0.2112,  0.2766], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2708, -0.1982, -0.2609,  0.3719], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:24:29 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy (top1, top5): 46.68000, 90.40571, Validation accuracy: 45.91412, 90.27714\n",
      "\u001b[32m[06/27 10:24:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.138030, -0.133204, -0.211241, +0.276649, 3\n",
      "-0.270782, -0.198166, -0.260876, +0.371851, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:24:30 nl.defaults.trainer]: \u001b[0mEpoch 28-8, Train loss: 1.98853, validation loss: 2.01717, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/27 10:24:35 nl.defaults.trainer]: \u001b[0mEpoch 28-38, Train loss: 2.01877, validation loss: 1.99459, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/27 10:24:40 nl.defaults.trainer]: \u001b[0mEpoch 28-68, Train loss: 1.98862, validation loss: 1.97756, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/27 10:24:46 nl.defaults.trainer]: \u001b[0mEpoch 28-98, Train loss: 1.99501, validation loss: 1.97998, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/27 10:24:51 nl.defaults.trainer]: \u001b[0mEpoch 28-128, Train loss: 1.98930, validation loss: 2.01476, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/27 10:24:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1271, -0.1276, -0.2010,  0.2671], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2672, -0.1952, -0.2585,  0.3697], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:24:52 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy (top1, top5): 47.51143, 90.51143, Validation accuracy: 47.11736, 90.47958\n",
      "\u001b[32m[06/27 10:24:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.127149, -0.127609, -0.201032, +0.267145, 3\n",
      "-0.267184, -0.195244, -0.258498, +0.369733, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:24:56 nl.defaults.trainer]: \u001b[0mEpoch 29-21, Train loss: 1.98537, validation loss: 2.02530, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/27 10:25:01 nl.defaults.trainer]: \u001b[0mEpoch 29-51, Train loss: 2.01823, validation loss: 2.00136, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/27 10:25:06 nl.defaults.trainer]: \u001b[0mEpoch 29-81, Train loss: 1.94557, validation loss: 1.98259, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/27 10:25:11 nl.defaults.trainer]: \u001b[0mEpoch 29-112, Train loss: 2.02485, validation loss: 1.95680, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/27 10:25:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1121, -0.1134, -0.1867,  0.2513], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2735, -0.1992, -0.2674,  0.3788], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:25:15 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy (top1, top5): 48.14000, 90.75714, Validation accuracy: 47.44240, 90.21156\n",
      "\u001b[32m[06/27 10:25:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.112051, -0.113417, -0.186662, +0.251310, 3\n",
      "-0.273527, -0.199224, -0.267422, +0.378801, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:25:16 nl.defaults.trainer]: \u001b[0mEpoch 30-5, Train loss: 1.98333, validation loss: 1.96687, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/27 10:25:21 nl.defaults.trainer]: \u001b[0mEpoch 30-35, Train loss: 1.98771, validation loss: 1.95877, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/27 10:25:26 nl.defaults.trainer]: \u001b[0mEpoch 30-66, Train loss: 1.99716, validation loss: 1.99830, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/27 10:25:31 nl.defaults.trainer]: \u001b[0mEpoch 30-96, Train loss: 1.96831, validation loss: 2.03399, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/27 10:25:36 nl.defaults.trainer]: \u001b[0mEpoch 30-126, Train loss: 1.97937, validation loss: 1.93288, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/27 10:25:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1045, -0.0999, -0.1852,  0.2430], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2741, -0.1958, -0.2711,  0.3806], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:25:38 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy (top1, top5): 48.21429, 90.90000, Validation accuracy: 47.55360, 90.65351\n",
      "\u001b[32m[06/27 10:25:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.104471, -0.099931, -0.185175, +0.242955, 3\n",
      "-0.274112, -0.195824, -0.271071, +0.380599, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:25:41 nl.defaults.trainer]: \u001b[0mEpoch 31-19, Train loss: 1.96978, validation loss: 1.94859, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/27 10:25:47 nl.defaults.trainer]: \u001b[0mEpoch 31-50, Train loss: 2.00156, validation loss: 1.95825, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/27 10:25:52 nl.defaults.trainer]: \u001b[0mEpoch 31-80, Train loss: 1.95030, validation loss: 1.96991, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/27 10:25:57 nl.defaults.trainer]: \u001b[0mEpoch 31-111, Train loss: 2.00486, validation loss: 1.95089, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/27 10:26:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0951, -0.0920, -0.1773,  0.2338], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2688, -0.1905, -0.2709,  0.3779], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:26:01 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy (top1, top5): 48.96571, 91.10857, Validation accuracy: 48.39758, 91.08406\n",
      "\u001b[32m[06/27 10:26:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.095122, -0.091981, -0.177292, +0.233808, 3\n",
      "-0.268844, -0.190518, -0.270941, +0.377862, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:26:02 nl.defaults.trainer]: \u001b[0mEpoch 32-4, Train loss: 1.94211, validation loss: 2.00577, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/27 10:26:07 nl.defaults.trainer]: \u001b[0mEpoch 32-35, Train loss: 2.00403, validation loss: 1.97381, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/27 10:26:12 nl.defaults.trainer]: \u001b[0mEpoch 32-65, Train loss: 1.94619, validation loss: 1.94250, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/27 10:26:17 nl.defaults.trainer]: \u001b[0mEpoch 32-95, Train loss: 1.97463, validation loss: 1.96083, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/27 10:26:22 nl.defaults.trainer]: \u001b[0mEpoch 32-125, Train loss: 1.99838, validation loss: 1.97820, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/27 10:26:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1027, -0.0954, -0.1805,  0.2395], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2691, -0.1877, -0.2706,  0.3780], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:26:24 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy (top1, top5): 49.62857, 91.37714, Validation accuracy: 48.67701, 90.93294\n",
      "\u001b[32m[06/27 10:26:24 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.102695, -0.095442, -0.180491, +0.239483, 3\n",
      "-0.269110, -0.187679, -0.270593, +0.377993, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:26:27 nl.defaults.trainer]: \u001b[0mEpoch 33-18, Train loss: 1.94227, validation loss: 2.00578, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/27 10:26:32 nl.defaults.trainer]: \u001b[0mEpoch 33-49, Train loss: 1.93770, validation loss: 1.99460, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/27 10:26:37 nl.defaults.trainer]: \u001b[0mEpoch 33-79, Train loss: 1.97781, validation loss: 1.99112, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/27 10:26:42 nl.defaults.trainer]: \u001b[0mEpoch 33-109, Train loss: 1.99209, validation loss: 1.97033, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/27 10:26:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0903, -0.0922, -0.1705,  0.2301], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2679, -0.1861, -0.2736,  0.3794], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:26:47 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy (top1, top5): 49.76571, 91.44857, Validation accuracy: 48.66275, 91.08406\n",
      "\u001b[32m[06/27 10:26:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.090276, -0.092171, -0.170485, +0.230065, 3\n",
      "-0.267931, -0.186095, -0.273614, +0.379363, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:26:48 nl.defaults.trainer]: \u001b[0mEpoch 34-2, Train loss: 1.90592, validation loss: 1.98265, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/27 10:26:53 nl.defaults.trainer]: \u001b[0mEpoch 34-32, Train loss: 1.94000, validation loss: 1.92089, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/27 10:26:58 nl.defaults.trainer]: \u001b[0mEpoch 34-62, Train loss: 1.93882, validation loss: 1.98543, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/27 10:27:03 nl.defaults.trainer]: \u001b[0mEpoch 34-93, Train loss: 1.97864, validation loss: 1.93435, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/27 10:27:08 nl.defaults.trainer]: \u001b[0mEpoch 34-123, Train loss: 1.95033, validation loss: 1.97043, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/27 10:27:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0895, -0.0868, -0.1661,  0.2265], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2685, -0.1859, -0.2721,  0.3801], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:27:10 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy (top1, top5): 50.53143, 91.77429, Validation accuracy: 49.42119, 91.72274\n",
      "\u001b[32m[06/27 10:27:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.089456, -0.086846, -0.166087, +0.226517, 3\n",
      "-0.268539, -0.185934, -0.272110, +0.380117, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:27:13 nl.defaults.trainer]: \u001b[0mEpoch 35-16, Train loss: 1.97568, validation loss: 1.94666, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/27 10:27:18 nl.defaults.trainer]: \u001b[0mEpoch 35-46, Train loss: 1.94615, validation loss: 1.95124, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/27 10:27:23 nl.defaults.trainer]: \u001b[0mEpoch 35-76, Train loss: 1.96707, validation loss: 2.00272, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/27 10:27:28 nl.defaults.trainer]: \u001b[0mEpoch 35-106, Train loss: 1.92623, validation loss: 1.91695, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/27 10:27:33 nl.defaults.trainer]: \u001b[0mEpoch 35-136, Train loss: 1.93643, validation loss: 1.93432, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/27 10:27:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0832, -0.0845, -0.1633,  0.2223], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2610, -0.1811, -0.2670,  0.3745], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:27:33 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy (top1, top5): 50.80857, 91.79714, Validation accuracy: 49.73198, 91.40625\n",
      "\u001b[32m[06/27 10:27:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.083204, -0.084514, -0.163267, +0.222250, 3\n",
      "-0.260957, -0.181071, -0.267036, +0.374482, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:27:38 nl.defaults.trainer]: \u001b[0mEpoch 36-29, Train loss: 1.93407, validation loss: 1.99802, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/27 10:27:44 nl.defaults.trainer]: \u001b[0mEpoch 36-59, Train loss: 1.94209, validation loss: 1.98084, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/27 10:27:49 nl.defaults.trainer]: \u001b[0mEpoch 36-89, Train loss: 1.99410, validation loss: 1.93596, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/27 10:27:54 nl.defaults.trainer]: \u001b[0mEpoch 36-119, Train loss: 1.94792, validation loss: 1.94316, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/27 10:27:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0773, -0.0766, -0.1617,  0.2163], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2576, -0.1752, -0.2668,  0.3719], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:27:57 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy (top1, top5): 51.06857, 91.99143, Validation accuracy: 49.98859, 91.85960\n",
      "\u001b[32m[06/27 10:27:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.077251, -0.076588, -0.161673, +0.216343, 3\n",
      "-0.257631, -0.175172, -0.266823, +0.371853, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:27:59 nl.defaults.trainer]: \u001b[0mEpoch 37-12, Train loss: 1.96948, validation loss: 1.96536, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/27 10:28:04 nl.defaults.trainer]: \u001b[0mEpoch 37-42, Train loss: 1.93280, validation loss: 1.91242, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/27 10:28:09 nl.defaults.trainer]: \u001b[0mEpoch 37-72, Train loss: 1.98875, validation loss: 1.95954, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/27 10:28:14 nl.defaults.trainer]: \u001b[0mEpoch 37-102, Train loss: 1.92900, validation loss: 1.95078, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/27 10:28:19 nl.defaults.trainer]: \u001b[0mEpoch 37-132, Train loss: 1.94162, validation loss: 1.93909, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/27 10:28:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0767, -0.0654, -0.1628,  0.2123], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2600, -0.1764, -0.2703,  0.3757], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:28:20 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy (top1, top5): 51.77429, 92.11143, Validation accuracy: 50.80976, 91.98791\n",
      "\u001b[32m[06/27 10:28:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.076690, -0.065361, -0.162822, +0.212334, 3\n",
      "-0.259968, -0.176420, -0.270311, +0.375666, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:28:24 nl.defaults.trainer]: \u001b[0mEpoch 38-25, Train loss: 1.93624, validation loss: 1.98066, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/27 10:28:29 nl.defaults.trainer]: \u001b[0mEpoch 38-55, Train loss: 1.96432, validation loss: 1.98973, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/27 10:28:34 nl.defaults.trainer]: \u001b[0mEpoch 38-85, Train loss: 1.94248, validation loss: 1.96497, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/27 10:28:39 nl.defaults.trainer]: \u001b[0mEpoch 38-115, Train loss: 1.95548, validation loss: 1.95669, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/27 10:28:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0732, -0.0670, -0.1607,  0.2109], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2572, -0.1727, -0.2655,  0.3723], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:28:43 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy (top1, top5): 52.20857, 92.14286, Validation accuracy: 51.09489, 91.92803\n",
      "\u001b[32m[06/27 10:28:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.073200, -0.066983, -0.160725, +0.210907, 3\n",
      "-0.257194, -0.172672, -0.265467, +0.372271, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:28:45 nl.defaults.trainer]: \u001b[0mEpoch 39-8, Train loss: 1.89308, validation loss: 1.96212, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/27 10:28:50 nl.defaults.trainer]: \u001b[0mEpoch 39-38, Train loss: 1.91747, validation loss: 1.93930, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/27 10:28:55 nl.defaults.trainer]: \u001b[0mEpoch 39-68, Train loss: 1.94759, validation loss: 1.93805, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/27 10:29:00 nl.defaults.trainer]: \u001b[0mEpoch 39-98, Train loss: 1.94276, validation loss: 1.90548, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/27 10:29:05 nl.defaults.trainer]: \u001b[0mEpoch 39-128, Train loss: 1.97839, validation loss: 1.92657, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/27 10:29:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0630, -0.0649, -0.1526,  0.2031], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2596, -0.1735, -0.2700,  0.3763], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:29:06 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy (top1, top5): 52.32286, 92.41714, Validation accuracy: 51.59672, 92.36713\n",
      "\u001b[32m[06/27 10:29:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.063043, -0.064879, -0.152597, +0.203102, 3\n",
      "-0.259572, -0.173455, -0.270007, +0.376266, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:29:10 nl.defaults.trainer]: \u001b[0mEpoch 40-21, Train loss: 1.91074, validation loss: 2.00093, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/27 10:29:15 nl.defaults.trainer]: \u001b[0mEpoch 40-51, Train loss: 1.93666, validation loss: 1.95702, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/27 10:29:20 nl.defaults.trainer]: \u001b[0mEpoch 40-82, Train loss: 1.99946, validation loss: 1.93516, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/27 10:29:25 nl.defaults.trainer]: \u001b[0mEpoch 40-112, Train loss: 1.92604, validation loss: 1.92815, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/27 10:29:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0536, -0.0565, -0.1485,  0.1943], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2648, -0.1756, -0.2743,  0.3817], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:29:29 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy (top1, top5): 52.67143, 92.34571, Validation accuracy: 51.38572, 92.10196\n",
      "\u001b[32m[06/27 10:29:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.053584, -0.056476, -0.148490, +0.194349, 3\n",
      "-0.264828, -0.175606, -0.274311, +0.381728, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:29:30 nl.defaults.trainer]: \u001b[0mEpoch 41-5, Train loss: 1.92735, validation loss: 1.91318, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/27 10:29:36 nl.defaults.trainer]: \u001b[0mEpoch 41-36, Train loss: 1.93593, validation loss: 1.95087, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/27 10:29:41 nl.defaults.trainer]: \u001b[0mEpoch 41-66, Train loss: 1.95679, validation loss: 1.92707, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/27 10:29:46 nl.defaults.trainer]: \u001b[0mEpoch 41-96, Train loss: 1.84390, validation loss: 1.97401, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/27 10:29:51 nl.defaults.trainer]: \u001b[0mEpoch 41-127, Train loss: 1.94938, validation loss: 1.94189, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/27 10:29:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0614, -0.0594, -0.1464,  0.1985], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2656, -0.1727, -0.2736,  0.3816], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:29:52 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy (top1, top5): 53.27429, 92.48286, Validation accuracy: 52.49487, 92.48403\n",
      "\u001b[32m[06/27 10:29:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.061361, -0.059436, -0.146414, +0.198503, 3\n",
      "-0.265564, -0.172729, -0.273561, +0.381560, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:29:56 nl.defaults.trainer]: \u001b[0mEpoch 42-20, Train loss: 1.91132, validation loss: 1.93264, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/27 10:30:01 nl.defaults.trainer]: \u001b[0mEpoch 42-50, Train loss: 1.94770, validation loss: 1.96594, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/27 10:30:06 nl.defaults.trainer]: \u001b[0mEpoch 42-80, Train loss: 1.95380, validation loss: 1.95007, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/27 10:30:11 nl.defaults.trainer]: \u001b[0mEpoch 42-110, Train loss: 1.90929, validation loss: 1.91706, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/27 10:30:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0578, -0.0600, -0.1437,  0.1964], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2691, -0.1715, -0.2760,  0.3843], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:30:16 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy (top1, top5): 53.67714, 92.58857, Validation accuracy: 52.48061, 92.54391\n",
      "\u001b[32m[06/27 10:30:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.057820, -0.060027, -0.143729, +0.196383, 3\n",
      "-0.269141, -0.171505, -0.276016, +0.384333, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:30:16 nl.defaults.trainer]: \u001b[0mEpoch 43-3, Train loss: 1.92582, validation loss: 1.92046, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/27 10:30:21 nl.defaults.trainer]: \u001b[0mEpoch 43-34, Train loss: 1.92321, validation loss: 1.93444, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/27 10:30:27 nl.defaults.trainer]: \u001b[0mEpoch 43-65, Train loss: 1.90267, validation loss: 1.89133, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/27 10:30:32 nl.defaults.trainer]: \u001b[0mEpoch 43-96, Train loss: 1.91991, validation loss: 1.92605, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/27 10:30:37 nl.defaults.trainer]: \u001b[0mEpoch 43-126, Train loss: 1.93241, validation loss: 1.97044, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/27 10:30:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0594, -0.0622, -0.1397,  0.1968], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2732, -0.1698, -0.2814,  0.3882], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:30:39 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy (top1, top5): 53.48286, 92.54571, Validation accuracy: 52.12420, 92.60949\n",
      "\u001b[32m[06/27 10:30:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.059397, -0.062206, -0.139685, +0.196781, 3\n",
      "-0.273225, -0.169757, -0.281398, +0.388161, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:30:42 nl.defaults.trainer]: \u001b[0mEpoch 44-19, Train loss: 1.95405, validation loss: 1.94678, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/27 10:30:47 nl.defaults.trainer]: \u001b[0mEpoch 44-49, Train loss: 1.92747, validation loss: 1.93751, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/27 10:30:52 nl.defaults.trainer]: \u001b[0mEpoch 44-79, Train loss: 1.93276, validation loss: 1.94358, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/27 10:30:57 nl.defaults.trainer]: \u001b[0mEpoch 44-109, Train loss: 1.90582, validation loss: 1.95304, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/27 10:31:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0553, -0.0557, -0.1352,  0.1910], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2773, -0.1670, -0.2822,  0.3898], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:31:02 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy (top1, top5): 53.75429, 92.79143, Validation accuracy: 53.37021, 92.64656\n",
      "\u001b[32m[06/27 10:31:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.055294, -0.055693, -0.135153, +0.190998, 3\n",
      "-0.277261, -0.167024, -0.282187, +0.389791, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:31:02 nl.defaults.trainer]: \u001b[0mEpoch 45-2, Train loss: 1.90369, validation loss: 1.89280, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/27 10:31:07 nl.defaults.trainer]: \u001b[0mEpoch 45-32, Train loss: 1.95316, validation loss: 1.90886, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/27 10:31:12 nl.defaults.trainer]: \u001b[0mEpoch 45-62, Train loss: 1.90818, validation loss: 1.91026, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/27 10:31:18 nl.defaults.trainer]: \u001b[0mEpoch 45-92, Train loss: 1.89014, validation loss: 1.89028, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/27 10:31:23 nl.defaults.trainer]: \u001b[0mEpoch 45-122, Train loss: 1.93289, validation loss: 1.94347, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/27 10:31:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0537, -0.0561, -0.1308,  0.1891], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2733, -0.1627, -0.2766,  0.3855], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:31:25 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy (top1, top5): 54.40857, 92.76857, Validation accuracy: 53.29608, 92.72924\n",
      "\u001b[32m[06/27 10:31:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.053680, -0.056138, -0.130812, +0.189106, 3\n",
      "-0.273259, -0.162746, -0.276617, +0.385470, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:31:28 nl.defaults.trainer]: \u001b[0mEpoch 46-15, Train loss: 1.91705, validation loss: 1.94319, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/27 10:31:33 nl.defaults.trainer]: \u001b[0mEpoch 46-45, Train loss: 1.91389, validation loss: 1.88492, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/27 10:31:38 nl.defaults.trainer]: \u001b[0mEpoch 46-75, Train loss: 1.93219, validation loss: 1.93117, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/27 10:31:43 nl.defaults.trainer]: \u001b[0mEpoch 46-105, Train loss: 1.90469, validation loss: 1.90626, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/27 10:31:48 nl.defaults.trainer]: \u001b[0mEpoch 46-135, Train loss: 1.93120, validation loss: 1.88259, learning rate: [0.014503998802771652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:31:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0472, -0.0518, -0.1277,  0.1834], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2704, -0.1547, -0.2745,  0.3812], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:31:48 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy (top1, top5): 54.57143, 92.97429, Validation accuracy: 53.84352, 92.83474\n",
      "\u001b[32m[06/27 10:31:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.047159, -0.051820, -0.127711, +0.183417, 3\n",
      "-0.270356, -0.154651, -0.274472, +0.381169, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:31:53 nl.defaults.trainer]: \u001b[0mEpoch 47-28, Train loss: 1.85741, validation loss: 1.91557, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/27 10:31:58 nl.defaults.trainer]: \u001b[0mEpoch 47-58, Train loss: 1.91024, validation loss: 1.91062, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/27 10:32:03 nl.defaults.trainer]: \u001b[0mEpoch 47-88, Train loss: 1.91087, validation loss: 1.94116, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/27 10:32:08 nl.defaults.trainer]: \u001b[0mEpoch 47-118, Train loss: 1.91937, validation loss: 1.92791, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/27 10:32:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0377, -0.0488, -0.1248,  0.1769], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2663, -0.1538, -0.2725,  0.3795], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:32:11 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy (top1, top5): 55.00286, 93.12286, Validation accuracy: 53.52703, 92.85185\n",
      "\u001b[32m[06/27 10:32:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.037650, -0.048793, -0.124819, +0.176902, 3\n",
      "-0.266348, -0.153780, -0.272526, +0.379528, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:32:13 nl.defaults.trainer]: \u001b[0mEpoch 48-11, Train loss: 1.94063, validation loss: 1.93113, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/27 10:32:18 nl.defaults.trainer]: \u001b[0mEpoch 48-41, Train loss: 1.86918, validation loss: 1.94831, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/27 10:32:23 nl.defaults.trainer]: \u001b[0mEpoch 48-71, Train loss: 1.89553, validation loss: 1.96651, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/27 10:32:29 nl.defaults.trainer]: \u001b[0mEpoch 48-101, Train loss: 1.90638, validation loss: 1.91233, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/27 10:32:34 nl.defaults.trainer]: \u001b[0mEpoch 48-131, Train loss: 1.91178, validation loss: 1.87153, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/27 10:32:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0356, -0.0438, -0.1160,  0.1712], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2622, -0.1517, -0.2680,  0.3764], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:32:34 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy (top1, top5): 55.06857, 93.25714, Validation accuracy: 53.76654, 93.13127\n",
      "\u001b[32m[06/27 10:32:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.035611, -0.043824, -0.116040, +0.171214, 3\n",
      "-0.262206, -0.151725, -0.267998, +0.376383, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:32:39 nl.defaults.trainer]: \u001b[0mEpoch 49-24, Train loss: 1.82668, validation loss: 1.92385, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/27 10:32:44 nl.defaults.trainer]: \u001b[0mEpoch 49-54, Train loss: 1.95118, validation loss: 1.96425, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/27 10:32:49 nl.defaults.trainer]: \u001b[0mEpoch 49-84, Train loss: 1.87981, validation loss: 1.93115, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/27 10:32:54 nl.defaults.trainer]: \u001b[0mEpoch 49-114, Train loss: 1.94540, validation loss: 1.89377, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/27 10:32:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0338, -0.0460, -0.1153,  0.1710], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2627, -0.1552, -0.2714,  0.3801], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:32:58 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy (top1, top5): 55.39714, 93.35714, Validation accuracy: 53.98894, 92.89177\n",
      "\u001b[32m[06/27 10:32:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.033804, -0.045961, -0.115325, +0.171009, 3\n",
      "-0.262717, -0.155163, -0.271442, +0.380108, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:32:59 nl.defaults.trainer]: \u001b[0mEpoch 50-6, Train loss: 1.85503, validation loss: 1.89500, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/27 10:33:04 nl.defaults.trainer]: \u001b[0mEpoch 50-36, Train loss: 1.89299, validation loss: 1.87790, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/27 10:33:09 nl.defaults.trainer]: \u001b[0mEpoch 50-66, Train loss: 1.90097, validation loss: 1.91881, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/27 10:33:14 nl.defaults.trainer]: \u001b[0mEpoch 50-96, Train loss: 1.88460, validation loss: 1.86389, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/27 10:33:19 nl.defaults.trainer]: \u001b[0mEpoch 50-126, Train loss: 1.92889, validation loss: 1.91704, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/27 10:33:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0206, -0.0316, -0.1076,  0.1565], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2668, -0.1593, -0.2789,  0.3868], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:33:21 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy (top1, top5): 55.83429, 93.31143, Validation accuracy: 54.37101, 92.99441\n",
      "\u001b[32m[06/27 10:33:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.020561, -0.031559, -0.107562, +0.156535, 3\n",
      "-0.266827, -0.159257, -0.278866, +0.386834, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:33:24 nl.defaults.trainer]: \u001b[0mEpoch 51-19, Train loss: 1.89619, validation loss: 1.89398, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/27 10:33:29 nl.defaults.trainer]: \u001b[0mEpoch 51-49, Train loss: 1.89983, validation loss: 1.91995, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/27 10:33:34 nl.defaults.trainer]: \u001b[0mEpoch 51-79, Train loss: 1.90784, validation loss: 1.92523, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/27 10:33:39 nl.defaults.trainer]: \u001b[0mEpoch 51-109, Train loss: 1.87770, validation loss: 1.89392, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/27 10:33:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0141, -0.0181, -0.1035,  0.1467], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2589, -0.1512, -0.2776,  0.3810], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:33:44 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy (top1, top5): 56.01429, 93.39429, Validation accuracy: 54.37671, 93.04003\n",
      "\u001b[32m[06/27 10:33:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.014055, -0.018079, -0.103518, +0.146718, 3\n",
      "-0.258909, -0.151241, -0.277553, +0.380964, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:33:45 nl.defaults.trainer]: \u001b[0mEpoch 52-2, Train loss: 1.87088, validation loss: 1.91641, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/27 10:33:50 nl.defaults.trainer]: \u001b[0mEpoch 52-32, Train loss: 1.87072, validation loss: 1.91459, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/27 10:33:55 nl.defaults.trainer]: \u001b[0mEpoch 52-62, Train loss: 1.90147, validation loss: 1.89031, learning rate: [0.012246513765648233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:34:00 nl.defaults.trainer]: \u001b[0mEpoch 52-92, Train loss: 1.92628, validation loss: 1.90777, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/27 10:34:05 nl.defaults.trainer]: \u001b[0mEpoch 52-122, Train loss: 1.86966, validation loss: 1.93218, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/27 10:34:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0054, -0.0084, -0.0959,  0.1362], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2535, -0.1526, -0.2721,  0.3785], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:34:07 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy (top1, top5): 56.21143, 93.44286, Validation accuracy: 55.15511, 93.09991\n",
      "\u001b[32m[06/27 10:34:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.005351, -0.008439, -0.095880, +0.136159, 3\n",
      "-0.253488, -0.152644, -0.272052, +0.378470, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:34:10 nl.defaults.trainer]: \u001b[0mEpoch 53-15, Train loss: 1.88156, validation loss: 1.90900, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/27 10:34:15 nl.defaults.trainer]: \u001b[0mEpoch 53-45, Train loss: 1.89025, validation loss: 1.89044, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/27 10:34:20 nl.defaults.trainer]: \u001b[0mEpoch 53-75, Train loss: 1.88493, validation loss: 1.95388, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/27 10:34:25 nl.defaults.trainer]: \u001b[0mEpoch 53-105, Train loss: 1.88964, validation loss: 1.90197, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/27 10:34:30 nl.defaults.trainer]: \u001b[0mEpoch 53-135, Train loss: 1.86809, validation loss: 1.91774, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/27 10:34:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0003, -0.0096, -0.0989,  0.1352], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2499, -0.1476, -0.2730,  0.3761], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:34:31 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy (top1, top5): 56.90571, 93.52571, Validation accuracy: 55.59135, 93.31661\n",
      "\u001b[32m[06/27 10:34:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000280, -0.009560, -0.098916, +0.135202, 3\n",
      "-0.249850, -0.147580, -0.272957, +0.376137, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:34:35 nl.defaults.trainer]: \u001b[0mEpoch 54-28, Train loss: 1.89024, validation loss: 1.89248, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/27 10:34:41 nl.defaults.trainer]: \u001b[0mEpoch 54-58, Train loss: 1.91832, validation loss: 1.86463, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/27 10:34:46 nl.defaults.trainer]: \u001b[0mEpoch 54-88, Train loss: 1.89971, validation loss: 1.89377, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/27 10:34:51 nl.defaults.trainer]: \u001b[0mEpoch 54-118, Train loss: 1.90437, validation loss: 1.88567, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/27 10:34:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0076, -0.0048, -0.1002,  0.1299], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2506, -0.1488, -0.2726,  0.3776], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:34:54 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy (top1, top5): 56.88286, 93.30571, Validation accuracy: 55.53148, 93.48483\n",
      "\u001b[32m[06/27 10:34:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.007586, -0.004819, -0.100200, +0.129859, 3\n",
      "-0.250616, -0.148804, -0.272626, +0.377628, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:34:56 nl.defaults.trainer]: \u001b[0mEpoch 55-11, Train loss: 1.84557, validation loss: 1.90081, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/27 10:35:01 nl.defaults.trainer]: \u001b[0mEpoch 55-42, Train loss: 1.88492, validation loss: 1.93778, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/27 10:35:06 nl.defaults.trainer]: \u001b[0mEpoch 55-72, Train loss: 1.85875, validation loss: 1.90339, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/27 10:35:11 nl.defaults.trainer]: \u001b[0mEpoch 55-103, Train loss: 1.88170, validation loss: 1.87264, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/27 10:35:16 nl.defaults.trainer]: \u001b[0mEpoch 55-134, Train loss: 1.92611, validation loss: 1.87268, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/27 10:35:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 1.2529e-02, -1.0327e-04, -9.7473e-02,  1.2450e-01], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2553, -0.1508, -0.2730,  0.3811], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:35:17 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy (top1, top5): 56.96857, 93.68571, Validation accuracy: 55.85082, 93.41355\n",
      "\u001b[32m[06/27 10:35:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.012529, -0.000103, -0.097473, +0.124504, 3\n",
      "-0.255254, -0.150795, -0.273014, +0.381059, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:35:21 nl.defaults.trainer]: \u001b[0mEpoch 56-27, Train loss: 1.91596, validation loss: 1.93295, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/27 10:35:26 nl.defaults.trainer]: \u001b[0mEpoch 56-57, Train loss: 1.89039, validation loss: 1.89508, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/27 10:35:32 nl.defaults.trainer]: \u001b[0mEpoch 56-87, Train loss: 1.87440, validation loss: 1.87905, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/27 10:35:37 nl.defaults.trainer]: \u001b[0mEpoch 56-116, Train loss: 1.83999, validation loss: 1.89521, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/27 10:35:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0173, -0.0033, -0.0969,  0.1233], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2559, -0.1546, -0.2754,  0.3845], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:35:40 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy (top1, top5): 57.53714, 93.72571, Validation accuracy: 56.00479, 93.36223\n",
      "\u001b[32m[06/27 10:35:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.017259, -0.003283, -0.096879, +0.123312, 3\n",
      "-0.255890, -0.154638, -0.275358, +0.384525, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:35:42 nl.defaults.trainer]: \u001b[0mEpoch 57-9, Train loss: 1.88283, validation loss: 1.92079, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/27 10:35:47 nl.defaults.trainer]: \u001b[0mEpoch 57-39, Train loss: 1.88295, validation loss: 1.89665, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/27 10:35:52 nl.defaults.trainer]: \u001b[0mEpoch 57-69, Train loss: 1.86237, validation loss: 1.88439, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/27 10:35:57 nl.defaults.trainer]: \u001b[0mEpoch 57-99, Train loss: 1.89252, validation loss: 1.89373, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/27 10:36:02 nl.defaults.trainer]: \u001b[0mEpoch 57-129, Train loss: 1.87709, validation loss: 1.94175, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/27 10:36:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0166, -0.0014, -0.0983,  0.1232], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2648, -0.1562, -0.2831,  0.3919], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:36:03 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy (top1, top5): 57.67429, 93.65714, Validation accuracy: 56.21293, 93.49053\n",
      "\u001b[32m[06/27 10:36:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.016578, -0.001376, -0.098261, +0.123225, 3\n",
      "-0.264804, -0.156169, -0.283133, +0.391936, 3\n",
      "+0.000000, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:36:07 nl.defaults.trainer]: \u001b[0mEpoch 58-22, Train loss: 1.87376, validation loss: 1.90462, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/27 10:36:12 nl.defaults.trainer]: \u001b[0mEpoch 58-52, Train loss: 1.93280, validation loss: 1.91972, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/27 10:36:18 nl.defaults.trainer]: \u001b[0mEpoch 58-82, Train loss: 1.92041, validation loss: 1.89420, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/27 10:36:23 nl.defaults.trainer]: \u001b[0mEpoch 58-112, Train loss: 1.87793, validation loss: 1.89864, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/27 10:36:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0189,  0.0024, -0.0950,  0.1194], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2631, -0.1515, -0.2832,  0.3902], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:36:27 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy (top1, top5): 57.70286, 93.78857, Validation accuracy: 55.90214, 93.52190\n",
      "\u001b[32m[06/27 10:36:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.018852, +0.002408, -0.095030, +0.119388, 3\n",
      "-0.263088, -0.151510, -0.283181, +0.390163, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:36:28 nl.defaults.trainer]: \u001b[0mEpoch 59-4, Train loss: 1.89121, validation loss: 1.91258, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/27 10:36:33 nl.defaults.trainer]: \u001b[0mEpoch 59-34, Train loss: 1.89807, validation loss: 1.89047, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/27 10:36:38 nl.defaults.trainer]: \u001b[0mEpoch 59-64, Train loss: 1.84430, validation loss: 1.90812, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/27 10:36:43 nl.defaults.trainer]: \u001b[0mEpoch 59-94, Train loss: 1.91852, validation loss: 1.90728, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/27 10:36:48 nl.defaults.trainer]: \u001b[0mEpoch 59-124, Train loss: 1.82525, validation loss: 1.88004, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/27 10:36:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0227,  0.0115, -0.0892,  0.1116], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2602, -0.1526, -0.2863,  0.3915], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:36:50 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy (top1, top5): 58.18857, 93.62857, Validation accuracy: 56.11599, 93.57322\n",
      "\u001b[32m[06/27 10:36:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.022661, +0.011491, -0.089201, +0.111610, 3\n",
      "-0.260246, -0.152620, -0.286290, +0.391474, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:36:53 nl.defaults.trainer]: \u001b[0mEpoch 60-16, Train loss: 1.89593, validation loss: 1.94460, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/27 10:36:59 nl.defaults.trainer]: \u001b[0mEpoch 60-46, Train loss: 1.87945, validation loss: 1.95782, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/27 10:37:04 nl.defaults.trainer]: \u001b[0mEpoch 60-76, Train loss: 1.86432, validation loss: 1.86910, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/27 10:37:09 nl.defaults.trainer]: \u001b[0mEpoch 60-106, Train loss: 1.87436, validation loss: 1.88588, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/27 10:37:14 nl.defaults.trainer]: \u001b[0mEpoch 60-135, Train loss: 1.83122, validation loss: 1.87907, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/27 10:37:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0261,  0.0131, -0.0842,  0.1074], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2590, -0.1522, -0.2902,  0.3930], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:37:14 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy (top1, top5): 58.41714, 93.83714, Validation accuracy: 56.62922, 93.66446\n",
      "\u001b[32m[06/27 10:37:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.026129, +0.013087, -0.084215, +0.107424, 3\n",
      "-0.258975, -0.152164, -0.290247, +0.392978, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:37:19 nl.defaults.trainer]: \u001b[0mEpoch 61-28, Train loss: 1.88741, validation loss: 1.90960, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/27 10:37:24 nl.defaults.trainer]: \u001b[0mEpoch 61-57, Train loss: 1.89168, validation loss: 1.90613, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/27 10:37:29 nl.defaults.trainer]: \u001b[0mEpoch 61-87, Train loss: 1.87772, validation loss: 1.89956, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/27 10:37:34 nl.defaults.trainer]: \u001b[0mEpoch 61-116, Train loss: 1.86510, validation loss: 1.88882, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/27 10:37:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0243,  0.0144, -0.0878,  0.1089], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2552, -0.1477, -0.2865,  0.3892], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:37:37 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy (top1, top5): 58.38286, 93.93143, Validation accuracy: 56.67769, 93.77566\n",
      "\u001b[32m[06/27 10:37:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.024285, +0.014415, -0.087833, +0.108928, 3\n",
      "-0.255160, -0.147724, -0.286522, +0.389222, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:37:39 nl.defaults.trainer]: \u001b[0mEpoch 62-9, Train loss: 1.87927, validation loss: 1.89574, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/27 10:37:44 nl.defaults.trainer]: \u001b[0mEpoch 62-39, Train loss: 1.90852, validation loss: 1.87994, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/27 10:37:49 nl.defaults.trainer]: \u001b[0mEpoch 62-69, Train loss: 1.87874, validation loss: 1.86860, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/27 10:37:55 nl.defaults.trainer]: \u001b[0mEpoch 62-99, Train loss: 1.84096, validation loss: 1.88170, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/27 10:38:00 nl.defaults.trainer]: \u001b[0mEpoch 62-128, Train loss: 1.90353, validation loss: 1.86858, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/27 10:38:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0324,  0.0226, -0.0934,  0.1032], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2459, -0.1424, -0.2785,  0.3816], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:38:01 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy (top1, top5): 58.21143, 93.90286, Validation accuracy: 56.12454, 93.61884\n",
      "\u001b[32m[06/27 10:38:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.032354, +0.022567, -0.093357, +0.103209, 3\n",
      "-0.245875, -0.142355, -0.278459, +0.381613, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:38:05 nl.defaults.trainer]: \u001b[0mEpoch 63-20, Train loss: 1.91208, validation loss: 1.91508, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/27 10:38:10 nl.defaults.trainer]: \u001b[0mEpoch 63-49, Train loss: 1.88701, validation loss: 1.90441, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/27 10:38:15 nl.defaults.trainer]: \u001b[0mEpoch 63-79, Train loss: 1.91514, validation loss: 1.88546, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/27 10:38:20 nl.defaults.trainer]: \u001b[0mEpoch 63-108, Train loss: 1.82901, validation loss: 1.87994, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/27 10:38:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0346,  0.0266, -0.0933,  0.1002], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2419, -0.1399, -0.2748,  0.3786], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:38:25 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy (top1, top5): 59.15714, 93.86571, Validation accuracy: 57.59295, 93.88971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:38:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.034585, +0.026617, -0.093265, +0.100196, 3\n",
      "-0.241882, -0.139880, -0.274825, +0.378599, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:38:25 nl.defaults.trainer]: \u001b[0mEpoch 64-0, Train loss: 1.92323, validation loss: 1.85493, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/27 10:38:30 nl.defaults.trainer]: \u001b[0mEpoch 64-30, Train loss: 1.86775, validation loss: 1.90693, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/27 10:38:35 nl.defaults.trainer]: \u001b[0mEpoch 64-60, Train loss: 1.86908, validation loss: 1.90975, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/27 10:38:40 nl.defaults.trainer]: \u001b[0mEpoch 64-90, Train loss: 1.88292, validation loss: 1.88692, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/27 10:38:45 nl.defaults.trainer]: \u001b[0mEpoch 64-120, Train loss: 1.85365, validation loss: 1.92398, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/27 10:38:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0428,  0.0311, -0.0942,  0.0944], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2465, -0.1483, -0.2800,  0.3863], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:38:48 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy (top1, top5): 59.01714, 94.03714, Validation accuracy: 57.67849, 93.66446\n",
      "\u001b[32m[06/27 10:38:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.042806, +0.031099, -0.094224, +0.094408, 3\n",
      "-0.246499, -0.148256, -0.280017, +0.386326, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:38:50 nl.defaults.trainer]: \u001b[0mEpoch 65-12, Train loss: 1.89677, validation loss: 1.87740, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/27 10:38:55 nl.defaults.trainer]: \u001b[0mEpoch 65-42, Train loss: 1.90128, validation loss: 1.89945, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/27 10:39:00 nl.defaults.trainer]: \u001b[0mEpoch 65-72, Train loss: 1.86259, validation loss: 1.86156, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/27 10:39:05 nl.defaults.trainer]: \u001b[0mEpoch 65-102, Train loss: 1.88685, validation loss: 1.88502, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/27 10:39:10 nl.defaults.trainer]: \u001b[0mEpoch 65-132, Train loss: 1.85914, validation loss: 1.90869, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/27 10:39:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0473,  0.0334, -0.0906,  0.0898], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2441, -0.1483, -0.2806,  0.3864], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:39:11 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy (top1, top5): 59.44571, 93.94571, Validation accuracy: 57.79539, 93.75285\n",
      "\u001b[32m[06/27 10:39:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.047302, +0.033371, -0.090639, +0.089775, 3\n",
      "-0.244133, -0.148273, -0.280559, +0.386394, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:39:15 nl.defaults.trainer]: \u001b[0mEpoch 66-24, Train loss: 1.90904, validation loss: 1.87206, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/27 10:39:20 nl.defaults.trainer]: \u001b[0mEpoch 66-54, Train loss: 1.86397, validation loss: 1.88857, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/27 10:39:25 nl.defaults.trainer]: \u001b[0mEpoch 66-84, Train loss: 1.84035, validation loss: 1.88390, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/27 10:39:31 nl.defaults.trainer]: \u001b[0mEpoch 66-114, Train loss: 1.89115, validation loss: 1.87615, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/27 10:39:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0530,  0.0403, -0.0875,  0.0826], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2421, -0.1495, -0.2768,  0.3856], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:39:34 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy (top1, top5): 59.38857, 94.12000, Validation accuracy: 58.23449, 93.90112\n",
      "\u001b[32m[06/27 10:39:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.052982, +0.040334, -0.087472, +0.082617, 3\n",
      "-0.242105, -0.149481, -0.276803, +0.385648, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:39:36 nl.defaults.trainer]: \u001b[0mEpoch 67-7, Train loss: 1.88717, validation loss: 1.90706, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/27 10:39:41 nl.defaults.trainer]: \u001b[0mEpoch 67-37, Train loss: 1.89459, validation loss: 1.89986, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/27 10:39:46 nl.defaults.trainer]: \u001b[0mEpoch 67-67, Train loss: 1.86420, validation loss: 1.88224, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/27 10:39:51 nl.defaults.trainer]: \u001b[0mEpoch 67-97, Train loss: 1.84939, validation loss: 1.81437, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/27 10:39:56 nl.defaults.trainer]: \u001b[0mEpoch 67-127, Train loss: 1.83769, validation loss: 1.85631, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/27 10:39:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0598,  0.0430, -0.0881,  0.0781], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2402, -0.1487, -0.2778,  0.3857], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:39:58 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy (top1, top5): 59.24857, 93.95429, Validation accuracy: 57.78114, 93.63595\n",
      "\u001b[32m[06/27 10:39:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.059800, +0.042999, -0.088058, +0.078106, 3\n",
      "-0.240196, -0.148663, -0.277832, +0.385666, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:40:01 nl.defaults.trainer]: \u001b[0mEpoch 68-19, Train loss: 1.86585, validation loss: 1.84541, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/27 10:40:06 nl.defaults.trainer]: \u001b[0mEpoch 68-49, Train loss: 1.88324, validation loss: 1.90276, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/27 10:40:11 nl.defaults.trainer]: \u001b[0mEpoch 68-79, Train loss: 1.90662, validation loss: 1.89760, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/27 10:40:16 nl.defaults.trainer]: \u001b[0mEpoch 68-109, Train loss: 1.89058, validation loss: 1.88061, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/27 10:40:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0672,  0.0537, -0.0762,  0.0653], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2472, -0.1550, -0.2907,  0.3959], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:40:21 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy (top1, top5): 59.58286, 94.01143, Validation accuracy: 57.92370, 94.21476\n",
      "\u001b[32m[06/27 10:40:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.067246, +0.053708, -0.076223, +0.065347, 0\n",
      "-0.247189, -0.155020, -0.290719, +0.395925, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:40:22 nl.defaults.trainer]: \u001b[0mEpoch 69-2, Train loss: 1.85912, validation loss: 1.91029, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/27 10:40:27 nl.defaults.trainer]: \u001b[0mEpoch 69-32, Train loss: 1.87807, validation loss: 1.86994, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/27 10:40:32 nl.defaults.trainer]: \u001b[0mEpoch 69-62, Train loss: 1.81736, validation loss: 1.89388, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/27 10:40:37 nl.defaults.trainer]: \u001b[0mEpoch 69-92, Train loss: 1.85293, validation loss: 1.90310, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/27 10:40:42 nl.defaults.trainer]: \u001b[0mEpoch 69-122, Train loss: 1.84426, validation loss: 1.87738, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/27 10:40:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0796,  0.0602, -0.0739,  0.0553], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2441, -0.1530, -0.2894,  0.3942], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:40:44 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy (top1, top5): 59.74000, 94.14286, Validation accuracy: 58.22879, 93.90682\n",
      "\u001b[32m[06/27 10:40:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.079594, +0.060192, -0.073885, +0.055281, 0\n",
      "-0.244054, -0.153009, -0.289380, +0.394228, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:40:47 nl.defaults.trainer]: \u001b[0mEpoch 70-15, Train loss: 1.83823, validation loss: 1.92398, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/27 10:40:52 nl.defaults.trainer]: \u001b[0mEpoch 70-45, Train loss: 1.82258, validation loss: 1.82930, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/27 10:40:57 nl.defaults.trainer]: \u001b[0mEpoch 70-75, Train loss: 1.86532, validation loss: 1.84459, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/27 10:41:02 nl.defaults.trainer]: \u001b[0mEpoch 70-104, Train loss: 1.85803, validation loss: 1.87528, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/27 10:41:07 nl.defaults.trainer]: \u001b[0mEpoch 70-134, Train loss: 1.89577, validation loss: 1.93563, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/27 10:41:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0855,  0.0658, -0.0690,  0.0479], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2469, -0.1576, -0.2892,  0.3978], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:41:08 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy (top1, top5): 59.95143, 94.10286, Validation accuracy: 58.59090, 93.97810\n",
      "\u001b[32m[06/27 10:41:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.085541, +0.065768, -0.069036, +0.047875, 0\n",
      "-0.246912, -0.157587, -0.289198, +0.397837, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:41:12 nl.defaults.trainer]: \u001b[0mEpoch 71-27, Train loss: 1.89527, validation loss: 1.86649, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/27 10:41:17 nl.defaults.trainer]: \u001b[0mEpoch 71-57, Train loss: 1.86068, validation loss: 1.87009, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/27 10:41:23 nl.defaults.trainer]: \u001b[0mEpoch 71-87, Train loss: 1.85145, validation loss: 1.85880, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/27 10:41:28 nl.defaults.trainer]: \u001b[0mEpoch 71-117, Train loss: 1.81831, validation loss: 1.87560, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/27 10:41:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0919,  0.0641, -0.0724,  0.0463], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2483, -0.1607, -0.2922,  0.4014], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:41:31 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy (top1, top5): 60.26571, 94.31714, Validation accuracy: 58.44548, 93.92963\n",
      "\u001b[32m[06/27 10:41:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.091940, +0.064146, -0.072355, +0.046335, 0\n",
      "-0.248258, -0.160678, -0.292220, +0.401374, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:41:33 nl.defaults.trainer]: \u001b[0mEpoch 72-9, Train loss: 1.84966, validation loss: 1.87804, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/27 10:41:38 nl.defaults.trainer]: \u001b[0mEpoch 72-39, Train loss: 1.87738, validation loss: 1.90455, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/27 10:41:43 nl.defaults.trainer]: \u001b[0mEpoch 72-69, Train loss: 1.88236, validation loss: 1.87991, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/27 10:41:48 nl.defaults.trainer]: \u001b[0mEpoch 72-99, Train loss: 1.84293, validation loss: 1.92349, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/27 10:41:53 nl.defaults.trainer]: \u001b[0mEpoch 72-129, Train loss: 1.83675, validation loss: 1.87718, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/27 10:41:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0950,  0.0710, -0.0702,  0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2515, -0.1617, -0.2990,  0.4060], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:41:54 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy (top1, top5): 60.49143, 94.32857, Validation accuracy: 58.60230, 94.10071\n",
      "\u001b[32m[06/27 10:41:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.094987, +0.071022, -0.070225, +0.040676, 0\n",
      "-0.251476, -0.161742, -0.298951, +0.406004, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:41:58 nl.defaults.trainer]: \u001b[0mEpoch 73-22, Train loss: 1.85880, validation loss: 1.86854, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/27 10:42:03 nl.defaults.trainer]: \u001b[0mEpoch 73-52, Train loss: 1.87817, validation loss: 1.86268, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/27 10:42:08 nl.defaults.trainer]: \u001b[0mEpoch 73-82, Train loss: 1.79252, validation loss: 1.90701, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/27 10:42:14 nl.defaults.trainer]: \u001b[0mEpoch 73-113, Train loss: 1.87244, validation loss: 1.89383, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/27 10:42:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1008,  0.0786, -0.0659,  0.0325], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2483, -0.1656, -0.3011,  0.4081], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:42:17 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy (top1, top5): 60.54857, 94.22000, Validation accuracy: 58.86462, 94.05794\n",
      "\u001b[32m[06/27 10:42:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.100765, +0.078575, -0.065927, +0.032508, 0\n",
      "-0.248284, -0.165649, -0.301064, +0.408062, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:42:19 nl.defaults.trainer]: \u001b[0mEpoch 74-6, Train loss: 1.84487, validation loss: 1.86602, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/27 10:42:24 nl.defaults.trainer]: \u001b[0mEpoch 74-36, Train loss: 1.86895, validation loss: 1.86110, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/27 10:42:29 nl.defaults.trainer]: \u001b[0mEpoch 74-66, Train loss: 1.86843, validation loss: 1.85889, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/27 10:42:34 nl.defaults.trainer]: \u001b[0mEpoch 74-96, Train loss: 1.84105, validation loss: 1.84528, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/27 10:42:39 nl.defaults.trainer]: \u001b[0mEpoch 74-126, Train loss: 1.89130, validation loss: 1.85953, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/27 10:42:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1067,  0.0856, -0.0661,  0.0260], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2514, -0.1696, -0.3050,  0.4130], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:42:41 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy (top1, top5): 60.50571, 94.12857, Validation accuracy: 59.01574, 94.19195\n",
      "\u001b[32m[06/27 10:42:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.106715, +0.085631, -0.066143, +0.026042, 0\n",
      "-0.251408, -0.169595, -0.305009, +0.412960, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:42:44 nl.defaults.trainer]: \u001b[0mEpoch 75-19, Train loss: 1.81435, validation loss: 1.84443, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/27 10:42:49 nl.defaults.trainer]: \u001b[0mEpoch 75-49, Train loss: 1.84688, validation loss: 1.90439, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/27 10:42:54 nl.defaults.trainer]: \u001b[0mEpoch 75-79, Train loss: 1.86209, validation loss: 1.85300, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/27 10:42:59 nl.defaults.trainer]: \u001b[0mEpoch 75-109, Train loss: 1.86660, validation loss: 1.89128, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/27 10:43:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1114,  0.0867, -0.0623,  0.0216], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2553, -0.1780, -0.3124,  0.4212], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:43:04 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy (top1, top5): 60.62571, 94.40857, Validation accuracy: 58.67359, 94.06364\n",
      "\u001b[32m[06/27 10:43:04 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.111382, +0.086710, -0.062312, +0.021563, 0\n",
      "-0.255282, -0.177951, -0.312404, +0.421237, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:43:04 nl.defaults.trainer]: \u001b[0mEpoch 76-2, Train loss: 1.87498, validation loss: 1.86836, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/27 10:43:09 nl.defaults.trainer]: \u001b[0mEpoch 76-32, Train loss: 1.80951, validation loss: 1.92093, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/27 10:43:14 nl.defaults.trainer]: \u001b[0mEpoch 76-62, Train loss: 1.86040, validation loss: 1.88246, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/27 10:43:19 nl.defaults.trainer]: \u001b[0mEpoch 76-92, Train loss: 1.87222, validation loss: 1.91990, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/27 10:43:24 nl.defaults.trainer]: \u001b[0mEpoch 76-122, Train loss: 1.85886, validation loss: 1.85686, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/27 10:43:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1181,  0.0929, -0.0617,  0.0147], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2579, -0.1765, -0.3169,  0.4239], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:43:27 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy (top1, top5): 60.87143, 94.39143, Validation accuracy: 58.96156, 94.18054\n",
      "\u001b[32m[06/27 10:43:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.118147, +0.092922, -0.061702, +0.014668, 0\n",
      "-0.257927, -0.176496, -0.316856, +0.423859, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:43:29 nl.defaults.trainer]: \u001b[0mEpoch 77-15, Train loss: 1.85493, validation loss: 1.83206, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/27 10:43:34 nl.defaults.trainer]: \u001b[0mEpoch 77-44, Train loss: 1.85774, validation loss: 1.83892, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/27 10:43:39 nl.defaults.trainer]: \u001b[0mEpoch 77-74, Train loss: 1.85779, validation loss: 1.83120, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/27 10:43:45 nl.defaults.trainer]: \u001b[0mEpoch 77-104, Train loss: 1.87154, validation loss: 1.85019, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/27 10:43:50 nl.defaults.trainer]: \u001b[0mEpoch 77-134, Train loss: 1.85379, validation loss: 1.89077, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/27 10:43:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1155,  0.0976, -0.0622,  0.0137], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2654, -0.1800, -0.3254,  0.4317], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:43:50 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy (top1, top5): 60.91143, 94.36000, Validation accuracy: 59.59455, 94.44571\n",
      "\u001b[32m[06/27 10:43:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.115541, +0.097614, -0.062195, +0.013705, 0\n",
      "-0.265437, -0.179964, -0.325375, +0.431690, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:43:55 nl.defaults.trainer]: \u001b[0mEpoch 78-27, Train loss: 1.88529, validation loss: 1.86299, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/27 10:44:00 nl.defaults.trainer]: \u001b[0mEpoch 78-58, Train loss: 1.84443, validation loss: 1.90527, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/27 10:44:05 nl.defaults.trainer]: \u001b[0mEpoch 78-88, Train loss: 1.87512, validation loss: 1.84499, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/27 10:44:10 nl.defaults.trainer]: \u001b[0mEpoch 78-118, Train loss: 1.84678, validation loss: 1.88774, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/27 10:44:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1184,  0.1087, -0.0644,  0.0075], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2601, -0.1682, -0.3245,  0.4252], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:44:13 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy (top1, top5): 60.83143, 94.44286, Validation accuracy: 58.89599, 94.03228\n",
      "\u001b[32m[06/27 10:44:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.118425, +0.108692, -0.064416, +0.007511, 0\n",
      "-0.260083, -0.168225, -0.324462, +0.425224, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:44:15 nl.defaults.trainer]: \u001b[0mEpoch 79-11, Train loss: 1.86519, validation loss: 1.89436, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/27 10:44:20 nl.defaults.trainer]: \u001b[0mEpoch 79-41, Train loss: 1.86891, validation loss: 1.86351, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/27 10:44:25 nl.defaults.trainer]: \u001b[0mEpoch 79-71, Train loss: 1.88697, validation loss: 1.88708, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/27 10:44:30 nl.defaults.trainer]: \u001b[0mEpoch 79-101, Train loss: 1.86293, validation loss: 1.87654, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/27 10:44:35 nl.defaults.trainer]: \u001b[0mEpoch 79-131, Train loss: 1.85636, validation loss: 1.88397, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/27 10:44:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1146,  0.1116, -0.0606,  0.0065], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2591, -0.1669, -0.3243,  0.4251], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:44:36 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy (top1, top5): 60.96000, 94.52000, Validation accuracy: 59.13549, 94.57687\n",
      "\u001b[32m[06/27 10:44:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.114572, +0.111585, -0.060612, +0.006546, 0\n",
      "-0.259116, -0.166864, -0.324333, +0.425050, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:44:41 nl.defaults.trainer]: \u001b[0mEpoch 80-24, Train loss: 1.87965, validation loss: 1.85867, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/27 10:44:46 nl.defaults.trainer]: \u001b[0mEpoch 80-54, Train loss: 1.87190, validation loss: 1.87604, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/27 10:44:51 nl.defaults.trainer]: \u001b[0mEpoch 80-84, Train loss: 1.87401, validation loss: 1.85520, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/27 10:44:56 nl.defaults.trainer]: \u001b[0mEpoch 80-114, Train loss: 1.83588, validation loss: 1.83603, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/27 10:44:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1142,  0.1113, -0.0611,  0.0068], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2595, -0.1665, -0.3297,  0.4276], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:44:59 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy (top1, top5): 61.22286, 94.53429, Validation accuracy: 59.54893, 94.41150\n",
      "\u001b[32m[06/27 10:44:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.114203, +0.111318, -0.061072, +0.006785, 0\n",
      "-0.259472, -0.166461, -0.329678, +0.427597, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:45:01 nl.defaults.trainer]: \u001b[0mEpoch 81-7, Train loss: 1.86838, validation loss: 1.88141, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/27 10:45:06 nl.defaults.trainer]: \u001b[0mEpoch 81-37, Train loss: 1.89544, validation loss: 1.90608, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/27 10:45:11 nl.defaults.trainer]: \u001b[0mEpoch 81-67, Train loss: 1.85101, validation loss: 1.83418, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/27 10:45:16 nl.defaults.trainer]: \u001b[0mEpoch 81-97, Train loss: 1.88250, validation loss: 1.88223, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/27 10:45:21 nl.defaults.trainer]: \u001b[0mEpoch 81-127, Train loss: 1.85948, validation loss: 1.85820, learning rate: [0.0030750331087052566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:45:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1223,  0.1131, -0.0561, -0.0004], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2620, -0.1715, -0.3374,  0.4341], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:45:22 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy (top1, top5): 61.30571, 94.39714, Validation accuracy: 59.12409, 94.06079\n",
      "\u001b[32m[06/27 10:45:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.122254, +0.113092, -0.056064, -0.000362, 0\n",
      "-0.262021, -0.171534, -0.337417, +0.434138, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:45:26 nl.defaults.trainer]: \u001b[0mEpoch 82-20, Train loss: 1.86792, validation loss: 1.88708, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/27 10:45:31 nl.defaults.trainer]: \u001b[0mEpoch 82-50, Train loss: 1.86205, validation loss: 1.87226, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/27 10:45:36 nl.defaults.trainer]: \u001b[0mEpoch 82-80, Train loss: 1.82839, validation loss: 1.89295, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/27 10:45:41 nl.defaults.trainer]: \u001b[0mEpoch 82-110, Train loss: 1.87101, validation loss: 1.85578, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/27 10:45:46 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1241,  0.1148, -0.0534, -0.0034], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2709, -0.1730, -0.3439,  0.4408], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:45:46 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy (top1, top5): 61.32000, 94.51429, Validation accuracy: 59.52897, 94.31740\n",
      "\u001b[32m[06/27 10:45:46 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.124138, +0.114814, -0.053412, -0.003423, 0\n",
      "-0.270904, -0.173039, -0.343854, +0.440849, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:45:46 nl.defaults.trainer]: \u001b[0mEpoch 83-3, Train loss: 1.86253, validation loss: 1.87051, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/27 10:45:52 nl.defaults.trainer]: \u001b[0mEpoch 83-33, Train loss: 1.89224, validation loss: 1.89059, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/27 10:45:57 nl.defaults.trainer]: \u001b[0mEpoch 83-63, Train loss: 1.86351, validation loss: 1.85778, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/27 10:46:02 nl.defaults.trainer]: \u001b[0mEpoch 83-93, Train loss: 1.85551, validation loss: 1.92907, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/27 10:46:07 nl.defaults.trainer]: \u001b[0mEpoch 83-123, Train loss: 1.90231, validation loss: 1.87048, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/27 10:46:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1287,  0.1173, -0.0474, -0.0095], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2811, -0.1811, -0.3565,  0.4530], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:46:09 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy (top1, top5): 61.64286, 94.58571, Validation accuracy: 59.68294, 94.31170\n",
      "\u001b[32m[06/27 10:46:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.128746, +0.117329, -0.047386, -0.009491, 0\n",
      "-0.281099, -0.181125, -0.356506, +0.453033, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:46:12 nl.defaults.trainer]: \u001b[0mEpoch 84-16, Train loss: 1.83064, validation loss: 1.87037, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/27 10:46:17 nl.defaults.trainer]: \u001b[0mEpoch 84-46, Train loss: 1.87089, validation loss: 1.91524, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/27 10:46:22 nl.defaults.trainer]: \u001b[0mEpoch 84-76, Train loss: 1.86342, validation loss: 1.84772, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/27 10:46:28 nl.defaults.trainer]: \u001b[0mEpoch 84-106, Train loss: 1.89654, validation loss: 1.83674, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/27 10:46:33 nl.defaults.trainer]: \u001b[0mEpoch 84-136, Train loss: 1.84613, validation loss: 1.90085, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/27 10:46:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1423,  0.1219, -0.0536, -0.0168], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2794, -0.1852, -0.3587,  0.4559], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:46:33 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy (top1, top5): 61.87429, 94.56857, Validation accuracy: 59.71715, 94.23472\n",
      "\u001b[32m[06/27 10:46:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.142310, +0.121928, -0.053587, -0.016793, 0\n",
      "-0.279384, -0.185191, -0.358699, +0.455898, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:46:38 nl.defaults.trainer]: \u001b[0mEpoch 85-29, Train loss: 1.81010, validation loss: 1.85440, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/27 10:46:43 nl.defaults.trainer]: \u001b[0mEpoch 85-59, Train loss: 1.90020, validation loss: 1.83162, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/27 10:46:48 nl.defaults.trainer]: \u001b[0mEpoch 85-89, Train loss: 1.91424, validation loss: 1.86371, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/27 10:46:53 nl.defaults.trainer]: \u001b[0mEpoch 85-119, Train loss: 1.77054, validation loss: 1.84792, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/27 10:46:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1392,  0.1282, -0.0523, -0.0190], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2806, -0.1825, -0.3586,  0.4560], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:46:56 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy (top1, top5): 61.58857, 94.41429, Validation accuracy: 59.18111, 94.09786\n",
      "\u001b[32m[06/27 10:46:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.139155, +0.128201, -0.052280, -0.019041, 0\n",
      "-0.280630, -0.182469, -0.358648, +0.456012, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:46:58 nl.defaults.trainer]: \u001b[0mEpoch 86-11, Train loss: 1.88987, validation loss: 1.84204, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/27 10:47:04 nl.defaults.trainer]: \u001b[0mEpoch 86-41, Train loss: 1.83166, validation loss: 1.86466, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/27 10:47:09 nl.defaults.trainer]: \u001b[0mEpoch 86-71, Train loss: 1.83794, validation loss: 1.82227, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/27 10:47:14 nl.defaults.trainer]: \u001b[0mEpoch 86-101, Train loss: 1.79463, validation loss: 1.86332, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/27 10:47:19 nl.defaults.trainer]: \u001b[0mEpoch 86-131, Train loss: 1.82108, validation loss: 1.89798, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/27 10:47:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1447,  0.1346, -0.0508, -0.0260], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2811, -0.1852, -0.3617,  0.4593], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:47:20 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy (top1, top5): 62.05714, 94.43714, Validation accuracy: 59.60880, 94.22901\n",
      "\u001b[32m[06/27 10:47:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.144687, +0.134639, -0.050816, -0.025972, 0\n",
      "-0.281062, -0.185207, -0.361735, +0.459328, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:47:24 nl.defaults.trainer]: \u001b[0mEpoch 87-23, Train loss: 1.85668, validation loss: 1.83428, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/27 10:47:29 nl.defaults.trainer]: \u001b[0mEpoch 87-53, Train loss: 1.89215, validation loss: 1.86704, learning rate: [0.0019869444917922276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:47:34 nl.defaults.trainer]: \u001b[0mEpoch 87-83, Train loss: 1.82481, validation loss: 1.89604, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/27 10:47:40 nl.defaults.trainer]: \u001b[0mEpoch 87-113, Train loss: 1.84941, validation loss: 1.84690, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/27 10:47:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1461,  0.1363, -0.0527, -0.0271], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2853, -0.1909, -0.3706,  0.4672], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:47:43 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy (top1, top5): 61.47143, 94.50857, Validation accuracy: 60.23038, 94.46567\n",
      "\u001b[32m[06/27 10:47:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.146070, +0.136280, -0.052718, -0.027112, 0\n",
      "-0.285336, -0.190907, -0.370565, +0.467217, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:47:45 nl.defaults.trainer]: \u001b[0mEpoch 88-6, Train loss: 1.84683, validation loss: 1.88079, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/27 10:47:50 nl.defaults.trainer]: \u001b[0mEpoch 88-36, Train loss: 1.80451, validation loss: 1.84838, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/27 10:47:55 nl.defaults.trainer]: \u001b[0mEpoch 88-66, Train loss: 1.89639, validation loss: 1.86197, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/27 10:48:00 nl.defaults.trainer]: \u001b[0mEpoch 88-96, Train loss: 1.86122, validation loss: 1.85938, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/27 10:48:05 nl.defaults.trainer]: \u001b[0mEpoch 88-126, Train loss: 1.83756, validation loss: 1.85435, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/27 10:48:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1525,  0.1365, -0.0471, -0.0329], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2868, -0.1951, -0.3758,  0.4723], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:48:07 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy (top1, top5): 61.88286, 94.46000, Validation accuracy: 59.85401, 94.55406\n",
      "\u001b[32m[06/27 10:48:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.152462, +0.136505, -0.047135, -0.032938, 0\n",
      "-0.286757, -0.195094, -0.375816, +0.472311, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:48:10 nl.defaults.trainer]: \u001b[0mEpoch 89-18, Train loss: 1.86078, validation loss: 1.85585, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/27 10:48:15 nl.defaults.trainer]: \u001b[0mEpoch 89-48, Train loss: 1.83607, validation loss: 1.84709, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/27 10:48:20 nl.defaults.trainer]: \u001b[0mEpoch 89-78, Train loss: 1.90564, validation loss: 1.81977, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/27 10:48:25 nl.defaults.trainer]: \u001b[0mEpoch 89-108, Train loss: 1.86705, validation loss: 1.86954, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/27 10:48:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1529,  0.1325, -0.0461, -0.0318], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2867, -0.1947, -0.3781,  0.4739], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:48:30 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy (top1, top5): 61.71143, 94.67143, Validation accuracy: 59.66298, 94.41150\n",
      "\u001b[32m[06/27 10:48:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.152931, +0.132453, -0.046124, -0.031835, 0\n",
      "-0.286691, -0.194727, -0.378142, +0.473897, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:48:30 nl.defaults.trainer]: \u001b[0mEpoch 90-0, Train loss: 1.88210, validation loss: 1.87306, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/27 10:48:35 nl.defaults.trainer]: \u001b[0mEpoch 90-30, Train loss: 1.87313, validation loss: 1.80769, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/27 10:48:40 nl.defaults.trainer]: \u001b[0mEpoch 90-60, Train loss: 1.83869, validation loss: 1.83169, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/27 10:48:45 nl.defaults.trainer]: \u001b[0mEpoch 90-90, Train loss: 1.84132, validation loss: 1.84092, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/27 10:48:50 nl.defaults.trainer]: \u001b[0mEpoch 90-120, Train loss: 1.86401, validation loss: 1.86050, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/27 10:48:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1622,  0.1380, -0.0435, -0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2855, -0.1934, -0.3838,  0.4758], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:48:53 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy (top1, top5): 61.88000, 94.57429, Validation accuracy: 60.25890, 94.42005\n",
      "\u001b[32m[06/27 10:48:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.162219, +0.137976, -0.043541, -0.040710, 0\n",
      "-0.285510, -0.193434, -0.383831, +0.475828, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:48:55 nl.defaults.trainer]: \u001b[0mEpoch 91-12, Train loss: 1.85716, validation loss: 1.85558, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/27 10:49:01 nl.defaults.trainer]: \u001b[0mEpoch 91-42, Train loss: 1.81985, validation loss: 1.85134, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/27 10:49:06 nl.defaults.trainer]: \u001b[0mEpoch 91-72, Train loss: 1.82100, validation loss: 1.87516, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/27 10:49:11 nl.defaults.trainer]: \u001b[0mEpoch 91-102, Train loss: 1.83168, validation loss: 1.87244, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/27 10:49:16 nl.defaults.trainer]: \u001b[0mEpoch 91-132, Train loss: 1.83013, validation loss: 1.87603, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/27 10:49:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1596,  0.1386, -0.0530, -0.0365], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2899, -0.1961, -0.3903,  0.4817], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:49:16 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy (top1, top5): 62.00857, 94.51143, Validation accuracy: 60.15340, 94.59113\n",
      "\u001b[32m[06/27 10:49:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159632, +0.138612, -0.052976, -0.036512, 0\n",
      "-0.289923, -0.196066, -0.390342, +0.481690, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:49:21 nl.defaults.trainer]: \u001b[0mEpoch 92-24, Train loss: 1.83012, validation loss: 1.82194, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/27 10:49:26 nl.defaults.trainer]: \u001b[0mEpoch 92-54, Train loss: 1.84912, validation loss: 1.85722, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/27 10:49:31 nl.defaults.trainer]: \u001b[0mEpoch 92-84, Train loss: 1.85620, validation loss: 1.87425, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/27 10:49:36 nl.defaults.trainer]: \u001b[0mEpoch 92-114, Train loss: 1.81606, validation loss: 1.83631, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/27 10:49:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1596,  0.1393, -0.0498, -0.0383], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2874, -0.1986, -0.3936,  0.4841], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:49:40 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy (top1, top5): 62.00857, 94.49714, Validation accuracy: 60.46989, 94.54836\n",
      "\u001b[32m[06/27 10:49:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159551, +0.139306, -0.049809, -0.038293, 0\n",
      "-0.287383, -0.198598, -0.393645, +0.484081, 3\n",
      "+0.000000, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:49:41 nl.defaults.trainer]: \u001b[0mEpoch 93-7, Train loss: 1.80829, validation loss: 1.90000, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/27 10:49:46 nl.defaults.trainer]: \u001b[0mEpoch 93-37, Train loss: 1.88105, validation loss: 1.85761, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/27 10:49:51 nl.defaults.trainer]: \u001b[0mEpoch 93-67, Train loss: 1.85404, validation loss: 1.83059, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/27 10:49:56 nl.defaults.trainer]: \u001b[0mEpoch 93-97, Train loss: 1.87670, validation loss: 1.85115, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/27 10:50:01 nl.defaults.trainer]: \u001b[0mEpoch 93-127, Train loss: 1.84609, validation loss: 1.84077, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/27 10:50:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1611,  0.1403, -0.0495, -0.0400], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2886, -0.2017, -0.3964,  0.4878], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:50:03 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy (top1, top5): 62.25143, 94.68857, Validation accuracy: 59.91959, 94.21191\n",
      "\u001b[32m[06/27 10:50:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.161115, +0.140305, -0.049539, -0.040009, 0\n",
      "-0.288555, -0.201710, -0.396384, +0.487808, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:50:06 nl.defaults.trainer]: \u001b[0mEpoch 94-19, Train loss: 1.83624, validation loss: 1.88808, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/27 10:50:11 nl.defaults.trainer]: \u001b[0mEpoch 94-49, Train loss: 1.82279, validation loss: 1.88921, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/27 10:50:17 nl.defaults.trainer]: \u001b[0mEpoch 94-79, Train loss: 1.80221, validation loss: 1.84574, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/27 10:50:22 nl.defaults.trainer]: \u001b[0mEpoch 94-109, Train loss: 1.81083, validation loss: 1.85820, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/27 10:50:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1597,  0.1401, -0.0506, -0.0391], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2909, -0.2032, -0.3997,  0.4914], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:50:26 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy (top1, top5): 62.19143, 94.53429, Validation accuracy: 60.54688, 94.50274\n",
      "\u001b[32m[06/27 10:50:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.159654, +0.140131, -0.050560, -0.039110, 0\n",
      "-0.290892, -0.203222, -0.399683, +0.491407, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:50:27 nl.defaults.trainer]: \u001b[0mEpoch 95-2, Train loss: 1.87397, validation loss: 1.84528, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/27 10:50:32 nl.defaults.trainer]: \u001b[0mEpoch 95-32, Train loss: 1.80956, validation loss: 1.93391, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/27 10:50:37 nl.defaults.trainer]: \u001b[0mEpoch 95-62, Train loss: 1.84098, validation loss: 1.80677, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/27 10:50:42 nl.defaults.trainer]: \u001b[0mEpoch 95-93, Train loss: 1.84009, validation loss: 1.85995, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/27 10:50:47 nl.defaults.trainer]: \u001b[0mEpoch 95-124, Train loss: 1.86595, validation loss: 1.88614, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/27 10:50:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1617,  0.1405, -0.0474, -0.0418], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2936, -0.2075, -0.4069,  0.4977], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:50:49 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy (top1, top5): 62.12000, 94.58286, Validation accuracy: 60.07356, 94.39724\n",
      "\u001b[32m[06/27 10:50:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.161716, +0.140455, -0.047393, -0.041805, 0\n",
      "-0.293574, -0.207505, -0.406945, +0.497719, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:50:52 nl.defaults.trainer]: \u001b[0mEpoch 96-17, Train loss: 1.79364, validation loss: 1.82391, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/27 10:50:57 nl.defaults.trainer]: \u001b[0mEpoch 96-47, Train loss: 1.85936, validation loss: 1.87903, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/27 10:51:03 nl.defaults.trainer]: \u001b[0mEpoch 96-77, Train loss: 1.81830, validation loss: 1.88726, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/27 10:51:08 nl.defaults.trainer]: \u001b[0mEpoch 96-107, Train loss: 1.79386, validation loss: 1.88167, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/27 10:51:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1628,  0.1433, -0.0504, -0.0431], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2900, -0.2019, -0.4049,  0.4944], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:51:13 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy (top1, top5): 62.56571, 94.62571, Validation accuracy: 59.70290, 94.44001\n",
      "\u001b[32m[06/27 10:51:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.162850, +0.143312, -0.050373, -0.043052, 0\n",
      "-0.290041, -0.201948, -0.404910, +0.494376, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:51:13 nl.defaults.trainer]: \u001b[0mEpoch 97-0, Train loss: 1.81626, validation loss: 1.83681, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/27 10:51:18 nl.defaults.trainer]: \u001b[0mEpoch 97-30, Train loss: 1.80386, validation loss: 1.85069, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/27 10:51:23 nl.defaults.trainer]: \u001b[0mEpoch 97-60, Train loss: 1.84442, validation loss: 1.85428, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/27 10:51:28 nl.defaults.trainer]: \u001b[0mEpoch 97-91, Train loss: 1.83849, validation loss: 1.87648, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/27 10:51:33 nl.defaults.trainer]: \u001b[0mEpoch 97-121, Train loss: 1.85088, validation loss: 1.84803, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/27 10:51:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1570,  0.1503, -0.0587, -0.0407], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2872, -0.2009, -0.4048,  0.4940], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:51:35 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy (top1, top5): 62.16000, 94.54571, Validation accuracy: 60.11633, 94.24327\n",
      "\u001b[32m[06/27 10:51:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.156956, +0.150270, -0.058707, -0.040681, 0\n",
      "-0.287225, -0.200906, -0.404808, +0.493964, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:51:38 nl.defaults.trainer]: \u001b[0mEpoch 98-14, Train loss: 1.83753, validation loss: 1.87992, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/27 10:51:43 nl.defaults.trainer]: \u001b[0mEpoch 98-44, Train loss: 1.79887, validation loss: 1.84858, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/27 10:51:48 nl.defaults.trainer]: \u001b[0mEpoch 98-75, Train loss: 1.82625, validation loss: 1.85054, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/27 10:51:53 nl.defaults.trainer]: \u001b[0mEpoch 98-105, Train loss: 1.83723, validation loss: 1.83128, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/27 10:51:58 nl.defaults.trainer]: \u001b[0mEpoch 98-135, Train loss: 1.85990, validation loss: 1.83376, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/27 10:51:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1643,  0.1561, -0.0610, -0.0469], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2866, -0.2031, -0.4085,  0.4971], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:51:59 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy (top1, top5): 62.53714, 94.43143, Validation accuracy: 60.39291, 94.36017\n",
      "\u001b[32m[06/27 10:51:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.164252, +0.156129, -0.061027, -0.046950, 0\n",
      "-0.286570, -0.203149, -0.408523, +0.497054, 3\n",
      "+0.000000, 0\n",
      "\u001b[32m[06/27 10:52:03 nl.defaults.trainer]: \u001b[0mEpoch 99-28, Train loss: 1.80052, validation loss: 1.86617, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/27 10:52:08 nl.defaults.trainer]: \u001b[0mEpoch 99-58, Train loss: 1.79381, validation loss: 1.88038, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/27 10:52:14 nl.defaults.trainer]: \u001b[0mEpoch 99-88, Train loss: 1.81332, validation loss: 1.83958, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/27 10:52:19 nl.defaults.trainer]: \u001b[0mEpoch 99-118, Train loss: 1.88037, validation loss: 1.82860, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/27 10:52:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.1711,  0.1627, -0.0638, -0.0532], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2905, -0.2078, -0.4157,  0.5039], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1.1911e-43], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/27 10:52:22 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy (top1, top5): 62.18857, 94.49143, Validation accuracy: 60.07641, 94.58542\n",
      "\u001b[32m[06/27 10:52:22 nl.defaults.trainer]: \u001b[0mTraining finished\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer, config)\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/27 10:58:44 nl.defaults.trainer]: \u001b[0mStart one-shot evaluation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/27 10:58:50 nl.defaults.trainer]: \u001b[0mEvaluation finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.559999985758466"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate_oneshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NASLib Project",
   "language": "python",
   "name": "naslib_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
