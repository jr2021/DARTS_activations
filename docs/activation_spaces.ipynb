{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.optimizers import DARTSOptimizer\n",
    "from naslib.search_spaces import DartsSearchSpace\n",
    "from naslib.utils import utils, setup_logger, get_config_from_args, set_seed, log_args\n",
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from fvcore.common.config import CfgNode\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mdataset....................................cifar10\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mseed.............................................0\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0msearch_space...........................nasbench201\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mout_dir........................................run\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0moptimizer....................................darts\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0msearchacq_fn_optimization: random_sampling\n",
      "acq_fn_type: its\n",
      "arch_learning_rate: 0.0003\n",
      "arch_weight_decay: 0.001\n",
      "batch_size: 256\n",
      "checkpoint_freq: 1000\n",
      "cutout: False\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 25000\n",
      "debug_predictor: False\n",
      "drop_path_prob: 0.0\n",
      "encoding_type: adjacency_one_hot\n",
      "epochs: 100\n",
      "fidelity: -1\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "k: 10\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.001\n",
      "max_mutations: 1\n",
      "momentum: 0.9\n",
      "num_arches_to_mutate: 2\n",
      "num_candidates: 20\n",
      "num_ensemble: 3\n",
      "num_init: 10\n",
      "output_weights: True\n",
      "population_size: 30\n",
      "predictor_type: var_sparse_gp\n",
      "sample_size: 10\n",
      "seed: 0\n",
      "tau_max: 10\n",
      "tau_min: 0.1\n",
      "train_portion: 0.7\n",
      "unrolled: False\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mevaluationauxiliary_weight: 0.4\n",
      "batch_size: 96\n",
      "checkpoint_freq: 30\n",
      "cutout: True\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 50000\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:8888\n",
      "drop_path_prob: 0.2\n",
      "epochs: 600\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.0\n",
      "momentum: 0.9\n",
      "multiprocessing_distributed: False\n",
      "rank: 0\n",
      "train_portion: 1.0\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "world_size: 1\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0meval_only....................................False\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mresume.......................................False\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mmodel_path....................................None\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mgpu...........................................None\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0msave.........................run/cifar10/bananas/0\n",
      "\u001b[32m[07/03 11:30:13 nl.utils.utils]: \u001b[0mdata../project/dl2022s/robertsj/NASLib/naslib/data\n"
     ]
    }
   ],
   "source": [
    "config = utils.get_config_from_args(config_type='nas')\n",
    "config.optimizer = 'darts'\n",
    "utils.set_seed(config.seed)\n",
    "clear_output(wait=True)\n",
    "utils.log_args(config)\n",
    "\n",
    "logger = setup_logger(config.save + '/log.log')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimum(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.minimum(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "\n",
    "class Maximum(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.maximum(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "class Addition(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.add(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "\n",
    "class Subtraction(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.subtract(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "\n",
    "class Multiplication(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.mul(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stack():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, tensors, edges_data=None):\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "\n",
    "class SimpleSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        stages = ['a_stage_1', 'a_stage_2']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2) # intermediate node\n",
    "        activation_cell.add_node(3) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())]) # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable output edge\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        for i, scope in zip(range(3, 5), stages):\n",
    "            self.add_node(i, subgraph=deepcopy(activation_cell).set_scope(scope).set_input([i-1])) # activation cell i\n",
    "            self.nodes[i]['subgraph'].name = scope\n",
    "        self.add_node(5) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 5)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[4, 5].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "    def _set_ops(self, edge):\n",
    "        edge.data.set('op', [\n",
    "            ops.Sequential(nn.ReLU()),\n",
    "            ops.Sequential(nn.Hardswish()),\n",
    "            ops.Sequential(nn.LeakyReLU()),\n",
    "            ops.Sequential(nn.Identity())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stack():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, tensors, edges_data=None):\n",
    "        return torch.stack(tensors)\n",
    "\n",
    "\n",
    "class ComplexSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'u_stage_1',\n",
    "        'u_stage_2',\n",
    "        'b_stage_1'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        u_stages = ['u_stage_1', 'u_stage_2']\n",
    "        \n",
    "        # unary cell definition\n",
    "        unary_cell = Graph()\n",
    "        unary_cell.name = 'u_cell'\n",
    "        unary_cell.add_node(1) # input node\n",
    "        unary_cell.add_node(2) # intermediate node\n",
    "        unary_cell.add_node(3) # output node\n",
    "        unary_cell.add_edges_from([(1, 2, EdgeData())]) # mutable edge\n",
    "        unary_cell.edges[1, 2].set('cell_name', 'u_cell')\n",
    "        unary_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable edge\n",
    "        \n",
    "        # binary cell definition\n",
    "        binary_cell = Graph()\n",
    "        binary_cell.name = 'b_cell'\n",
    "        binary_cell.add_node(1) # input node\n",
    "        binary_cell.add_node(2) # input node\n",
    "        binary_cell.add_node(3) # concatination node\n",
    "        binary_cell.nodes[3]['comb_op'] = stack()\n",
    "        binary_cell.add_node(4) # intermediate node\n",
    "        binary_cell.add_node(5) # output node\n",
    "        binary_cell.add_edges_from([(3, 4, EdgeData())]) # mutable edge\n",
    "        binary_cell.edges[3, 4].set('cell_name', 'b_cell') \n",
    "        binary_cell.add_edges_from([(1, 3, EdgeData().finalize()),\n",
    "                                    (2, 3, EdgeData().finalize()),\n",
    "                                    (4, 5, EdgeData().finalize())]) # immutable edges\n",
    "        \n",
    "        # activation cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'a_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2, subgraph=deepcopy(unary_cell).set_scope('u_stage_1').set_input([1])) # unary node\n",
    "        activation_cell.nodes[2]['subgraph'].name = 'u_stage_1'\n",
    "        activation_cell.add_node(3, subgraph=deepcopy(unary_cell).set_scope('u_stage_2').set_input([1])) # unary node\n",
    "        activation_cell.nodes[3]['subgraph'].name = 'u_stage_2'\n",
    "        activation_cell.add_node(4, subgraph=deepcopy(binary_cell).set_scope('b_stage_1').set_input([2, 3])) # binary node\n",
    "        activation_cell.nodes[4]['subgraph'].name = 'b_stage_1'\n",
    "        activation_cell.add_node(5) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData().finalize()), \n",
    "                                        (1, 3, EdgeData().finalize()),\n",
    "                                        (2, 4, EdgeData().finalize()),\n",
    "                                        (3, 4, EdgeData().finalize()), \n",
    "                                        (4, 5, EdgeData().finalize())])\n",
    "        \n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3, subgraph=deepcopy(activation_cell).set_input([2])) # activation cell\n",
    "        self.nodes[3]['subgraph'].name = 'a_stage_1'\n",
    "        self.add_node(4) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 4)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[3, 4].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in u_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_unary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            ) # set unary cell ops\n",
    "        \n",
    "        self.update_edges(\n",
    "            update_func=lambda edge: self._set_binary_ops(edge),\n",
    "            scope='b_stage_1',\n",
    "            private_edge_data=True\n",
    "        ) # set binary cell ops\n",
    "        \n",
    "\n",
    "    def _set_unary_ops(self, edge):\n",
    "        edge.data.set('op', [ops.Identity(), ops.Zero(stride=1)]) \n",
    "        \n",
    "        \n",
    "    def _set_binary_ops(self, edge):\n",
    "        edge.data.set('op', [Minimum(), Maximum()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'u_stage_1',\n",
    "        'u_stage_2',\n",
    "        'u_stage_3',\n",
    "        'u_stage_4',\n",
    "        'b_stage_1',\n",
    "        'b_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        u_stages = ['u_stage_1', 'u_stage_2', 'u_stage_3', 'u_stage_4']\n",
    "        b_stages = ['b_stage_1', 'b_stage_2']\n",
    "        \n",
    "        # unary cell definition\n",
    "        unary_cell = Graph()\n",
    "        unary_cell.name = 'u_cell'\n",
    "        unary_cell.add_node(1) # input node\n",
    "        unary_cell.add_node(2) # intermediate node\n",
    "        unary_cell.add_node(3) # output node\n",
    "        unary_cell.add_edges_from([(1, 2, EdgeData())]) # mutable edge\n",
    "        unary_cell.edges[1, 2].set('cell_name', 'u_cell')\n",
    "        unary_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable edge\n",
    "        \n",
    "        # binary cell definition\n",
    "        binary_cell = Graph()\n",
    "        binary_cell.name = 'b_cell'\n",
    "        binary_cell.add_node(1) # input node\n",
    "        binary_cell.add_node(2) # input node\n",
    "        binary_cell.add_node(3) # concatination node\n",
    "        binary_cell.nodes[3]['comb_op'] = stack()\n",
    "        binary_cell.add_node(4) # intermediate node\n",
    "        binary_cell.add_node(5) # output node\n",
    "        binary_cell.add_edges_from([(3, 4, EdgeData())]) # mutable edge\n",
    "        binary_cell.edges[3, 4].set('cell_name', 'b_cell') \n",
    "        binary_cell.add_edges_from([(1, 3, EdgeData().finalize()),\n",
    "                                    (2, 3, EdgeData().finalize()),\n",
    "                                    (4, 5, EdgeData().finalize())]) # immutable edges\n",
    "        \n",
    "        # activation cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'a_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2, subgraph=deepcopy(unary_cell).set_scope('u_stage_1').set_input([1])) # unary cell 1\n",
    "        activation_cell.nodes[2]['subgraph'].name = 'u_stage_1'\n",
    "        activation_cell.add_node(3, subgraph=deepcopy(unary_cell).set_scope('u_stage_2').set_input([1])) # unary cell 2\n",
    "        activation_cell.nodes[3]['subgraph'].name = 'u_stage_2'\n",
    "        activation_cell.add_node(4, subgraph=deepcopy(unary_cell).set_scope('u_stage_3').set_input([1])) # unary cell 3\n",
    "        activation_cell.nodes[4]['subgraph'].name = 'u_stage_3'\n",
    "        activation_cell.add_node(5, subgraph=deepcopy(binary_cell).set_scope('b_stage_1').set_input([2, 3])) # binary cell 1\n",
    "        activation_cell.nodes[5]['subgraph'].name = 'b_stage_1'\n",
    "        activation_cell.add_node(6, subgraph=deepcopy(unary_cell).set_scope('u_stage_4').set_input([5])) # unary cell 4\n",
    "        activation_cell.nodes[6]['subgraph'].name = 'u_stage_4'\n",
    "        activation_cell.add_node(7, subgraph=deepcopy(binary_cell).set_scope('b_stage_2').set_input([4, 6])) # binary cell 2\n",
    "        activation_cell.nodes[7]['subgraph'].name = 'b_stage_2'\n",
    "        activation_cell.add_node(8) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData().finalize()), \n",
    "                                        (1, 3, EdgeData().finalize()),\n",
    "                                        (1, 4, EdgeData().finalize()),\n",
    "                                        (2, 5, EdgeData().finalize()),\n",
    "                                        (3, 5, EdgeData().finalize()), \n",
    "                                        (4, 7, EdgeData().finalize()),\n",
    "                                        (5, 6, EdgeData().finalize()),\n",
    "                                        (6, 7, EdgeData().finalize()),\n",
    "                                        (7, 8, EdgeData().finalize())])\n",
    "        \n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3, subgraph=deepcopy(activation_cell).set_input([2])) # activation cell\n",
    "        self.nodes[3]['subgraph'].name = 'a_stage_1'\n",
    "        self.add_node(4) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 4)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[3, 4].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in u_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_unary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            ) # set unary cell ops\n",
    "        \n",
    "        for scope in b_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_binary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True\n",
    "            ) # set binary cell ops\n",
    "        \n",
    "\n",
    "    def _set_unary_ops(self, edge):\n",
    "        edge.data.set('op', [\n",
    "            ops.Identity(), ops.Zero(stride=1)]) \n",
    "        \n",
    "        \n",
    "    def _set_binary_ops(self, edge):\n",
    "        edge.data.set('op', [\n",
    "            Minimum(), \n",
    "            Maximum()\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = RNNSearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:45:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mParsed graph:\n",
      "Graph a_stage_1:\n",
      " Graph(\n",
      "  (a_stage_1-edge(1,2)): Identity()\n",
      "  (a_stage_1-edge(1,3)): Identity()\n",
      "  (a_stage_1-edge(1,4)): Identity()\n",
      "  (a_stage_1-subgraph_at(2)): Graph u_stage_1-0.4099054, scope u_stage_1, 3 nodes\n",
      "  (a_stage_1-edge(2,5)): Identity()\n",
      "  (a_stage_1-subgraph_at(3)): Graph u_stage_2-0.4099054, scope u_stage_2, 3 nodes\n",
      "  (a_stage_1-edge(3,5)): Identity()\n",
      "  (a_stage_1-subgraph_at(4)): Graph u_stage_3-0.4099054, scope u_stage_3, 3 nodes\n",
      "  (a_stage_1-edge(4,7)): Identity()\n",
      "  (a_stage_1-subgraph_at(5)): Graph b_stage_1-0.5691127, scope b_stage_1, 5 nodes\n",
      "  (a_stage_1-edge(5,6)): Identity()\n",
      "  (a_stage_1-subgraph_at(6)): Graph u_stage_4-0.4099054, scope u_stage_4, 3 nodes\n",
      "  (a_stage_1-edge(6,7)): Identity()\n",
      "  (a_stage_1-subgraph_at(7)): Graph b_stage_2-0.5691127, scope b_stage_2, 5 nodes\n",
      "  (a_stage_1-edge(7,8)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph b_stage_1:\n",
      " Graph(\n",
      "  (b_stage_1-edge(1,3)): Identity()\n",
      "  (b_stage_1-edge(2,3)): Identity()\n",
      "  (b_stage_1-edge(3,4)): MixedOp(\n",
      "    (primitive-0): Minimum()\n",
      "    (primitive-1): Maximum()\n",
      "  )\n",
      "  (b_stage_1-edge(4,5)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph b_stage_2:\n",
      " Graph(\n",
      "  (b_stage_2-edge(1,3)): Identity()\n",
      "  (b_stage_2-edge(2,3)): Identity()\n",
      "  (b_stage_2-edge(3,4)): MixedOp(\n",
      "    (primitive-0): Minimum()\n",
      "    (primitive-1): Maximum()\n",
      "  )\n",
      "  (b_stage_2-edge(4,5)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_1:\n",
      " Graph(\n",
      "  (u_stage_1-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_1-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_2:\n",
      " Graph(\n",
      "  (u_stage_2-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_2-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_3:\n",
      " Graph(\n",
      "  (u_stage_3-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_3-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_4:\n",
      " Graph(\n",
      "  (u_stage_4-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_4-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph makrograph:\n",
      " RNNSearchSpace(\n",
      "  (makrograph-edge(1,2)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(2,3)): Identity()\n",
      "  (makrograph-subgraph_at(3)): Graph a_stage_1-0.5086001, scope None, 8 nodes\n",
      "  (makrograph-edge(3,4)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=10, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = DARTSOptimizer(config)\n",
    "optimizer.adapt_search_space(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:45:42 nl.defaults.trainer]: \u001b[0mparam size = 0.006882MB\n",
      "\u001b[32m[07/03 11:45:42 nl.defaults.trainer]: \u001b[0mStart training\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[07/03 11:45:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000257, -0.000239, 1\n",
      "+0.000919, +0.000448, 0\n",
      "-0.001975, +0.001175, 1\n",
      "+0.000275, -0.000229, 0\n",
      "+0.000869, +0.001941, 1\n",
      "+0.000945, +0.001190, 1\n",
      "\u001b[32m[07/03 11:45:43 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.30342, validation loss: 2.30370, learning rate: [0.025]\n",
      "\u001b[32m[07/03 11:45:48 nl.defaults.trainer]: \u001b[0mEpoch 0-29, Train loss: 2.30125, validation loss: 2.30104, learning rate: [0.025]\n",
      "\u001b[32m[07/03 11:45:53 nl.defaults.trainer]: \u001b[0mEpoch 0-58, Train loss: 2.28639, validation loss: 2.29286, learning rate: [0.025]\n",
      "\u001b[32m[07/03 11:45:58 nl.defaults.trainer]: \u001b[0mEpoch 0-87, Train loss: 2.26142, validation loss: 2.24306, learning rate: [0.025]\n",
      "\u001b[32m[07/03 11:46:03 nl.defaults.trainer]: \u001b[0mEpoch 0-116, Train loss: 2.24192, validation loss: 2.23459, learning rate: [0.025]\n",
      "\u001b[32m[07/03 11:46:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0016,  0.0016], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0166,  0.0178], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0533, -0.0539], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0552, -0.0552], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0558, -0.0531], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0561, -0.0541], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:46:07 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 18.08571, 64.66286, Validation accuracy: 17.69218, 64.50160\n",
      "\u001b[32m[07/03 11:46:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001615, +0.001615, 1\n",
      "-0.016563, +0.017752, 1\n",
      "+0.053255, -0.053943, 0\n",
      "+0.055189, -0.055150, 0\n",
      "+0.055779, -0.053073, 0\n",
      "+0.056074, -0.054096, 0\n",
      "\u001b[32m[07/03 11:46:08 nl.defaults.trainer]: \u001b[0mEpoch 1-7, Train loss: 2.16500, validation loss: 2.20083, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[07/03 11:46:13 nl.defaults.trainer]: \u001b[0mEpoch 1-36, Train loss: 2.17288, validation loss: 2.18820, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[07/03 11:46:18 nl.defaults.trainer]: \u001b[0mEpoch 1-65, Train loss: 2.18262, validation loss: 2.19443, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[07/03 11:46:24 nl.defaults.trainer]: \u001b[0mEpoch 1-94, Train loss: 2.18236, validation loss: 2.16448, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[07/03 11:46:29 nl.defaults.trainer]: \u001b[0mEpoch 1-123, Train loss: 2.19507, validation loss: 2.15031, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[07/03 11:46:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0014, -0.0014], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0318,  0.0329], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0882, -0.0888], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0901, -0.0901], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0916, -0.0890], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0917, -0.0897], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:46:31 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 27.84571, 73.42571, Validation accuracy: 27.87409, 73.25502\n",
      "\u001b[32m[07/03 11:46:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.001385, -0.001385, 0\n",
      "-0.031764, +0.032933, 1\n",
      "+0.088158, -0.088835, 0\n",
      "+0.090091, -0.090053, 0\n",
      "+0.091649, -0.088954, 0\n",
      "+0.091707, -0.089742, 0\n",
      "\u001b[32m[07/03 11:46:34 nl.defaults.trainer]: \u001b[0mEpoch 2-15, Train loss: 2.13886, validation loss: 2.17956, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[07/03 11:46:39 nl.defaults.trainer]: \u001b[0mEpoch 2-44, Train loss: 2.12845, validation loss: 2.13227, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[07/03 11:46:44 nl.defaults.trainer]: \u001b[0mEpoch 2-73, Train loss: 2.11509, validation loss: 2.14020, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[07/03 11:46:49 nl.defaults.trainer]: \u001b[0mEpoch 2-102, Train loss: 2.12206, validation loss: 2.11027, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[07/03 11:46:54 nl.defaults.trainer]: \u001b[0mEpoch 2-131, Train loss: 2.12716, validation loss: 2.14403, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[07/03 11:46:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0028,  0.0028], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0375,  0.0387], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1118, -0.1125], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1138, -0.1138], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1170, -0.1143], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1167, -0.1147], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:46:55 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 32.55714, 76.88286, Validation accuracy: 31.41252, 75.87819\n",
      "\u001b[32m[07/03 11:46:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002789, +0.002789, 1\n",
      "-0.037531, +0.038688, 1\n",
      "+0.111846, -0.112509, 0\n",
      "+0.113792, -0.113755, 0\n",
      "+0.116985, -0.114303, 0\n",
      "+0.116651, -0.114707, 0\n",
      "\u001b[32m[07/03 11:46:59 nl.defaults.trainer]: \u001b[0mEpoch 3-23, Train loss: 2.11545, validation loss: 2.13808, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[07/03 11:47:04 nl.defaults.trainer]: \u001b[0mEpoch 3-52, Train loss: 2.10231, validation loss: 2.10742, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[07/03 11:47:09 nl.defaults.trainer]: \u001b[0mEpoch 3-81, Train loss: 2.10775, validation loss: 2.07456, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[07/03 11:47:14 nl.defaults.trainer]: \u001b[0mEpoch 3-111, Train loss: 2.12293, validation loss: 2.11797, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[07/03 11:47:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0012,  0.0012], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0421,  0.0433], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1387, -0.1394], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1407, -0.1406], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1457, -0.1430], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1451, -0.1431], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:47:19 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 34.88000, 79.16571, Validation accuracy: 34.78844, 78.62968\n",
      "\u001b[32m[07/03 11:47:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001201, +0.001201, 1\n",
      "-0.042136, +0.043282, 1\n",
      "+0.138740, -0.139390, 0\n",
      "+0.140669, -0.140633, 0\n",
      "+0.145660, -0.142990, 0\n",
      "+0.145067, -0.143140, 0\n",
      "\u001b[32m[07/03 11:47:19 nl.defaults.trainer]: \u001b[0mEpoch 4-3, Train loss: 2.12086, validation loss: 2.10063, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[07/03 11:47:25 nl.defaults.trainer]: \u001b[0mEpoch 4-33, Train loss: 2.09761, validation loss: 2.12389, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[07/03 11:47:30 nl.defaults.trainer]: \u001b[0mEpoch 4-62, Train loss: 2.09066, validation loss: 2.12389, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[07/03 11:47:35 nl.defaults.trainer]: \u001b[0mEpoch 4-91, Train loss: 2.13315, validation loss: 2.08690, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[07/03 11:47:40 nl.defaults.trainer]: \u001b[0mEpoch 4-120, Train loss: 2.06267, validation loss: 2.08538, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[07/03 11:47:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0012,  0.0012], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0395,  0.0406], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1607, -0.1614], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1626, -0.1626], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1688, -0.1662], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1690, -0.1671], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:47:43 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 36.60286, 80.89143, Validation accuracy: 36.30246, 80.36896\n",
      "\u001b[32m[07/03 11:47:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001209, +0.001209, 1\n",
      "-0.039513, +0.040649, 1\n",
      "+0.160732, -0.161366, 0\n",
      "+0.162611, -0.162575, 0\n",
      "+0.168845, -0.166191, 0\n",
      "+0.169028, -0.167122, 0\n",
      "\u001b[32m[07/03 11:47:45 nl.defaults.trainer]: \u001b[0mEpoch 5-11, Train loss: 2.14681, validation loss: 2.09733, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[07/03 11:47:50 nl.defaults.trainer]: \u001b[0mEpoch 5-40, Train loss: 2.08374, validation loss: 2.10976, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[07/03 11:47:55 nl.defaults.trainer]: \u001b[0mEpoch 5-69, Train loss: 2.07845, validation loss: 2.10258, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[07/03 11:48:00 nl.defaults.trainer]: \u001b[0mEpoch 5-98, Train loss: 2.09986, validation loss: 2.07366, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[07/03 11:48:05 nl.defaults.trainer]: \u001b[0mEpoch 5-127, Train loss: 2.12846, validation loss: 2.07621, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[07/03 11:48:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-4.0442e-05,  4.0464e-05], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0406,  0.0417], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1713, -0.1719], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1731, -0.1731], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1828, -0.1802], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1823, -0.1804], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:48:07 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 37.39714, 81.58286, Validation accuracy: 36.84706, 81.43818\n",
      "\u001b[32m[07/03 11:48:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000040, +0.000040, 1\n",
      "-0.040589, +0.041715, 1\n",
      "+0.171313, -0.171926, 0\n",
      "+0.173141, -0.173106, 0\n",
      "+0.182831, -0.180200, 0\n",
      "+0.182325, -0.180448, 0\n",
      "\u001b[32m[07/03 11:48:10 nl.defaults.trainer]: \u001b[0mEpoch 6-18, Train loss: 2.10638, validation loss: 2.08793, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[07/03 11:48:15 nl.defaults.trainer]: \u001b[0mEpoch 6-47, Train loss: 2.09853, validation loss: 2.05260, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[07/03 11:48:20 nl.defaults.trainer]: \u001b[0mEpoch 6-76, Train loss: 2.05979, validation loss: 2.07757, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[07/03 11:48:25 nl.defaults.trainer]: \u001b[0mEpoch 6-105, Train loss: 2.06995, validation loss: 2.09719, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[07/03 11:48:30 nl.defaults.trainer]: \u001b[0mEpoch 6-134, Train loss: 2.03028, validation loss: 2.04783, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[07/03 11:48:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0011,  0.0011], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0625,  0.0636], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1790, -0.1796], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1811, -0.1811], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2009, -0.1983], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1933, -0.1914], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:48:31 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 38.37714, 83.21714, Validation accuracy: 37.80509, 82.48745\n",
      "\u001b[32m[07/03 11:48:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001116, +0.001116, 1\n",
      "-0.062458, +0.063575, 1\n",
      "+0.178982, -0.179572, 0\n",
      "+0.181104, -0.181071, 0\n",
      "+0.200878, -0.198268, 0\n",
      "+0.193272, -0.191429, 0\n",
      "\u001b[32m[07/03 11:48:36 nl.defaults.trainer]: \u001b[0mEpoch 7-26, Train loss: 2.08551, validation loss: 2.08519, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[07/03 11:48:41 nl.defaults.trainer]: \u001b[0mEpoch 7-55, Train loss: 2.10001, validation loss: 2.05517, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[07/03 11:48:46 nl.defaults.trainer]: \u001b[0mEpoch 7-84, Train loss: 2.01947, validation loss: 2.08825, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[07/03 11:48:51 nl.defaults.trainer]: \u001b[0mEpoch 7-113, Train loss: 2.05779, validation loss: 2.07138, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[07/03 11:48:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0024,  0.0024], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0791,  0.0802], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1873, -0.1879], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1896, -0.1896], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2205, -0.2179], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2051, -0.2033], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:48:55 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 39.29429, 83.72000, Validation accuracy: 38.66332, 83.30007\n",
      "\u001b[32m[07/03 11:48:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002423, +0.002423, 1\n",
      "-0.079121, +0.080230, 1\n",
      "+0.187327, -0.187893, 0\n",
      "+0.189634, -0.189602, 0\n",
      "+0.220464, -0.217876, 0\n",
      "+0.205066, -0.203256, 0\n",
      "\u001b[32m[07/03 11:48:56 nl.defaults.trainer]: \u001b[0mEpoch 8-5, Train loss: 2.08177, validation loss: 2.05748, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[07/03 11:49:01 nl.defaults.trainer]: \u001b[0mEpoch 8-34, Train loss: 2.06435, validation loss: 2.05525, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[07/03 11:49:06 nl.defaults.trainer]: \u001b[0mEpoch 8-63, Train loss: 2.05021, validation loss: 2.03814, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[07/03 11:49:11 nl.defaults.trainer]: \u001b[0mEpoch 8-92, Train loss: 2.06056, validation loss: 2.03961, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[07/03 11:49:16 nl.defaults.trainer]: \u001b[0mEpoch 8-121, Train loss: 2.01278, validation loss: 2.08897, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[07/03 11:49:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0034,  0.0034], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0950,  0.0961], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1908, -0.1913], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1933, -0.1933], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2378, -0.2353], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2124, -0.2107], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:49:19 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 40.63143, 84.64000, Validation accuracy: 39.49304, 84.13834\n",
      "\u001b[32m[07/03 11:49:19 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.003361, +0.003361, 1\n",
      "-0.095021, +0.096123, 1\n",
      "+0.190797, -0.191337, 0\n",
      "+0.193326, -0.193296, 0\n",
      "+0.237827, -0.235262, 0\n",
      "+0.212437, -0.210666, 0\n",
      "\u001b[32m[07/03 11:49:21 nl.defaults.trainer]: \u001b[0mEpoch 9-13, Train loss: 2.02085, validation loss: 2.06194, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[07/03 11:49:26 nl.defaults.trainer]: \u001b[0mEpoch 9-42, Train loss: 2.02664, validation loss: 2.01424, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[07/03 11:49:31 nl.defaults.trainer]: \u001b[0mEpoch 9-71, Train loss: 2.03467, validation loss: 2.05972, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[07/03 11:49:36 nl.defaults.trainer]: \u001b[0mEpoch 9-100, Train loss: 2.02343, validation loss: 2.08325, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[07/03 11:49:41 nl.defaults.trainer]: \u001b[0mEpoch 9-129, Train loss: 2.04760, validation loss: 2.04633, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[07/03 11:49:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0021,  0.0021], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1014,  0.1025], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2082, -0.2087], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2107, -0.2107], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2622, -0.2597], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2331, -0.2314], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:49:43 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 41.44857, 85.43429, Validation accuracy: 41.22092, 85.20757\n",
      "\u001b[32m[07/03 11:49:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002063, +0.002063, 1\n",
      "-0.101387, +0.102481, 1\n",
      "+0.208230, -0.208750, 0\n",
      "+0.210682, -0.210653, 0\n",
      "+0.262222, -0.259678, 0\n",
      "+0.233113, -0.231373, 0\n",
      "\u001b[32m[07/03 11:49:46 nl.defaults.trainer]: \u001b[0mEpoch 10-21, Train loss: 2.04142, validation loss: 2.04411, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[07/03 11:49:51 nl.defaults.trainer]: \u001b[0mEpoch 10-50, Train loss: 2.05918, validation loss: 2.06415, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[07/03 11:49:56 nl.defaults.trainer]: \u001b[0mEpoch 10-79, Train loss: 2.05028, validation loss: 2.04868, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[07/03 11:50:02 nl.defaults.trainer]: \u001b[0mEpoch 10-109, Train loss: 2.03026, validation loss: 2.00814, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[07/03 11:50:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0042,  0.0042], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1230,  0.1240], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2071, -0.2076], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2101, -0.2101], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2842, -0.2817], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2365, -0.2348], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:50:06 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 41.69143, 86.26286, Validation accuracy: 41.57448, 86.27680\n",
      "\u001b[32m[07/03 11:50:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.004153, +0.004153, 1\n",
      "-0.122957, +0.124045, 1\n",
      "+0.207077, -0.207570, 0\n",
      "+0.210084, -0.210057, 0\n",
      "+0.284183, -0.281661, 0\n",
      "+0.236514, -0.234815, 0\n",
      "\u001b[32m[07/03 11:50:07 nl.defaults.trainer]: \u001b[0mEpoch 11-1, Train loss: 2.04479, validation loss: 2.00912, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[07/03 11:50:12 nl.defaults.trainer]: \u001b[0mEpoch 11-30, Train loss: 2.07416, validation loss: 2.04216, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[07/03 11:50:17 nl.defaults.trainer]: \u001b[0mEpoch 11-59, Train loss: 2.01507, validation loss: 2.04675, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[07/03 11:50:22 nl.defaults.trainer]: \u001b[0mEpoch 11-88, Train loss: 2.03135, validation loss: 2.02850, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[07/03 11:50:27 nl.defaults.trainer]: \u001b[0mEpoch 11-117, Train loss: 2.04324, validation loss: 2.01083, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[07/03 11:50:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0056,  0.0056], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1422,  0.1432], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2129, -0.2133], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2164, -0.2164], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3132, -0.3107], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2464, -0.2447], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:50:30 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 43.08000, 86.94000, Validation accuracy: 43.13698, 86.90693\n",
      "\u001b[32m[07/03 11:50:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.005565, +0.005565, 1\n",
      "-0.142165, +0.143246, 1\n",
      "+0.212878, -0.213349, 0\n",
      "+0.216418, -0.216391, 0\n",
      "+0.313171, -0.310667, 0\n",
      "+0.246399, -0.244737, 0\n",
      "\u001b[32m[07/03 11:50:32 nl.defaults.trainer]: \u001b[0mEpoch 12-9, Train loss: 1.99106, validation loss: 2.03779, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[07/03 11:50:37 nl.defaults.trainer]: \u001b[0mEpoch 12-38, Train loss: 1.99540, validation loss: 1.97403, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[07/03 11:50:42 nl.defaults.trainer]: \u001b[0mEpoch 12-67, Train loss: 2.02901, validation loss: 1.97865, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[07/03 11:50:47 nl.defaults.trainer]: \u001b[0mEpoch 12-96, Train loss: 1.96572, validation loss: 2.01044, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[07/03 11:50:52 nl.defaults.trainer]: \u001b[0mEpoch 12-125, Train loss: 2.00492, validation loss: 2.03941, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[07/03 11:50:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0037,  0.0037], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1502,  0.1513], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2243, -0.2247], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2279, -0.2278], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3374, -0.3349], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2614, -0.2597], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:50:54 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 44.39714, 87.80857, Validation accuracy: 43.48198, 87.39735\n",
      "\u001b[32m[07/03 11:50:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.003737, +0.003737, 1\n",
      "-0.150179, +0.151254, 1\n",
      "+0.224284, -0.224735, 0\n",
      "+0.227862, -0.227837, 0\n",
      "+0.337352, -0.334869, 0\n",
      "+0.261371, -0.259741, 0\n",
      "\u001b[32m[07/03 11:50:57 nl.defaults.trainer]: \u001b[0mEpoch 13-17, Train loss: 1.99487, validation loss: 1.97288, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[07/03 11:51:02 nl.defaults.trainer]: \u001b[0mEpoch 13-46, Train loss: 1.99738, validation loss: 2.04566, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[07/03 11:51:08 nl.defaults.trainer]: \u001b[0mEpoch 13-75, Train loss: 2.00391, validation loss: 2.04898, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[07/03 11:51:13 nl.defaults.trainer]: \u001b[0mEpoch 13-104, Train loss: 2.02101, validation loss: 2.03812, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[07/03 11:51:18 nl.defaults.trainer]: \u001b[0mEpoch 13-134, Train loss: 1.99477, validation loss: 1.99222, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[07/03 11:51:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0017, -0.0017], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1543,  0.1554], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2317, -0.2321], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2353, -0.2353], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3544, -0.3519], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2724, -0.2708], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:51:18 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 44.79714, 88.09429, Validation accuracy: 43.79562, 87.74236\n",
      "\u001b[32m[07/03 11:51:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.001704, -0.001704, 0\n",
      "-0.154331, +0.155400, 1\n",
      "+0.231654, -0.232087, 0\n",
      "+0.235278, -0.235254, 0\n",
      "+0.354405, -0.351946, 0\n",
      "+0.272354, -0.270757, 0\n",
      "\u001b[32m[07/03 11:51:23 nl.defaults.trainer]: \u001b[0mEpoch 14-26, Train loss: 2.00829, validation loss: 2.02055, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[07/03 11:51:28 nl.defaults.trainer]: \u001b[0mEpoch 14-55, Train loss: 1.98918, validation loss: 2.05508, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[07/03 11:51:33 nl.defaults.trainer]: \u001b[0mEpoch 14-84, Train loss: 2.00828, validation loss: 1.99903, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[07/03 11:51:38 nl.defaults.trainer]: \u001b[0mEpoch 14-114, Train loss: 1.99998, validation loss: 1.98789, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[07/03 11:51:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0083,  0.0083], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1687,  0.1697], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2223, -0.2227], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2260, -0.2260], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3705, -0.3681], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2670, -0.2654], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:51:42 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 44.24000, 87.88571, Validation accuracy: 43.99806, 87.61975\n",
      "\u001b[32m[07/03 11:51:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.008301, +0.008301, 1\n",
      "-0.168675, +0.169739, 1\n",
      "+0.222315, -0.222729, 0\n",
      "+0.226030, -0.226007, 0\n",
      "+0.370534, -0.368101, 0\n",
      "+0.266981, -0.265419, 0\n",
      "\u001b[32m[07/03 11:51:43 nl.defaults.trainer]: \u001b[0mEpoch 15-6, Train loss: 2.05078, validation loss: 1.95587, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[07/03 11:51:48 nl.defaults.trainer]: \u001b[0mEpoch 15-36, Train loss: 1.98981, validation loss: 2.04263, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[07/03 11:51:53 nl.defaults.trainer]: \u001b[0mEpoch 15-66, Train loss: 2.00929, validation loss: 2.00930, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[07/03 11:51:58 nl.defaults.trainer]: \u001b[0mEpoch 15-95, Train loss: 2.03443, validation loss: 2.02829, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[07/03 11:52:04 nl.defaults.trainer]: \u001b[0mEpoch 15-125, Train loss: 1.98364, validation loss: 2.00809, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[07/03 11:52:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0086,  0.0086], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1848,  0.1859], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2130, -0.2134], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2171, -0.2171], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3908, -0.3883], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2612, -0.2597], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:52:06 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 46.09714, 88.90286, Validation accuracy: 45.77441, 88.53786\n",
      "\u001b[32m[07/03 11:52:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.008577, +0.008577, 1\n",
      "-0.184843, +0.185902, 1\n",
      "+0.212980, -0.213378, 0\n",
      "+0.217127, -0.217105, 0\n",
      "+0.390752, -0.388343, 0\n",
      "+0.261246, -0.259716, 0\n",
      "\u001b[32m[07/03 11:52:09 nl.defaults.trainer]: \u001b[0mEpoch 16-17, Train loss: 1.98682, validation loss: 2.01470, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[07/03 11:52:14 nl.defaults.trainer]: \u001b[0mEpoch 16-46, Train loss: 1.94849, validation loss: 2.01202, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[07/03 11:52:19 nl.defaults.trainer]: \u001b[0mEpoch 16-75, Train loss: 1.95725, validation loss: 1.98817, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[07/03 11:52:24 nl.defaults.trainer]: \u001b[0mEpoch 16-104, Train loss: 2.01019, validation loss: 2.01135, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[07/03 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 16-133, Train loss: 1.90688, validation loss: 1.90416, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[07/03 11:52:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0043,  0.0043], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1945,  0.1955], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2112, -0.2116], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2155, -0.2155], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4093, -0.4070], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2624, -0.2609], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:52:30 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 46.38000, 89.02857, Validation accuracy: 45.68031, 88.68043\n",
      "\u001b[32m[07/03 11:52:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.004317, +0.004317, 1\n",
      "-0.194473, +0.195527, 1\n",
      "+0.211203, -0.211586, 0\n",
      "+0.215532, -0.215511, 0\n",
      "+0.409345, -0.406962, 0\n",
      "+0.262363, -0.260862, 0\n",
      "\u001b[32m[07/03 11:52:34 nl.defaults.trainer]: \u001b[0mEpoch 17-25, Train loss: 1.98162, validation loss: 1.98661, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[07/03 11:52:39 nl.defaults.trainer]: \u001b[0mEpoch 17-55, Train loss: 2.02685, validation loss: 1.99728, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[07/03 11:52:45 nl.defaults.trainer]: \u001b[0mEpoch 17-85, Train loss: 1.98399, validation loss: 1.99465, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[07/03 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 17-115, Train loss: 1.92121, validation loss: 2.03806, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[07/03 11:52:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0030,  0.0030], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2056,  0.2067], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2056, -0.2059], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2101, -0.2100], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4266, -0.4243], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2594, -0.2579], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:52:53 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 47.15714, 89.03714, Validation accuracy: 46.54425, 88.85721\n",
      "\u001b[32m[07/03 11:52:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002987, +0.002987, 1\n",
      "-0.205630, +0.206680, 1\n",
      "+0.205577, -0.205947, 0\n",
      "+0.210064, -0.210044, 0\n",
      "+0.426646, -0.424287, 0\n",
      "+0.259383, -0.257909, 0\n",
      "\u001b[32m[07/03 11:52:55 nl.defaults.trainer]: \u001b[0mEpoch 18-7, Train loss: 1.99512, validation loss: 2.01873, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[07/03 11:53:00 nl.defaults.trainer]: \u001b[0mEpoch 18-36, Train loss: 1.94887, validation loss: 2.00788, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[07/03 11:53:05 nl.defaults.trainer]: \u001b[0mEpoch 18-65, Train loss: 2.04392, validation loss: 2.00204, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[07/03 11:53:10 nl.defaults.trainer]: \u001b[0mEpoch 18-94, Train loss: 1.97948, validation loss: 2.03240, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[07/03 11:53:15 nl.defaults.trainer]: \u001b[0mEpoch 18-123, Train loss: 1.98624, validation loss: 2.00473, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[07/03 11:53:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0068,  0.0068], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2154,  0.2164], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1978, -0.1981], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2021, -0.2021], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4392, -0.4369], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2539, -0.2525], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:53:17 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 47.31714, 89.34857, Validation accuracy: 46.21350, 88.88287\n",
      "\u001b[32m[07/03 11:53:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.006758, +0.006758, 1\n",
      "-0.215389, +0.216434, 1\n",
      "+0.197785, -0.198142, 0\n",
      "+0.202129, -0.202110, 0\n",
      "+0.439240, -0.436909, 0\n",
      "+0.253920, -0.252473, 0\n",
      "\u001b[32m[07/03 11:53:20 nl.defaults.trainer]: \u001b[0mEpoch 19-15, Train loss: 1.94457, validation loss: 1.98357, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[07/03 11:53:25 nl.defaults.trainer]: \u001b[0mEpoch 19-44, Train loss: 1.99190, validation loss: 1.97089, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[07/03 11:53:30 nl.defaults.trainer]: \u001b[0mEpoch 19-73, Train loss: 1.95931, validation loss: 1.97929, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[07/03 11:53:35 nl.defaults.trainer]: \u001b[0mEpoch 19-102, Train loss: 1.99622, validation loss: 1.94762, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[07/03 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 19-131, Train loss: 1.97644, validation loss: 1.92179, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[07/03 11:53:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0088,  0.0088], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2318,  0.2329], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1875, -0.1879], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1922, -0.1922], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4621, -0.4598], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2459, -0.2445], device='cuda:0', requires_grad=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/03 11:53:41 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 48.27714, 89.97143, Validation accuracy: 47.82448, 89.50730\n",
      "\u001b[32m[07/03 11:53:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.008811, +0.008811, 1\n",
      "-0.231842, +0.232884, 1\n",
      "+0.187530, -0.187876, 0\n",
      "+0.192206, -0.192187, 0\n",
      "+0.462062, -0.459755, 0\n",
      "+0.245949, -0.244526, 0\n",
      "\u001b[32m[07/03 11:53:45 nl.defaults.trainer]: \u001b[0mEpoch 20-23, Train loss: 1.96353, validation loss: 2.00127, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[07/03 11:53:50 nl.defaults.trainer]: \u001b[0mEpoch 20-52, Train loss: 1.97124, validation loss: 1.97209, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[07/03 11:53:55 nl.defaults.trainer]: \u001b[0mEpoch 20-81, Train loss: 1.98043, validation loss: 1.94313, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[07/03 11:54:00 nl.defaults.trainer]: \u001b[0mEpoch 20-110, Train loss: 1.98552, validation loss: 1.94863, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[07/03 11:54:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0082,  0.0082], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2496,  0.2507], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1701, -0.1705], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1752, -0.1752], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4789, -0.4766], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2306, -0.2292], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[07/03 11:54:05 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 47.68857, 89.73429, Validation accuracy: 46.93488, 89.40750\n",
      "\u001b[32m[07/03 11:54:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.008212, +0.008211, 1\n",
      "-0.249637, +0.250675, 1\n",
      "+0.170115, -0.170451, 0\n",
      "+0.175191, -0.175172, 0\n",
      "+0.478901, -0.476619, 0\n",
      "+0.230551, -0.229151, 0\n",
      "\u001b[32m[07/03 11:54:05 nl.defaults.trainer]: \u001b[0mEpoch 21-2, Train loss: 1.97444, validation loss: 1.98930, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[07/03 11:54:11 nl.defaults.trainer]: \u001b[0mEpoch 21-31, Train loss: 1.99510, validation loss: 1.99492, learning rate: [0.02248186014850829]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer, config)\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 18:09:42 nl.defaults.trainer]: \u001b[0mStart one-shot evaluation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/29 18:09:48 nl.defaults.trainer]: \u001b[0mEvaluation finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.85999998982747"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate_oneshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NASLib Project",
   "language": "python",
   "name": "naslib_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
