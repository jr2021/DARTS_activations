{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "device: cpu\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from naslib.defaults.trainer import Trainer\n",
    "from naslib.optimizers import DARTSOptimizer\n",
    "from naslib.search_spaces import DartsSearchSpace\n",
    "from naslib.utils import utils, setup_logger, get_config_from_args, set_seed, log_args\n",
    "from naslib.search_spaces.core.graph import Graph, EdgeData\n",
    "from naslib.search_spaces.core import primitives as ops\n",
    "from torch import nn\n",
    "from fvcore.common.config import CfgNode\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mdataset....................................cifar10\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mseed.............................................0\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0msearch_space...........................nasbench201\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mout_dir........................................run\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0moptimizer....................................darts\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0msearchacq_fn_optimization: random_sampling\n",
      "acq_fn_type: its\n",
      "arch_learning_rate: 0.0003\n",
      "arch_weight_decay: 0.001\n",
      "batch_size: 256\n",
      "checkpoint_freq: 1000\n",
      "cutout: False\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 25000\n",
      "debug_predictor: False\n",
      "drop_path_prob: 0.0\n",
      "encoding_type: adjacency_one_hot\n",
      "epochs: 100\n",
      "fidelity: -1\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "k: 10\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.001\n",
      "max_mutations: 1\n",
      "momentum: 0.9\n",
      "num_arches_to_mutate: 2\n",
      "num_candidates: 20\n",
      "num_ensemble: 3\n",
      "num_init: 10\n",
      "output_weights: True\n",
      "population_size: 30\n",
      "predictor_type: var_sparse_gp\n",
      "sample_size: 10\n",
      "seed: 0\n",
      "tau_max: 10\n",
      "tau_min: 0.1\n",
      "train_portion: 0.7\n",
      "unrolled: False\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mevaluationauxiliary_weight: 0.4\n",
      "batch_size: 96\n",
      "checkpoint_freq: 30\n",
      "cutout: True\n",
      "cutout_length: 16\n",
      "cutout_prob: 1.0\n",
      "data_size: 50000\n",
      "dist_backend: nccl\n",
      "dist_url: tcp://127.0.0.1:8888\n",
      "drop_path_prob: 0.2\n",
      "epochs: 600\n",
      "gpu: None\n",
      "grad_clip: 5\n",
      "learning_rate: 0.025\n",
      "learning_rate_min: 0.0\n",
      "momentum: 0.9\n",
      "multiprocessing_distributed: False\n",
      "rank: 0\n",
      "train_portion: 1.0\n",
      "warm_start_epochs: 0\n",
      "weight_decay: 0.0003\n",
      "world_size: 1\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0meval_only....................................False\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mresume.......................................False\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mmodel_path....................................None\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mgpu...........................................None\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0msave.........................run/cifar10/bananas/0\n",
      "\u001b[32m[06/29 16:28:08 nl.utils.utils]: \u001b[0mdata../project/dl2022s/robertsj/NASLib/naslib/data\n"
     ]
    }
   ],
   "source": [
    "config = utils.get_config_from_args(config_type='nas')\n",
    "config.optimizer = 'darts'\n",
    "utils.set_seed(config.seed)\n",
    "clear_output(wait=True)\n",
    "utils.log_args(config)\n",
    "\n",
    "logger = setup_logger(config.save + '/log.log')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimum(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.minimum(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None\n",
    "    \n",
    "\n",
    "class Maximum(ops.AbstractPrimitive):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(locals())\n",
    "\n",
    "    def forward(self, x, edge_data=None):\n",
    "        return torch.maximum(x[0], x[1])\n",
    "\n",
    "    def get_embedded_ops(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'a_stage_2'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        stages = ['a_stage_1', 'a_stage_2']\n",
    "\n",
    "        # cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'activation_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2) # intermediate node\n",
    "        activation_cell.add_node(3) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData())]) # mutable intermediate edge\n",
    "        activation_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable output edge\n",
    "\n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        for i, scope in zip(range(3, 5), stages):\n",
    "            self.add_node(i, subgraph=deepcopy(activation_cell).set_scope(scope).set_input([i-1])) # activation cell i\n",
    "            self.nodes[i]['subgraph'].name = scope\n",
    "        self.add_node(5) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 5)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[4, 5].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            )\n",
    "\n",
    "    def _set_ops(self, edge):\n",
    "        edge.data.set('op', [\n",
    "            ops.Sequential(nn.ReLU()),\n",
    "            ops.Sequential(nn.Hardswish()),\n",
    "            ops.Sequential(nn.LeakyReLU()),\n",
    "            ops.Sequential(nn.Identity())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stack():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, tensors, edges_data=None):\n",
    "        return torch.stack(tensors)\n",
    "    \n",
    "\n",
    "class ComplexSearchSpace(Graph):\n",
    "\n",
    "    OPTIMIZER_SCOPE = [\n",
    "        'a_stage_1',\n",
    "        'u_stage_1',\n",
    "        'u_stage_2',\n",
    "        'b_stage_1'\n",
    "    ]\n",
    "\n",
    "    QUERYABLE = False\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        u_stages = ['u_stage_1', 'u_stage_2']\n",
    "        \n",
    "        # unary cell definition\n",
    "        unary_cell = Graph()\n",
    "        unary_cell.name = 'u_cell'\n",
    "        unary_cell.add_node(1) # input node\n",
    "        unary_cell.add_node(2) # intermediate node\n",
    "        unary_cell.add_node(3) # output node\n",
    "        unary_cell.add_edges_from([(1, 2, EdgeData())]) # mutable edge\n",
    "        unary_cell.edges[1, 2].set('cell_name', 'u_cell')\n",
    "        unary_cell.add_edges_from([(2, 3, EdgeData().finalize())]) # immutable edge\n",
    "        \n",
    "        # binary cell definition\n",
    "        binary_cell = Graph()\n",
    "        binary_cell.name = 'b_cell'\n",
    "        binary_cell.add_node(1) # input node\n",
    "        binary_cell.add_node(2) # input node\n",
    "        binary_cell.add_node(3) # concatination node\n",
    "        binary_cell.nodes[3]['comb_op'] = stack()\n",
    "        binary_cell.add_node(4) # intermediate node\n",
    "        binary_cell.add_node(5) # output node\n",
    "        binary_cell.add_edges_from([(3, 4, EdgeData())]) # mutable edge\n",
    "        binary_cell.edges[3, 4].set('cell_name', 'b_cell') \n",
    "        binary_cell.add_edges_from([(1, 3, EdgeData().finalize()),\n",
    "                                    (2, 3, EdgeData().finalize()),\n",
    "                                    (4, 5, EdgeData().finalize())]) # immutable edges\n",
    "        \n",
    "        # activation cell definition\n",
    "        activation_cell = Graph()\n",
    "        activation_cell.name = 'a_cell'\n",
    "        activation_cell.add_node(1) # input node\n",
    "        activation_cell.add_node(2, subgraph=deepcopy(unary_cell).set_scope('u_stage_1').set_input([1])) # unary node\n",
    "        activation_cell.nodes[2]['subgraph'].name = 'u_stage_1'\n",
    "        activation_cell.add_node(3, subgraph=deepcopy(unary_cell).set_scope('u_stage_2').set_input([1])) # unary node\n",
    "        activation_cell.nodes[3]['subgraph'].name = 'u_stage_2'\n",
    "        activation_cell.add_node(4, subgraph=deepcopy(binary_cell).set_scope('b_stage_1').set_input([2, 3])) # binary node\n",
    "        activation_cell.nodes[4]['subgraph'].name = 'b_stage_1'\n",
    "        activation_cell.add_node(5) # output node\n",
    "        activation_cell.add_edges_from([(1, 2, EdgeData().finalize()), \n",
    "                                        (1, 3, EdgeData().finalize()),\n",
    "                                        (2, 4, EdgeData().finalize()),\n",
    "                                        (3, 4, EdgeData().finalize()), \n",
    "                                        (4, 5, EdgeData().finalize())])\n",
    "        \n",
    "        # macroarchitecture definition\n",
    "        self.name = 'makrograph'\n",
    "        self.add_node(1) # input node\n",
    "        self.add_node(2) # intermediate node\n",
    "        self.add_node(3, subgraph=deepcopy(activation_cell).set_input([2])) # activation cell\n",
    "        self.nodes[3]['subgraph'].name = 'a_stage_1'\n",
    "        self.add_node(4) # output node\n",
    "        self.add_edges_from([(i, i+1, EdgeData()) for i in range(1, 4)])\n",
    "        self.edges[1, 2].set('op',\n",
    "            ops.Sequential(\n",
    "                nn.Conv2d(3, 6, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(6, 16, 5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )) # convolutional edge\n",
    "        self.edges[3, 4].set('op', \n",
    "            ops.Sequential(\n",
    "                nn.Linear(400, 10), \n",
    "                nn.Softmax(dim=1)\n",
    "            )) # linear edge\n",
    "        \n",
    "        for scope in u_stages:\n",
    "            self.update_edges(\n",
    "                update_func=lambda edge: self._set_unary_ops(edge),\n",
    "                scope=scope,\n",
    "                private_edge_data=True,\n",
    "            ) # set unary cell ops\n",
    "        \n",
    "        self.update_edges(\n",
    "            update_func=lambda edge: self._set_binary_ops(edge),\n",
    "            scope='b_stage_1',\n",
    "            private_edge_data=True\n",
    "        ) # set binary cell ops\n",
    "        \n",
    "\n",
    "    def _set_unary_ops(self, edge):\n",
    "        edge.data.set('op', [ops.Identity(), ops.Zero(stride=1)]) \n",
    "        \n",
    "        \n",
    "    def _set_binary_ops(self, edge):\n",
    "        edge.data.set('op', [Minimum(), Maximum()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = ComplexSearchSpace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:47:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mParsed graph:\n",
      "Graph a_stage_1:\n",
      " Graph(\n",
      "  (a_stage_1-edge(1,2)): Identity()\n",
      "  (a_stage_1-edge(1,3)): Identity()\n",
      "  (a_stage_1-subgraph_at(2)): Graph u_stage_1-0.1007012, scope u_stage_1, 3 nodes\n",
      "  (a_stage_1-edge(2,4)): Identity()\n",
      "  (a_stage_1-subgraph_at(3)): Graph u_stage_2-0.1007012, scope u_stage_2, 3 nodes\n",
      "  (a_stage_1-edge(3,4)): Identity()\n",
      "  (a_stage_1-subgraph_at(4)): Graph b_stage_1-0.4341718, scope b_stage_1, 5 nodes\n",
      "  (a_stage_1-edge(4,5)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph b_stage_1:\n",
      " Graph(\n",
      "  (b_stage_1-edge(1,3)): Identity()\n",
      "  (b_stage_1-edge(2,3)): Identity()\n",
      "  (b_stage_1-edge(3,4)): MixedOp(\n",
      "    (primitive-0): Minimum()\n",
      "    (primitive-1): Maximum()\n",
      "  )\n",
      "  (b_stage_1-edge(4,5)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_1:\n",
      " Graph(\n",
      "  (u_stage_1-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_1-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph u_stage_2:\n",
      " Graph(\n",
      "  (u_stage_2-edge(1,2)): MixedOp(\n",
      "    (primitive-0): Identity()\n",
      "    (primitive-1): Zero (stride=1)\n",
      "  )\n",
      "  (u_stage_2-edge(2,3)): Identity()\n",
      ")\n",
      "==========\n",
      "Graph makrograph:\n",
      " ComplexSearchSpace(\n",
      "  (makrograph-edge(1,2)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (makrograph-edge(2,3)): Identity()\n",
      "  (makrograph-subgraph_at(3)): Graph a_stage_1-0.6108870, scope None, 5 nodes\n",
      "  (makrograph-edge(3,4)): Sequential(\n",
      "    (op): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=10, bias=True)\n",
      "      (1): Softmax(dim=1)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = DARTSOptimizer(config)\n",
    "optimizer.adapt_search_space(search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:47:51 nl.defaults.trainer]: \u001b[0mparam size = 0.006882MB\n",
      "\u001b[32m[06/29 16:47:51 nl.defaults.trainer]: \u001b[0mStart training\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/29 16:47:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000181, -0.000723, 0\n",
      "+0.000181, -0.001192, 0\n",
      "+0.000235, -0.000816, 0\n",
      "\u001b[32m[06/29 16:47:53 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.30400, validation loss: 2.30305, learning rate: [0.025]\n",
      "\u001b[32m[06/29 16:47:53 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   43548 KB |   60153 KB |  200065 KB |  156517 KB |\n",
      "|       from large pool |   40960 KB |   54320 KB |  141824 KB |  100864 KB |\n",
      "|       from small pool |    2588 KB |    9361 KB |   58241 KB |   55653 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   43548 KB |   60153 KB |  200065 KB |  156517 KB |\n",
      "|       from large pool |   40960 KB |   54320 KB |  141824 KB |  100864 KB |\n",
      "|       from small pool |    2588 KB |    9361 KB |   58241 KB |   55653 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   73728 KB |   73728 KB |   73728 KB |       0 B  |\n",
      "|       from large pool |   61440 KB |   61440 KB |   61440 KB |       0 B  |\n",
      "|       from small pool |   12288 KB |   12288 KB |   12288 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    3556 KB |   44766 KB |  241651 KB |  238095 KB |\n",
      "|       from large pool |       0 KB |   41936 KB |  170528 KB |  170528 KB |\n",
      "|       from small pool |    3556 KB |    4057 KB |   71123 KB |   67567 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      78    |      96    |     413    |     335    |\n",
      "|       from large pool |      14    |      19    |      52    |      38    |\n",
      "|       from small pool |      64    |      78    |     361    |     297    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      78    |      96    |     413    |     335    |\n",
      "|       from large pool |      14    |      19    |      52    |      38    |\n",
      "|       from small pool |      64    |      78    |     361    |     297    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       9    |       9    |       9    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       6    |       6    |       6    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       9    |      12    |     157    |     148    |\n",
      "|       from large pool |       0    |       5    |      23    |      23    |\n",
      "|       from small pool |       9    |      10    |     134    |     125    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[06/29 16:47:54 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   25881 KB |   60153 KB |  294150 KB |  268269 KB |\n",
      "|       from large pool |   24528 KB |   54320 KB |  185176 KB |  160648 KB |\n",
      "|       from small pool |    1353 KB |    9361 KB |  108974 KB |  107621 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   25881 KB |   60153 KB |  294150 KB |  268269 KB |\n",
      "|       from large pool |   24528 KB |   54320 KB |  185176 KB |  160648 KB |\n",
      "|       from small pool |    1353 KB |    9361 KB |  108974 KB |  107621 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   73728 KB |   73728 KB |   73728 KB |       0 B  |\n",
      "|       from large pool |   61440 KB |   61440 KB |   61440 KB |       0 B  |\n",
      "|       from small pool |   12288 KB |   12288 KB |   12288 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   21223 KB |   44766 KB |  357691 KB |  336468 KB |\n",
      "|       from large pool |   16432 KB |   41936 KB |  230312 KB |  213880 KB |\n",
      "|       from small pool |    4791 KB |    4873 KB |  127379 KB |  122588 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      58    |      96    |     673    |     615    |\n",
      "|       from large pool |       8    |      19    |      68    |      60    |\n",
      "|       from small pool |      50    |      78    |     605    |     555    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      58    |      96    |     673    |     615    |\n",
      "|       from large pool |       8    |      19    |      68    |      60    |\n",
      "|       from small pool |      50    |      78    |     605    |     555    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       9    |       9    |       9    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       6    |       6    |       6    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      16    |     297    |     284    |\n",
      "|       from large pool |       2    |       5    |      35    |      33    |\n",
      "|       from small pool |      11    |      13    |     262    |     251    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "\u001b[32m[06/29 16:47:54 nl.defaults.trainer]: \u001b[0mcuda consumption\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   25881 KB |   60153 KB |  388235 KB |  362354 KB |\n",
      "|       from large pool |   24528 KB |   54320 KB |  228528 KB |  204000 KB |\n",
      "|       from small pool |    1353 KB |    9361 KB |  159707 KB |  158354 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   25881 KB |   60153 KB |  388235 KB |  362354 KB |\n",
      "|       from large pool |   24528 KB |   54320 KB |  228528 KB |  204000 KB |\n",
      "|       from small pool |    1353 KB |    9361 KB |  159707 KB |  158354 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   73728 KB |   73728 KB |   73728 KB |       0 B  |\n",
      "|       from large pool |   61440 KB |   61440 KB |   61440 KB |       0 B  |\n",
      "|       from small pool |   12288 KB |   12288 KB |   12288 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   21223 KB |   44766 KB |  456064 KB |  434841 KB |\n",
      "|       from large pool |   16432 KB |   41936 KB |  273664 KB |  257232 KB |\n",
      "|       from small pool |    4791 KB |    4873 KB |  182400 KB |  177609 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      58    |      96    |     933    |     875    |\n",
      "|       from large pool |       8    |      19    |      84    |      76    |\n",
      "|       from small pool |      50    |      78    |     849    |     799    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      58    |      96    |     933    |     875    |\n",
      "|       from large pool |       8    |      19    |      84    |      76    |\n",
      "|       from small pool |      50    |      78    |     849    |     799    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       9    |       9    |       9    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       6    |       6    |       6    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      14    |      16    |     452    |     438    |\n",
      "|       from large pool |       2    |       5    |      43    |      41    |\n",
      "|       from small pool |      12    |      13    |     409    |     397    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:47:58 nl.defaults.trainer]: \u001b[0mEpoch 0-29, Train loss: 2.29714, validation loss: 2.29821, learning rate: [0.025]\n",
      "\u001b[32m[06/29 16:48:03 nl.defaults.trainer]: \u001b[0mEpoch 0-59, Train loss: 2.24897, validation loss: 2.24524, learning rate: [0.025]\n",
      "\u001b[32m[06/29 16:48:08 nl.defaults.trainer]: \u001b[0mEpoch 0-88, Train loss: 2.23534, validation loss: 2.21499, learning rate: [0.025]\n",
      "\u001b[32m[06/29 16:48:14 nl.defaults.trainer]: \u001b[0mEpoch 0-118, Train loss: 2.17038, validation loss: 2.23552, learning rate: [0.025]\n",
      "\u001b[32m[06/29 16:48:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0002, -0.0002], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0435, -0.0445], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0436, -0.0442], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:48:18 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 20.32286, 65.93429, Validation accuracy: 20.08725, 66.05839\n",
      "\u001b[32m[06/29 16:48:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.000198, -0.000198, 0\n",
      "+0.043537, -0.044523, 0\n",
      "+0.043586, -0.044153, 0\n",
      "\u001b[32m[06/29 16:48:19 nl.defaults.trainer]: \u001b[0mEpoch 1-3, Train loss: 2.13455, validation loss: 2.16058, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/29 16:48:24 nl.defaults.trainer]: \u001b[0mEpoch 1-33, Train loss: 2.16751, validation loss: 2.17440, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/29 16:48:29 nl.defaults.trainer]: \u001b[0mEpoch 1-63, Train loss: 2.14393, validation loss: 2.16291, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/29 16:48:34 nl.defaults.trainer]: \u001b[0mEpoch 1-92, Train loss: 2.15088, validation loss: 2.13417, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/29 16:48:39 nl.defaults.trainer]: \u001b[0mEpoch 1-122, Train loss: 2.13651, validation loss: 2.15954, learning rate: [0.02499407872438878]\n",
      "\u001b[32m[06/29 16:48:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 0.0010, -0.0010], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0697, -0.0707], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0697, -0.0703], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:48:41 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 29.35429, 74.37714, Validation accuracy: 29.19993, 73.84523\n",
      "\u001b[32m[06/29 16:48:41 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.000964, -0.000964, 0\n",
      "+0.069724, -0.070703, 0\n",
      "+0.069697, -0.070260, 0\n",
      "\u001b[32m[06/29 16:48:44 nl.defaults.trainer]: \u001b[0mEpoch 2-14, Train loss: 2.12552, validation loss: 2.17513, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/29 16:48:49 nl.defaults.trainer]: \u001b[0mEpoch 2-44, Train loss: 2.09560, validation loss: 2.11216, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/29 16:48:54 nl.defaults.trainer]: \u001b[0mEpoch 2-74, Train loss: 2.10932, validation loss: 2.14910, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/29 16:48:59 nl.defaults.trainer]: \u001b[0mEpoch 2-104, Train loss: 2.10151, validation loss: 2.14729, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/29 16:49:05 nl.defaults.trainer]: \u001b[0mEpoch 2-134, Train loss: 2.15428, validation loss: 2.08195, learning rate: [0.02497632074113926]\n",
      "\u001b[32m[06/29 16:49:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0014,  0.0014], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0938, -0.0947], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0937, -0.0943], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:49:05 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 32.94000, 76.97714, Validation accuracy: 32.25080, 76.20324\n",
      "\u001b[32m[06/29 16:49:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001447, +0.001447, 1\n",
      "+0.093778, -0.094749, 0\n",
      "+0.093731, -0.094289, 0\n",
      "\u001b[32m[06/29 16:49:10 nl.defaults.trainer]: \u001b[0mEpoch 3-27, Train loss: 2.06670, validation loss: 2.12642, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/29 16:49:15 nl.defaults.trainer]: \u001b[0mEpoch 3-57, Train loss: 2.16251, validation loss: 2.10940, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/29 16:49:20 nl.defaults.trainer]: \u001b[0mEpoch 3-87, Train loss: 2.07409, validation loss: 2.12244, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/29 16:49:25 nl.defaults.trainer]: \u001b[0mEpoch 3-117, Train loss: 2.06061, validation loss: 2.10485, learning rate: [0.024946743575236963]\n",
      "\u001b[32m[06/29 16:49:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0015,  0.0015], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1219, -0.1229], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1218, -0.1224], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:49:28 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 35.48000, 79.66000, Validation accuracy: 35.55258, 79.40807\n",
      "\u001b[32m[06/29 16:49:28 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001456, +0.001456, 1\n",
      "+0.121928, -0.122892, 0\n",
      "+0.121842, -0.122396, 0\n",
      "\u001b[32m[06/29 16:49:30 nl.defaults.trainer]: \u001b[0mEpoch 4-9, Train loss: 2.06756, validation loss: 2.10458, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/29 16:49:35 nl.defaults.trainer]: \u001b[0mEpoch 4-39, Train loss: 2.05781, validation loss: 2.04883, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/29 16:49:40 nl.defaults.trainer]: \u001b[0mEpoch 4-68, Train loss: 2.06015, validation loss: 2.10179, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/29 16:49:45 nl.defaults.trainer]: \u001b[0mEpoch 4-98, Train loss: 2.06044, validation loss: 2.06940, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/29 16:49:51 nl.defaults.trainer]: \u001b[0mEpoch 4-128, Train loss: 2.07842, validation loss: 2.06811, learning rate: [0.024905376415773738]\n",
      "\u001b[32m[06/29 16:49:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0006,  0.0006], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1422, -0.1431], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1421, -0.1427], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:49:52 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 37.02571, 81.74000, Validation accuracy: 36.63036, 81.33554\n",
      "\u001b[32m[06/29 16:49:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000576, +0.000576, 1\n",
      "+0.142193, -0.143148, 0\n",
      "+0.142127, -0.142676, 0\n",
      "\u001b[32m[06/29 16:49:56 nl.defaults.trainer]: \u001b[0mEpoch 5-21, Train loss: 2.03055, validation loss: 2.08578, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/29 16:50:01 nl.defaults.trainer]: \u001b[0mEpoch 5-50, Train loss: 2.09154, validation loss: 2.06555, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/29 16:50:06 nl.defaults.trainer]: \u001b[0mEpoch 5-79, Train loss: 2.05993, validation loss: 2.13044, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/29 16:50:11 nl.defaults.trainer]: \u001b[0mEpoch 5-108, Train loss: 2.05690, validation loss: 2.11580, learning rate: [0.024852260087141656]\n",
      "\u001b[32m[06/29 16:50:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([ 4.0772e-05, -4.0788e-05], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1521, -0.1530], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1520, -0.1525], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:50:16 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy (top1, top5): 37.82571, 82.24286, Validation accuracy: 37.08371, 82.20518\n",
      "\u001b[32m[06/29 16:50:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "+0.000041, -0.000041, 0\n",
      "+0.152066, -0.153009, 0\n",
      "+0.151999, -0.152541, 0\n",
      "\u001b[32m[06/29 16:50:16 nl.defaults.trainer]: \u001b[0mEpoch 6-0, Train loss: 2.09625, validation loss: 2.09718, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/29 16:50:21 nl.defaults.trainer]: \u001b[0mEpoch 6-29, Train loss: 2.06143, validation loss: 2.08847, learning rate: [0.02478744700874427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:50:26 nl.defaults.trainer]: \u001b[0mEpoch 6-59, Train loss: 2.07154, validation loss: 2.01475, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/29 16:50:31 nl.defaults.trainer]: \u001b[0mEpoch 6-89, Train loss: 2.04304, validation loss: 2.09309, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/29 16:50:36 nl.defaults.trainer]: \u001b[0mEpoch 6-119, Train loss: 2.07289, validation loss: 2.07034, learning rate: [0.02478744700874427]\n",
      "\u001b[32m[06/29 16:50:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0019,  0.0019], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1640, -0.1649], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1638, -0.1644], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:50:39 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy (top1, top5): 38.77429, 83.17714, Validation accuracy: 38.22708, 82.88948\n",
      "\u001b[32m[06/29 16:50:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.001871, +0.001871, 1\n",
      "+0.163980, -0.164909, 0\n",
      "+0.163846, -0.164380, 0\n",
      "\u001b[32m[06/29 16:50:41 nl.defaults.trainer]: \u001b[0mEpoch 7-11, Train loss: 2.06390, validation loss: 2.02717, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/29 16:50:47 nl.defaults.trainer]: \u001b[0mEpoch 7-41, Train loss: 2.06033, validation loss: 2.10985, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/29 16:50:52 nl.defaults.trainer]: \u001b[0mEpoch 7-70, Train loss: 2.06104, validation loss: 2.09136, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/29 16:50:57 nl.defaults.trainer]: \u001b[0mEpoch 7-100, Train loss: 2.09790, validation loss: 2.01998, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/29 16:51:02 nl.defaults.trainer]: \u001b[0mEpoch 7-129, Train loss: 2.04617, validation loss: 2.07273, learning rate: [0.024711001143264973]\n",
      "\u001b[32m[06/29 16:51:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0004,  0.0004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1746, -0.1755], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1743, -0.1748], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:51:03 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy (top1, top5): 39.43429, 83.81143, Validation accuracy: 38.86006, 83.46259\n",
      "\u001b[32m[06/29 16:51:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.000393, +0.000393, 1\n",
      "+0.174592, -0.175508, 0\n",
      "+0.174270, -0.174796, 0\n",
      "\u001b[32m[06/29 16:51:07 nl.defaults.trainer]: \u001b[0mEpoch 8-21, Train loss: 2.04990, validation loss: 2.10876, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/29 16:51:12 nl.defaults.trainer]: \u001b[0mEpoch 8-50, Train loss: 2.08902, validation loss: 2.04726, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/29 16:51:17 nl.defaults.trainer]: \u001b[0mEpoch 8-80, Train loss: 1.94534, validation loss: 2.05003, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/29 16:51:22 nl.defaults.trainer]: \u001b[0mEpoch 8-109, Train loss: 2.00119, validation loss: 2.05694, learning rate: [0.024622997933543576]\n",
      "\u001b[32m[06/29 16:51:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0022,  0.0022], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1882, -0.1891], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1877, -0.1882], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:51:27 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy (top1, top5): 41.23143, 85.05143, Validation accuracy: 39.70404, 84.58599\n",
      "\u001b[32m[06/29 16:51:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002180, +0.002180, 1\n",
      "+0.188174, -0.189077, 0\n",
      "+0.187686, -0.188205, 0\n",
      "\u001b[32m[06/29 16:51:27 nl.defaults.trainer]: \u001b[0mEpoch 9-2, Train loss: 2.01436, validation loss: 2.05478, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/29 16:51:32 nl.defaults.trainer]: \u001b[0mEpoch 9-32, Train loss: 2.03018, validation loss: 2.03404, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/29 16:51:37 nl.defaults.trainer]: \u001b[0mEpoch 9-62, Train loss: 2.00482, validation loss: 2.04270, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/29 16:51:43 nl.defaults.trainer]: \u001b[0mEpoch 9-92, Train loss: 2.01131, validation loss: 2.05318, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/29 16:51:48 nl.defaults.trainer]: \u001b[0mEpoch 9-122, Train loss: 2.09199, validation loss: 2.03801, learning rate: [0.02452352422812332]\n",
      "\u001b[32m[06/29 16:51:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0027,  0.0027], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2027, -0.2036], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2019, -0.2024], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:51:50 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy (top1, top5): 41.59429, 85.62857, Validation accuracy: 40.95575, 85.33588\n",
      "\u001b[32m[06/29 16:51:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.002688, +0.002688, 1\n",
      "+0.202672, -0.203560, 0\n",
      "+0.201881, -0.202391, 0\n",
      "\u001b[32m[06/29 16:51:53 nl.defaults.trainer]: \u001b[0mEpoch 10-15, Train loss: 2.03085, validation loss: 2.02627, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/29 16:51:58 nl.defaults.trainer]: \u001b[0mEpoch 10-45, Train loss: 1.98829, validation loss: 2.03314, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/29 16:52:03 nl.defaults.trainer]: \u001b[0mEpoch 10-75, Train loss: 1.95933, validation loss: 2.03710, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/29 16:52:08 nl.defaults.trainer]: \u001b[0mEpoch 10-105, Train loss: 2.04109, validation loss: 1.96883, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/29 16:52:13 nl.defaults.trainer]: \u001b[0mEpoch 10-135, Train loss: 2.03197, validation loss: 2.07749, learning rate: [0.024412678195541847]\n",
      "\u001b[32m[06/29 16:52:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0076,  0.0076], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2173, -0.2182], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2158, -0.2163], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:52:13 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy (top1, top5): 42.47429, 86.60571, Validation accuracy: 42.15899, 86.22263\n",
      "\u001b[32m[06/29 16:52:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.007580, +0.007580, 1\n",
      "+0.217343, -0.218218, 0\n",
      "+0.215762, -0.216264, 0\n",
      "\u001b[32m[06/29 16:52:18 nl.defaults.trainer]: \u001b[0mEpoch 11-27, Train loss: 2.00717, validation loss: 2.01685, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/29 16:52:23 nl.defaults.trainer]: \u001b[0mEpoch 11-56, Train loss: 2.07889, validation loss: 2.03890, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/29 16:52:28 nl.defaults.trainer]: \u001b[0mEpoch 11-86, Train loss: 2.02136, validation loss: 2.01996, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/29 16:52:33 nl.defaults.trainer]: \u001b[0mEpoch 11-116, Train loss: 1.98870, validation loss: 2.01364, learning rate: [0.02429056922745071]\n",
      "\u001b[32m[06/29 16:52:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0133,  0.0133], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2332, -0.2340], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2297, -0.2302], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:52:37 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy (top1, top5): 43.00857, 86.98571, Validation accuracy: 42.89177, 87.17495\n",
      "\u001b[32m[06/29 16:52:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.013287, +0.013287, 1\n",
      "+0.233177, -0.234038, 0\n",
      "+0.229711, -0.230205, 0\n",
      "\u001b[32m[06/29 16:52:38 nl.defaults.trainer]: \u001b[0mEpoch 12-8, Train loss: 1.99052, validation loss: 2.02038, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/29 16:52:44 nl.defaults.trainer]: \u001b[0mEpoch 12-37, Train loss: 2.01675, validation loss: 1.98940, learning rate: [0.02415731783065902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:52:49 nl.defaults.trainer]: \u001b[0mEpoch 12-66, Train loss: 1.97799, validation loss: 2.04178, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/29 16:52:54 nl.defaults.trainer]: \u001b[0mEpoch 12-95, Train loss: 2.02713, validation loss: 2.06236, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/29 16:52:59 nl.defaults.trainer]: \u001b[0mEpoch 12-124, Train loss: 2.02625, validation loss: 2.03613, learning rate: [0.02415731783065902]\n",
      "\u001b[32m[06/29 16:53:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0077,  0.0077], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2398, -0.2407], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2366, -0.2370], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:53:01 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy (top1, top5): 43.85143, 87.68571, Validation accuracy: 42.86040, 87.29756\n",
      "\u001b[32m[06/29 16:53:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.007718, +0.007718, 1\n",
      "+0.239809, -0.240653, 0\n",
      "+0.236558, -0.237043, 0\n",
      "\u001b[32m[06/29 16:53:04 nl.defaults.trainer]: \u001b[0mEpoch 13-16, Train loss: 1.98925, validation loss: 2.04311, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/29 16:53:09 nl.defaults.trainer]: \u001b[0mEpoch 13-45, Train loss: 2.01017, validation loss: 2.03173, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/29 16:53:14 nl.defaults.trainer]: \u001b[0mEpoch 13-74, Train loss: 1.96614, validation loss: 2.04097, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/29 16:53:19 nl.defaults.trainer]: \u001b[0mEpoch 13-103, Train loss: 2.00526, validation loss: 2.01911, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/29 16:53:24 nl.defaults.trainer]: \u001b[0mEpoch 13-132, Train loss: 1.99793, validation loss: 2.02999, learning rate: [0.024013055508207773]\n",
      "\u001b[32m[06/29 16:53:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0192,  0.0192], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2511, -0.2520], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2455, -0.2460], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:53:25 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy (top1, top5): 44.52857, 88.05429, Validation accuracy: 43.50194, 87.98757\n",
      "\u001b[32m[06/29 16:53:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.019248, +0.019248, 1\n",
      "+0.251128, -0.251956, 0\n",
      "+0.245493, -0.245968, 0\n",
      "\u001b[32m[06/29 16:53:29 nl.defaults.trainer]: \u001b[0mEpoch 14-24, Train loss: 2.07468, validation loss: 2.02948, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/29 16:53:34 nl.defaults.trainer]: \u001b[0mEpoch 14-54, Train loss: 2.01360, validation loss: 2.00500, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/29 16:53:39 nl.defaults.trainer]: \u001b[0mEpoch 14-83, Train loss: 2.01460, validation loss: 2.05403, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/29 16:53:44 nl.defaults.trainer]: \u001b[0mEpoch 14-113, Train loss: 2.06177, validation loss: 2.02748, learning rate: [0.023857924629592235]\n",
      "\u001b[32m[06/29 16:53:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0316,  0.0316], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2612, -0.2621], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2513, -0.2518], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:53:48 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy (top1, top5): 44.73429, 88.14571, Validation accuracy: 43.88116, 88.00753\n",
      "\u001b[32m[06/29 16:53:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.031596, +0.031596, 1\n",
      "+0.261246, -0.262058, 0\n",
      "+0.251312, -0.251777, 0\n",
      "\u001b[32m[06/29 16:53:50 nl.defaults.trainer]: \u001b[0mEpoch 15-5, Train loss: 1.97945, validation loss: 2.03024, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/29 16:53:55 nl.defaults.trainer]: \u001b[0mEpoch 15-35, Train loss: 2.01782, validation loss: 2.04406, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/29 16:54:00 nl.defaults.trainer]: \u001b[0mEpoch 15-64, Train loss: 2.01860, validation loss: 1.96542, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/29 16:54:05 nl.defaults.trainer]: \u001b[0mEpoch 15-94, Train loss: 2.02153, validation loss: 2.03674, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/29 16:54:10 nl.defaults.trainer]: \u001b[0mEpoch 15-123, Train loss: 2.01951, validation loss: 2.00870, learning rate: [0.023692078290260415]\n",
      "\u001b[32m[06/29 16:54:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0472,  0.0472], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2720, -0.2728], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2543, -0.2547], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:54:12 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy (top1, top5): 45.24857, 88.80571, Validation accuracy: 45.14142, 88.79448\n",
      "\u001b[32m[06/29 16:54:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.047203, +0.047203, 1\n",
      "+0.271990, -0.272785, 0\n",
      "+0.254259, -0.254714, 0\n",
      "\u001b[32m[06/29 16:54:15 nl.defaults.trainer]: \u001b[0mEpoch 16-16, Train loss: 1.99051, validation loss: 2.02064, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/29 16:54:20 nl.defaults.trainer]: \u001b[0mEpoch 16-46, Train loss: 1.99917, validation loss: 2.03762, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/29 16:54:25 nl.defaults.trainer]: \u001b[0mEpoch 16-76, Train loss: 1.97347, validation loss: 1.98764, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/29 16:54:30 nl.defaults.trainer]: \u001b[0mEpoch 16-106, Train loss: 2.00420, validation loss: 2.02071, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/29 16:54:36 nl.defaults.trainer]: \u001b[0mEpoch 16-136, Train loss: 1.98991, validation loss: 1.99995, learning rate: [0.023515680160526364]\n",
      "\u001b[32m[06/29 16:54:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0727,  0.0727], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2822, -0.2830], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2508, -0.2512], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:54:36 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy (top1, top5): 45.39143, 88.69143, Validation accuracy: 44.38583, 88.57493\n",
      "\u001b[32m[06/29 16:54:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.072672, +0.072672, 1\n",
      "+0.282235, -0.283014, 0\n",
      "+0.250752, -0.251194, 0\n",
      "\u001b[32m[06/29 16:54:41 nl.defaults.trainer]: \u001b[0mEpoch 17-28, Train loss: 1.99436, validation loss: 2.02246, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/29 16:54:46 nl.defaults.trainer]: \u001b[0mEpoch 17-57, Train loss: 1.99259, validation loss: 1.98195, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/29 16:54:51 nl.defaults.trainer]: \u001b[0mEpoch 17-87, Train loss: 2.02471, validation loss: 2.05565, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/29 16:54:56 nl.defaults.trainer]: \u001b[0mEpoch 17-116, Train loss: 2.01346, validation loss: 1.98350, learning rate: [0.023328904324047328]\n",
      "\u001b[32m[06/29 16:54:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.0830,  0.0830], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2874, -0.2882], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2448, -0.2452], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:54:59 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy (top1, top5): 45.56286, 88.80857, Validation accuracy: 44.84489, 88.51220\n",
      "\u001b[32m[06/29 16:54:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.083009, +0.083009, 1\n",
      "+0.287446, -0.288207, 0\n",
      "+0.244765, -0.245196, 0\n",
      "\u001b[32m[06/29 16:55:01 nl.defaults.trainer]: \u001b[0mEpoch 18-9, Train loss: 2.04233, validation loss: 1.98810, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/29 16:55:06 nl.defaults.trainer]: \u001b[0mEpoch 18-39, Train loss: 1.95770, validation loss: 1.99286, learning rate: [0.023131935106024185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:55:12 nl.defaults.trainer]: \u001b[0mEpoch 18-69, Train loss: 1.97087, validation loss: 2.00626, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/29 16:55:17 nl.defaults.trainer]: \u001b[0mEpoch 18-99, Train loss: 1.96976, validation loss: 2.00142, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/29 16:55:22 nl.defaults.trainer]: \u001b[0mEpoch 18-129, Train loss: 1.98807, validation loss: 2.02667, learning rate: [0.023131935106024185]\n",
      "\u001b[32m[06/29 16:55:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1066,  0.1066], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2996, -0.3004], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2365, -0.2369], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:55:23 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy (top1, top5): 46.68571, 89.32857, Validation accuracy: 45.54630, 88.65192\n",
      "\u001b[32m[06/29 16:55:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.106566, +0.106566, 1\n",
      "+0.299610, -0.300355, 0\n",
      "+0.236485, -0.236906, 0\n",
      "\u001b[32m[06/29 16:55:27 nl.defaults.trainer]: \u001b[0mEpoch 19-22, Train loss: 2.02192, validation loss: 2.02230, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/29 16:55:32 nl.defaults.trainer]: \u001b[0mEpoch 19-52, Train loss: 2.05371, validation loss: 2.07725, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/29 16:55:37 nl.defaults.trainer]: \u001b[0mEpoch 19-82, Train loss: 1.95119, validation loss: 1.91341, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/29 16:55:42 nl.defaults.trainer]: \u001b[0mEpoch 19-112, Train loss: 1.98884, validation loss: 1.98291, learning rate: [0.022924966891294748]\n",
      "\u001b[32m[06/29 16:55:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1407,  0.1407], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3217, -0.3225], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2266, -0.2270], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:55:47 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy (top1, top5): 47.20857, 89.59429, Validation accuracy: 47.25422, 89.27635\n",
      "\u001b[32m[06/29 16:55:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.140664, +0.140664, 1\n",
      "+0.321742, -0.322474, 0\n",
      "+0.226635, -0.227045, 0\n",
      "\u001b[32m[06/29 16:55:47 nl.defaults.trainer]: \u001b[0mEpoch 20-4, Train loss: 1.99330, validation loss: 1.99139, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/29 16:55:53 nl.defaults.trainer]: \u001b[0mEpoch 20-34, Train loss: 1.96129, validation loss: 2.05452, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/29 16:55:58 nl.defaults.trainer]: \u001b[0mEpoch 20-63, Train loss: 1.98379, validation loss: 1.96818, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/29 16:56:03 nl.defaults.trainer]: \u001b[0mEpoch 20-93, Train loss: 1.96886, validation loss: 2.00540, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/29 16:56:08 nl.defaults.trainer]: \u001b[0mEpoch 20-122, Train loss: 2.01578, validation loss: 2.02716, learning rate: [0.022708203932499376]\n",
      "\u001b[32m[06/29 16:56:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1687,  0.1687], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3418, -0.3425], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2135, -0.2139], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:56:10 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy (top1, top5): 47.49714, 89.86000, Validation accuracy: 46.93203, 89.66697\n",
      "\u001b[32m[06/29 16:56:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.168702, +0.168702, 1\n",
      "+0.341807, -0.342527, 0\n",
      "+0.213463, -0.213863, 0\n",
      "\u001b[32m[06/29 16:56:13 nl.defaults.trainer]: \u001b[0mEpoch 21-15, Train loss: 1.96812, validation loss: 1.96306, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/29 16:56:18 nl.defaults.trainer]: \u001b[0mEpoch 21-45, Train loss: 1.96954, validation loss: 2.02236, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/29 16:56:23 nl.defaults.trainer]: \u001b[0mEpoch 21-75, Train loss: 1.97751, validation loss: 2.01031, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/29 16:56:28 nl.defaults.trainer]: \u001b[0mEpoch 21-105, Train loss: 1.98152, validation loss: 1.98428, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/29 16:56:33 nl.defaults.trainer]: \u001b[0mEpoch 21-135, Train loss: 1.97944, validation loss: 2.06941, learning rate: [0.02248186014850829]\n",
      "\u001b[32m[06/29 16:56:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.1956,  0.1956], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3630, -0.3637], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1982, -0.1986], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:56:33 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy (top1, top5): 47.81429, 89.72857, Validation accuracy: 47.15443, 89.67838\n",
      "\u001b[32m[06/29 16:56:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.195559, +0.195558, 1\n",
      "+0.363000, -0.363708, 0\n",
      "+0.198189, -0.198581, 0\n",
      "\u001b[32m[06/29 16:56:38 nl.defaults.trainer]: \u001b[0mEpoch 22-28, Train loss: 1.98509, validation loss: 1.96148, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/29 16:56:43 nl.defaults.trainer]: \u001b[0mEpoch 22-57, Train loss: 1.97980, validation loss: 2.00073, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/29 16:56:49 nl.defaults.trainer]: \u001b[0mEpoch 22-87, Train loss: 1.99677, validation loss: 1.97271, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/29 16:56:54 nl.defaults.trainer]: \u001b[0mEpoch 22-116, Train loss: 1.95581, validation loss: 1.96502, learning rate: [0.022246158913309475]\n",
      "\u001b[32m[06/29 16:56:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2265,  0.2265], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.3829, -0.3836], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1746, -0.1750], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:56:57 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy (top1, top5): 47.60857, 89.82286, Validation accuracy: 46.82083, 89.75821\n",
      "\u001b[32m[06/29 16:56:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.226461, +0.226461, 1\n",
      "+0.382924, -0.383621, 0\n",
      "+0.174597, -0.174982, 0\n",
      "\u001b[32m[06/29 16:56:59 nl.defaults.trainer]: \u001b[0mEpoch 23-9, Train loss: 1.98037, validation loss: 1.98887, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/29 16:57:04 nl.defaults.trainer]: \u001b[0mEpoch 23-39, Train loss: 2.02381, validation loss: 1.95653, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/29 16:57:09 nl.defaults.trainer]: \u001b[0mEpoch 23-68, Train loss: 1.97038, validation loss: 2.02243, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/29 16:57:14 nl.defaults.trainer]: \u001b[0mEpoch 23-97, Train loss: 2.04057, validation loss: 1.91111, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/29 16:57:19 nl.defaults.trainer]: \u001b[0mEpoch 23-126, Train loss: 1.97426, validation loss: 2.03112, learning rate: [0.022001332835565518]\n",
      "\u001b[32m[06/29 16:57:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2599,  0.2599], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4077, -0.4084], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1478, -0.1481], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:57:21 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy (top1, top5): 48.30571, 90.22286, Validation accuracy: 47.65910, 89.89792\n",
      "\u001b[32m[06/29 16:57:21 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.259947, +0.259947, 1\n",
      "+0.407687, -0.408374, 0\n",
      "+0.147755, -0.148134, 0\n",
      "\u001b[32m[06/29 16:57:24 nl.defaults.trainer]: \u001b[0mEpoch 24-18, Train loss: 1.92024, validation loss: 1.90740, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/29 16:57:29 nl.defaults.trainer]: \u001b[0mEpoch 24-48, Train loss: 1.98255, validation loss: 1.99986, learning rate: [0.02174762352905694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:57:35 nl.defaults.trainer]: \u001b[0mEpoch 24-78, Train loss: 2.00406, validation loss: 2.00500, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/29 16:57:40 nl.defaults.trainer]: \u001b[0mEpoch 24-108, Train loss: 1.93881, validation loss: 1.97700, learning rate: [0.02174762352905694]\n",
      "\u001b[32m[06/29 16:57:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2740,  0.2740], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4179, -0.4186], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1333, -0.1337], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:57:45 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy (top1, top5): 48.75714, 90.04857, Validation accuracy: 47.87865, 90.14884\n",
      "\u001b[32m[06/29 16:57:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.274009, +0.274009, 1\n",
      "+0.417941, -0.418618, 0\n",
      "+0.133333, -0.133707, 0\n",
      "\u001b[32m[06/29 16:57:45 nl.defaults.trainer]: \u001b[0mEpoch 25-1, Train loss: 1.95015, validation loss: 1.97454, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/29 16:57:50 nl.defaults.trainer]: \u001b[0mEpoch 25-30, Train loss: 1.96729, validation loss: 1.96508, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/29 16:57:55 nl.defaults.trainer]: \u001b[0mEpoch 25-60, Train loss: 1.96608, validation loss: 1.97849, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/29 16:58:00 nl.defaults.trainer]: \u001b[0mEpoch 25-89, Train loss: 1.97185, validation loss: 1.99817, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/29 16:58:05 nl.defaults.trainer]: \u001b[0mEpoch 25-118, Train loss: 1.97933, validation loss: 1.94231, learning rate: [0.021485281374238573]\n",
      "\u001b[32m[06/29 16:58:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2837,  0.2837], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4289, -0.4296], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1252, -0.1256], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:58:08 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy (top1, top5): 48.94286, 90.25429, Validation accuracy: 48.59717, 90.47388\n",
      "\u001b[32m[06/29 16:58:08 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.283672, +0.283672, 1\n",
      "+0.428921, -0.429588, 0\n",
      "+0.125246, -0.125616, 0\n",
      "\u001b[32m[06/29 16:58:10 nl.defaults.trainer]: \u001b[0mEpoch 26-10, Train loss: 1.95376, validation loss: 1.97355, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/29 16:58:15 nl.defaults.trainer]: \u001b[0mEpoch 26-39, Train loss: 1.98367, validation loss: 1.98742, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/29 16:58:21 nl.defaults.trainer]: \u001b[0mEpoch 26-69, Train loss: 1.97369, validation loss: 1.97598, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/29 16:58:26 nl.defaults.trainer]: \u001b[0mEpoch 26-99, Train loss: 1.94570, validation loss: 1.97172, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/29 16:58:31 nl.defaults.trainer]: \u001b[0mEpoch 26-128, Train loss: 2.00307, validation loss: 1.92996, learning rate: [0.021214565271144268]\n",
      "\u001b[32m[06/29 16:58:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.2969,  0.2969], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4449, -0.4455], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1164, -0.1168], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:58:32 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy (top1, top5): 49.48286, 90.56000, Validation accuracy: 49.27578, 90.69913\n",
      "\u001b[32m[06/29 16:58:32 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.296905, +0.296905, 1\n",
      "+0.444866, -0.445524, 0\n",
      "+0.116408, -0.116773, 0\n",
      "\u001b[32m[06/29 16:58:36 nl.defaults.trainer]: \u001b[0mEpoch 27-20, Train loss: 1.92390, validation loss: 1.96814, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/29 16:58:41 nl.defaults.trainer]: \u001b[0mEpoch 27-49, Train loss: 1.97404, validation loss: 1.97223, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/29 16:58:46 nl.defaults.trainer]: \u001b[0mEpoch 27-79, Train loss: 1.93263, validation loss: 1.97860, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/29 16:58:51 nl.defaults.trainer]: \u001b[0mEpoch 27-109, Train loss: 1.87297, validation loss: 1.93646, learning rate: [0.020935742383883828]\n",
      "\u001b[32m[06/29 16:58:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3157,  0.3157], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4605, -0.4611], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0983, -0.0986], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:58:56 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy (top1, top5): 49.89714, 90.64857, Validation accuracy: 48.43750, 90.40545\n",
      "\u001b[32m[06/29 16:58:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.315722, +0.315722, 1\n",
      "+0.460484, -0.461135, 0\n",
      "+0.098281, -0.098643, 0\n",
      "\u001b[32m[06/29 16:58:56 nl.defaults.trainer]: \u001b[0mEpoch 28-1, Train loss: 1.95917, validation loss: 1.95414, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/29 16:59:01 nl.defaults.trainer]: \u001b[0mEpoch 28-30, Train loss: 1.94518, validation loss: 1.97087, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/29 16:59:06 nl.defaults.trainer]: \u001b[0mEpoch 28-59, Train loss: 1.94744, validation loss: 2.00977, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/29 16:59:11 nl.defaults.trainer]: \u001b[0mEpoch 28-88, Train loss: 1.93211, validation loss: 1.97815, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/29 16:59:16 nl.defaults.trainer]: \u001b[0mEpoch 28-117, Train loss: 1.95554, validation loss: 1.97103, learning rate: [0.020649087876984284]\n",
      "\u001b[32m[06/29 16:59:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3247,  0.3247], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4695, -0.4701], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0903, -0.0906], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:59:20 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy (top1, top5): 50.08857, 90.78000, Validation accuracy: 49.43260, 90.87876\n",
      "\u001b[32m[06/29 16:59:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.324662, +0.324662, 1\n",
      "+0.469482, -0.470125, 0\n",
      "+0.090287, -0.090645, 0\n",
      "\u001b[32m[06/29 16:59:21 nl.defaults.trainer]: \u001b[0mEpoch 29-9, Train loss: 1.94916, validation loss: 1.98317, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/29 16:59:26 nl.defaults.trainer]: \u001b[0mEpoch 29-38, Train loss: 1.97309, validation loss: 2.00317, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/29 16:59:32 nl.defaults.trainer]: \u001b[0mEpoch 29-67, Train loss: 1.97745, validation loss: 1.97035, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/29 16:59:37 nl.defaults.trainer]: \u001b[0mEpoch 29-96, Train loss: 1.96395, validation loss: 1.92914, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/29 16:59:42 nl.defaults.trainer]: \u001b[0mEpoch 29-125, Train loss: 1.96467, validation loss: 2.00188, learning rate: [0.020354884643835724]\n",
      "\u001b[32m[06/29 16:59:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3355,  0.3355], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4790, -0.4797], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0795, -0.0799], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 16:59:44 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy (top1, top5): 49.90286, 90.74000, Validation accuracy: 49.19594, 90.38264\n",
      "\u001b[32m[06/29 16:59:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.335463, +0.335463, 1\n",
      "+0.479035, -0.479670, 0\n",
      "+0.079535, -0.079891, 0\n",
      "\u001b[32m[06/29 16:59:47 nl.defaults.trainer]: \u001b[0mEpoch 30-17, Train loss: 1.96521, validation loss: 1.99480, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/29 16:59:52 nl.defaults.trainer]: \u001b[0mEpoch 30-46, Train loss: 1.94143, validation loss: 1.98433, learning rate: [0.020053423027509686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 16:59:57 nl.defaults.trainer]: \u001b[0mEpoch 30-76, Train loss: 1.91114, validation loss: 1.97223, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/29 17:00:02 nl.defaults.trainer]: \u001b[0mEpoch 30-106, Train loss: 1.95163, validation loss: 1.97738, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/29 17:00:07 nl.defaults.trainer]: \u001b[0mEpoch 30-135, Train loss: 1.92003, validation loss: 1.98991, learning rate: [0.020053423027509686]\n",
      "\u001b[32m[06/29 17:00:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3446,  0.3446], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4863, -0.4869], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0695, -0.0699], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:00:07 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy (top1, top5): 50.00857, 90.93143, Validation accuracy: 49.46966, 90.58223\n",
      "\u001b[32m[06/29 17:00:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.344589, +0.344589, 1\n",
      "+0.486265, -0.486894, 0\n",
      "+0.069512, -0.069865, 0\n",
      "\u001b[32m[06/29 17:00:12 nl.defaults.trainer]: \u001b[0mEpoch 31-27, Train loss: 1.99608, validation loss: 1.95656, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/29 17:00:17 nl.defaults.trainer]: \u001b[0mEpoch 31-56, Train loss: 1.92484, validation loss: 1.94139, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/29 17:00:22 nl.defaults.trainer]: \u001b[0mEpoch 31-86, Train loss: 1.94647, validation loss: 1.97013, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/29 17:00:28 nl.defaults.trainer]: \u001b[0mEpoch 31-116, Train loss: 1.96231, validation loss: 1.97845, learning rate: [0.019745000534225576]\n",
      "\u001b[32m[06/29 17:00:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3569,  0.3569], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.4969, -0.4975], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0573, -0.0576], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:00:31 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy (top1, top5): 50.64286, 90.99429, Validation accuracy: 49.93442, 91.06980\n",
      "\u001b[32m[06/29 17:00:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.356853, +0.356853, 1\n",
      "+0.496887, -0.497509, 0\n",
      "+0.057255, -0.057606, 0\n",
      "\u001b[32m[06/29 17:00:33 nl.defaults.trainer]: \u001b[0mEpoch 32-9, Train loss: 1.93684, validation loss: 1.96503, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/29 17:00:38 nl.defaults.trainer]: \u001b[0mEpoch 32-39, Train loss: 1.95047, validation loss: 1.93723, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/29 17:00:43 nl.defaults.trainer]: \u001b[0mEpoch 32-69, Train loss: 1.91200, validation loss: 1.99640, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/29 17:00:48 nl.defaults.trainer]: \u001b[0mEpoch 32-99, Train loss: 1.90651, validation loss: 1.96609, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/29 17:00:54 nl.defaults.trainer]: \u001b[0mEpoch 32-129, Train loss: 1.96094, validation loss: 1.94943, learning rate: [0.019429921539747964]\n",
      "\u001b[32m[06/29 17:00:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3657,  0.3657], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5042, -0.5048], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0477, -0.0481], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:00:55 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy (top1, top5): 50.49429, 91.28000, Validation accuracy: 49.77475, 90.73905\n",
      "\u001b[32m[06/29 17:00:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.365745, +0.365745, 1\n",
      "+0.504155, -0.504771, 0\n",
      "+0.047744, -0.048092, 0\n",
      "\u001b[32m[06/29 17:00:59 nl.defaults.trainer]: \u001b[0mEpoch 33-22, Train loss: 1.89264, validation loss: 1.93616, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/29 17:01:04 nl.defaults.trainer]: \u001b[0mEpoch 33-51, Train loss: 1.96067, validation loss: 1.93369, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/29 17:01:09 nl.defaults.trainer]: \u001b[0mEpoch 33-81, Train loss: 1.92417, validation loss: 1.97754, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/29 17:01:14 nl.defaults.trainer]: \u001b[0mEpoch 33-110, Train loss: 1.92674, validation loss: 1.97078, learning rate: [0.01910849698900446]\n",
      "\u001b[32m[06/29 17:01:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3739,  0.3739], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5119, -0.5125], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0399, -0.0403], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:01:18 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy (top1, top5): 51.07714, 91.13429, Validation accuracy: 50.50182, 90.87876\n",
      "\u001b[32m[06/29 17:01:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.373852, +0.373852, 1\n",
      "+0.511904, -0.512513, 0\n",
      "+0.039905, -0.040251, 0\n",
      "\u001b[32m[06/29 17:01:19 nl.defaults.trainer]: \u001b[0mEpoch 34-2, Train loss: 1.92717, validation loss: 1.97608, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/29 17:01:24 nl.defaults.trainer]: \u001b[0mEpoch 34-32, Train loss: 1.92175, validation loss: 1.98623, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/29 17:01:29 nl.defaults.trainer]: \u001b[0mEpoch 34-62, Train loss: 1.95148, validation loss: 1.96825, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/29 17:01:34 nl.defaults.trainer]: \u001b[0mEpoch 34-92, Train loss: 1.95143, validation loss: 1.99430, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/29 17:01:40 nl.defaults.trainer]: \u001b[0mEpoch 34-122, Train loss: 1.90122, validation loss: 1.98532, learning rate: [0.01878104408922059]\n",
      "\u001b[32m[06/29 17:01:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3808,  0.3808], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5176, -0.5182], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0322, -0.0326], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:01:42 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy (top1, top5): 50.76286, 91.05714, Validation accuracy: 49.79186, 90.80463\n",
      "\u001b[32m[06/29 17:01:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.380831, +0.380831, 1\n",
      "+0.517617, -0.518221, 0\n",
      "+0.032245, -0.032589, 0\n",
      "\u001b[32m[06/29 17:01:45 nl.defaults.trainer]: \u001b[0mEpoch 35-15, Train loss: 1.95381, validation loss: 1.94578, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/29 17:01:50 nl.defaults.trainer]: \u001b[0mEpoch 35-45, Train loss: 1.94372, validation loss: 1.95214, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/29 17:01:55 nl.defaults.trainer]: \u001b[0mEpoch 35-75, Train loss: 1.91286, validation loss: 1.90093, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/29 17:02:00 nl.defaults.trainer]: \u001b[0mEpoch 35-105, Train loss: 1.95835, validation loss: 1.89664, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/29 17:02:05 nl.defaults.trainer]: \u001b[0mEpoch 35-135, Train loss: 1.94520, validation loss: 1.93599, learning rate: [0.018447885996874566]\n",
      "\u001b[32m[06/29 17:02:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3913,  0.3913], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5263, -0.5269], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0214, -0.0218], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:02:05 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy (top1, top5): 51.28286, 91.14286, Validation accuracy: 50.75559, 91.32641\n",
      "\u001b[32m[06/29 17:02:05 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.391253, +0.391253, 1\n",
      "+0.526333, -0.526931, 0\n",
      "+0.021436, -0.021778, 0\n",
      "\u001b[32m[06/29 17:02:10 nl.defaults.trainer]: \u001b[0mEpoch 36-28, Train loss: 1.91405, validation loss: 1.94561, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/29 17:02:15 nl.defaults.trainer]: \u001b[0mEpoch 36-58, Train loss: 1.89422, validation loss: 1.92300, learning rate: [0.018109351498780877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:02:21 nl.defaults.trainer]: \u001b[0mEpoch 36-88, Train loss: 1.94927, validation loss: 1.93521, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/29 17:02:26 nl.defaults.trainer]: \u001b[0mEpoch 36-118, Train loss: 1.93874, validation loss: 1.93164, learning rate: [0.018109351498780877]\n",
      "\u001b[32m[06/29 17:02:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.3917,  0.3917], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5270, -0.5276], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0209, -0.0213], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:02:29 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy (top1, top5): 51.47143, 91.50286, Validation accuracy: 50.43910, 91.13823\n",
      "\u001b[32m[06/29 17:02:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.391667, +0.391667, 1\n",
      "+0.527040, -0.527633, 0\n",
      "+0.020941, -0.021281, 0\n",
      "\u001b[32m[06/29 17:02:31 nl.defaults.trainer]: \u001b[0mEpoch 37-10, Train loss: 2.00094, validation loss: 1.97941, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/29 17:02:36 nl.defaults.trainer]: \u001b[0mEpoch 37-40, Train loss: 1.97548, validation loss: 1.99619, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/29 17:02:41 nl.defaults.trainer]: \u001b[0mEpoch 37-70, Train loss: 1.97086, validation loss: 1.98976, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/29 17:02:46 nl.defaults.trainer]: \u001b[0mEpoch 37-100, Train loss: 1.93360, validation loss: 1.93831, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/29 17:02:51 nl.defaults.trainer]: \u001b[0mEpoch 37-130, Train loss: 1.96157, validation loss: 1.94907, learning rate: [0.01776577468761737]\n",
      "\u001b[32m[06/29 17:02:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4036,  0.4036], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5373, -0.5379], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0085, -0.0089], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:02:52 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy (top1, top5): 51.30571, 91.45143, Validation accuracy: 50.59021, 91.11827\n",
      "\u001b[32m[06/29 17:02:52 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.403591, +0.403591, 1\n",
      "+0.537264, -0.537851, 0\n",
      "+0.008539, -0.008878, 0\n",
      "\u001b[32m[06/29 17:02:56 nl.defaults.trainer]: \u001b[0mEpoch 38-22, Train loss: 1.92998, validation loss: 1.93328, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/29 17:03:01 nl.defaults.trainer]: \u001b[0mEpoch 38-52, Train loss: 1.93601, validation loss: 1.93699, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/29 17:03:06 nl.defaults.trainer]: \u001b[0mEpoch 38-82, Train loss: 1.95577, validation loss: 1.98071, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/29 17:03:11 nl.defaults.trainer]: \u001b[0mEpoch 38-112, Train loss: 1.92454, validation loss: 2.00810, learning rate: [0.017417494632216143]\n",
      "\u001b[32m[06/29 17:03:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4135,  0.4135], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5452, -0.5457], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0021,  0.0018], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:03:15 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy (top1, top5): 51.63714, 91.48857, Validation accuracy: 50.88675, 91.57162\n",
      "\u001b[32m[06/29 17:03:15 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.413502, +0.413502, 1\n",
      "+0.545161, -0.545743, 0\n",
      "-0.002144, +0.001808, 1\n",
      "\u001b[32m[06/29 17:03:16 nl.defaults.trainer]: \u001b[0mEpoch 39-4, Train loss: 1.97431, validation loss: 1.98717, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/29 17:03:21 nl.defaults.trainer]: \u001b[0mEpoch 39-34, Train loss: 1.91861, validation loss: 1.99591, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/29 17:03:27 nl.defaults.trainer]: \u001b[0mEpoch 39-64, Train loss: 1.98679, validation loss: 1.91441, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/29 17:03:32 nl.defaults.trainer]: \u001b[0mEpoch 39-94, Train loss: 1.90074, validation loss: 1.88934, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/29 17:03:37 nl.defaults.trainer]: \u001b[0mEpoch 39-124, Train loss: 1.95203, validation loss: 1.93576, learning rate: [0.017064855042943503]\n",
      "\u001b[32m[06/29 17:03:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4202,  0.4202], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5506, -0.5512], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0094,  0.0091], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:03:39 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy (top1, top5): 52.10000, 91.59429, Validation accuracy: 51.14906, 91.40625\n",
      "\u001b[32m[06/29 17:03:39 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.420244, +0.420244, 1\n",
      "+0.550646, -0.551223, 0\n",
      "-0.009387, +0.009052, 1\n",
      "\u001b[32m[06/29 17:03:42 nl.defaults.trainer]: \u001b[0mEpoch 40-16, Train loss: 1.97867, validation loss: 1.96627, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/29 17:03:47 nl.defaults.trainer]: \u001b[0mEpoch 40-46, Train loss: 1.93751, validation loss: 1.93675, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/29 17:03:52 nl.defaults.trainer]: \u001b[0mEpoch 40-76, Train loss: 1.94745, validation loss: 1.98121, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/29 17:03:57 nl.defaults.trainer]: \u001b[0mEpoch 40-106, Train loss: 1.94877, validation loss: 1.97345, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/29 17:04:02 nl.defaults.trainer]: \u001b[0mEpoch 40-136, Train loss: 1.90265, validation loss: 1.95890, learning rate: [0.016708203932499374]\n",
      "\u001b[32m[06/29 17:04:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4333,  0.4333], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5627, -0.5632], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0221,  0.0218], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:04:02 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy (top1, top5): 52.14286, 91.68286, Validation accuracy: 51.07778, 91.27794\n",
      "\u001b[32m[06/29 17:04:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.433308, +0.433308, 1\n",
      "+0.562651, -0.563222, 0\n",
      "-0.022095, +0.021761, 1\n",
      "\u001b[32m[06/29 17:04:07 nl.defaults.trainer]: \u001b[0mEpoch 41-28, Train loss: 1.96116, validation loss: 1.95139, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/29 17:04:12 nl.defaults.trainer]: \u001b[0mEpoch 41-57, Train loss: 1.94541, validation loss: 1.90053, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/29 17:04:17 nl.defaults.trainer]: \u001b[0mEpoch 41-86, Train loss: 1.90064, validation loss: 1.93744, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/29 17:04:22 nl.defaults.trainer]: \u001b[0mEpoch 41-115, Train loss: 1.92550, validation loss: 1.91329, learning rate: [0.016347893272470757]\n",
      "\u001b[32m[06/29 17:04:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4476,  0.4476], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5757, -0.5762], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0360,  0.0357], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:04:26 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy (top1, top5): 52.71143, 91.72571, Validation accuracy: 52.28958, 91.65716\n",
      "\u001b[32m[06/29 17:04:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.447600, +0.447600, 1\n",
      "+0.575653, -0.576220, 0\n",
      "-0.035994, +0.035662, 1\n",
      "\u001b[32m[06/29 17:04:27 nl.defaults.trainer]: \u001b[0mEpoch 42-7, Train loss: 1.95418, validation loss: 1.96472, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/29 17:04:33 nl.defaults.trainer]: \u001b[0mEpoch 42-36, Train loss: 1.88760, validation loss: 1.95278, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/29 17:04:38 nl.defaults.trainer]: \u001b[0mEpoch 42-65, Train loss: 1.91697, validation loss: 1.93835, learning rate: [0.015984278645978258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:04:43 nl.defaults.trainer]: \u001b[0mEpoch 42-94, Train loss: 1.90924, validation loss: 1.95648, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/29 17:04:48 nl.defaults.trainer]: \u001b[0mEpoch 42-123, Train loss: 1.92022, validation loss: 1.91043, learning rate: [0.015984278645978258]\n",
      "\u001b[32m[06/29 17:04:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4542,  0.4542], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5813, -0.5818], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0426,  0.0423], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:04:50 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy (top1, top5): 52.37143, 91.49714, Validation accuracy: 51.89040, 91.19811\n",
      "\u001b[32m[06/29 17:04:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.454154, +0.454154, 1\n",
      "+0.581273, -0.581836, 0\n",
      "-0.042585, +0.042254, 1\n",
      "\u001b[32m[06/29 17:04:53 nl.defaults.trainer]: \u001b[0mEpoch 43-15, Train loss: 1.93023, validation loss: 1.92878, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/29 17:04:58 nl.defaults.trainer]: \u001b[0mEpoch 43-44, Train loss: 1.91616, validation loss: 1.95287, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/29 17:05:03 nl.defaults.trainer]: \u001b[0mEpoch 43-73, Train loss: 1.90730, validation loss: 1.92790, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/29 17:05:08 nl.defaults.trainer]: \u001b[0mEpoch 43-103, Train loss: 1.91972, validation loss: 1.97140, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/29 17:05:13 nl.defaults.trainer]: \u001b[0mEpoch 43-133, Train loss: 1.94392, validation loss: 1.91745, learning rate: [0.015617718896758514]\n",
      "\u001b[32m[06/29 17:05:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4574,  0.4574], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5836, -0.5842], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0462,  0.0459], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:05:14 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy (top1, top5): 52.10000, 91.52571, Validation accuracy: 51.49122, 91.24943\n",
      "\u001b[32m[06/29 17:05:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.457430, +0.457430, 1\n",
      "+0.583629, -0.584187, 0\n",
      "-0.046223, +0.045894, 1\n",
      "\u001b[32m[06/29 17:05:18 nl.defaults.trainer]: \u001b[0mEpoch 44-25, Train loss: 1.92069, validation loss: 1.98256, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/29 17:05:23 nl.defaults.trainer]: \u001b[0mEpoch 44-55, Train loss: 1.90553, validation loss: 1.91191, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/29 17:05:28 nl.defaults.trainer]: \u001b[0mEpoch 44-85, Train loss: 1.91788, validation loss: 1.96174, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/29 17:05:34 nl.defaults.trainer]: \u001b[0mEpoch 44-115, Train loss: 1.93492, validation loss: 1.97641, learning rate: [0.015248575775028698]\n",
      "\u001b[32m[06/29 17:05:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4610,  0.4610], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5867, -0.5873], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0499,  0.0495], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:05:37 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy (top1, top5): 52.58000, 91.67714, Validation accuracy: 51.26312, 91.37774\n",
      "\u001b[32m[06/29 17:05:37 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.461019, +0.461019, 1\n",
      "+0.586727, -0.587280, 0\n",
      "-0.049854, +0.049526, 1\n",
      "\u001b[32m[06/29 17:05:39 nl.defaults.trainer]: \u001b[0mEpoch 45-8, Train loss: 1.94240, validation loss: 1.94880, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/29 17:05:44 nl.defaults.trainer]: \u001b[0mEpoch 45-37, Train loss: 1.88056, validation loss: 1.94578, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/29 17:05:49 nl.defaults.trainer]: \u001b[0mEpoch 45-67, Train loss: 1.93180, validation loss: 1.92986, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/29 17:05:54 nl.defaults.trainer]: \u001b[0mEpoch 45-96, Train loss: 1.93859, validation loss: 1.92514, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/29 17:05:59 nl.defaults.trainer]: \u001b[0mEpoch 45-126, Train loss: 1.96006, validation loss: 1.90435, learning rate: [0.01487721358048277]\n",
      "\u001b[32m[06/29 17:06:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4639,  0.4639], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.5901, -0.5906], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0522,  0.0519], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:06:01 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy (top1, top5): 53.05143, 91.80571, Validation accuracy: 51.96168, 91.24943\n",
      "\u001b[32m[06/29 17:06:01 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.463912, +0.463912, 1\n",
      "+0.590092, -0.590640, 0\n",
      "-0.052182, +0.051855, 1\n",
      "\u001b[32m[06/29 17:06:04 nl.defaults.trainer]: \u001b[0mEpoch 46-18, Train loss: 1.88886, validation loss: 1.93306, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/29 17:06:10 nl.defaults.trainer]: \u001b[0mEpoch 46-48, Train loss: 1.91740, validation loss: 1.89839, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/29 17:06:15 nl.defaults.trainer]: \u001b[0mEpoch 46-78, Train loss: 1.90996, validation loss: 2.00245, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/29 17:06:20 nl.defaults.trainer]: \u001b[0mEpoch 46-108, Train loss: 1.93164, validation loss: 1.92843, learning rate: [0.014503998802771652]\n",
      "\u001b[32m[06/29 17:06:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4755,  0.4755], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6011, -0.6017], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0633,  0.0630], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:06:25 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy (top1, top5): 53.58286, 91.98000, Validation accuracy: 52.76859, 91.81113\n",
      "\u001b[32m[06/29 17:06:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.475511, +0.475511, 1\n",
      "+0.601112, -0.601657, 0\n",
      "-0.063319, +0.062994, 1\n",
      "\u001b[32m[06/29 17:06:25 nl.defaults.trainer]: \u001b[0mEpoch 47-0, Train loss: 1.91489, validation loss: 1.91679, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/29 17:06:30 nl.defaults.trainer]: \u001b[0mEpoch 47-29, Train loss: 1.93134, validation loss: 1.91942, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/29 17:06:35 nl.defaults.trainer]: \u001b[0mEpoch 47-59, Train loss: 1.92836, validation loss: 1.95505, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/29 17:06:40 nl.defaults.trainer]: \u001b[0mEpoch 47-89, Train loss: 1.94510, validation loss: 1.89659, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/29 17:06:45 nl.defaults.trainer]: \u001b[0mEpoch 47-119, Train loss: 1.94601, validation loss: 1.91907, learning rate: [0.014129299759822168]\n",
      "\u001b[32m[06/29 17:06:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4762,  0.4762], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6024, -0.6029], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0634,  0.0631], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:06:48 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy (top1, top5): 53.36000, 91.82000, Validation accuracy: 52.54904, 91.42051\n",
      "\u001b[32m[06/29 17:06:48 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.476215, +0.476215, 1\n",
      "+0.602392, -0.602933, 0\n",
      "-0.063420, +0.063096, 1\n",
      "\u001b[32m[06/29 17:06:50 nl.defaults.trainer]: \u001b[0mEpoch 48-11, Train loss: 1.97189, validation loss: 1.90258, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/29 17:06:56 nl.defaults.trainer]: \u001b[0mEpoch 48-41, Train loss: 1.92577, validation loss: 1.92901, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/29 17:07:01 nl.defaults.trainer]: \u001b[0mEpoch 48-70, Train loss: 1.89507, validation loss: 1.94760, learning rate: [0.013753486234351759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:07:06 nl.defaults.trainer]: \u001b[0mEpoch 48-100, Train loss: 1.90583, validation loss: 1.92787, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/29 17:07:11 nl.defaults.trainer]: \u001b[0mEpoch 48-129, Train loss: 1.93382, validation loss: 1.95207, learning rate: [0.013753486234351759]\n",
      "\u001b[32m[06/29 17:07:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4809,  0.4809], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6069, -0.6075], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0678,  0.0675], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:07:12 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy (top1, top5): 53.12571, 91.96857, Validation accuracy: 52.88549, 91.71704\n",
      "\u001b[32m[06/29 17:07:12 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.480936, +0.480936, 1\n",
      "+0.606941, -0.607478, 0\n",
      "-0.067825, +0.067503, 1\n",
      "\u001b[32m[06/29 17:07:16 nl.defaults.trainer]: \u001b[0mEpoch 49-22, Train loss: 1.95697, validation loss: 1.89956, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/29 17:07:21 nl.defaults.trainer]: \u001b[0mEpoch 49-52, Train loss: 1.92880, validation loss: 1.95350, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/29 17:07:26 nl.defaults.trainer]: \u001b[0mEpoch 49-82, Train loss: 1.92920, validation loss: 1.98618, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/29 17:07:31 nl.defaults.trainer]: \u001b[0mEpoch 49-112, Train loss: 1.91595, validation loss: 1.91529, learning rate: [0.013376929108937535]\n",
      "\u001b[32m[06/29 17:07:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4856,  0.4856], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6118, -0.6123], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0719,  0.0716], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:07:35 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy (top1, top5): 53.86571, 92.20000, Validation accuracy: 52.70586, 91.87386\n",
      "\u001b[32m[06/29 17:07:35 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.485629, +0.485629, 1\n",
      "+0.611807, -0.612340, 0\n",
      "-0.071946, +0.071624, 1\n",
      "\u001b[32m[06/29 17:07:36 nl.defaults.trainer]: \u001b[0mEpoch 50-4, Train loss: 1.93684, validation loss: 1.87972, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/29 17:07:41 nl.defaults.trainer]: \u001b[0mEpoch 50-33, Train loss: 1.92161, validation loss: 1.90070, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/29 17:07:46 nl.defaults.trainer]: \u001b[0mEpoch 50-62, Train loss: 1.93461, validation loss: 1.91695, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/29 17:07:51 nl.defaults.trainer]: \u001b[0mEpoch 50-91, Train loss: 1.96220, validation loss: 1.91689, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/29 17:07:57 nl.defaults.trainer]: \u001b[0mEpoch 50-121, Train loss: 1.90091, validation loss: 1.88704, learning rate: [0.012999999999999994]\n",
      "\u001b[32m[06/29 17:07:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4900,  0.4900], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6177, -0.6182], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0748,  0.0745], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:07:59 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy (top1, top5): 53.68571, 92.21714, Validation accuracy: 53.14496, 91.79688\n",
      "\u001b[32m[06/29 17:07:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.489955, +0.489954, 1\n",
      "+0.617670, -0.618199, 0\n",
      "-0.074842, +0.074522, 1\n",
      "\u001b[32m[06/29 17:08:02 nl.defaults.trainer]: \u001b[0mEpoch 51-13, Train loss: 1.88077, validation loss: 1.84827, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/29 17:08:07 nl.defaults.trainer]: \u001b[0mEpoch 51-43, Train loss: 1.88086, validation loss: 2.00906, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/29 17:08:12 nl.defaults.trainer]: \u001b[0mEpoch 51-73, Train loss: 1.92303, validation loss: 1.93384, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/29 17:08:17 nl.defaults.trainer]: \u001b[0mEpoch 51-102, Train loss: 1.89424, validation loss: 1.90517, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/29 17:08:22 nl.defaults.trainer]: \u001b[0mEpoch 51-131, Train loss: 1.97249, validation loss: 1.91686, learning rate: [0.012623070891062453]\n",
      "\u001b[32m[06/29 17:08:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.4957,  0.4957], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6249, -0.6254], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0791,  0.0788], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:08:23 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy (top1, top5): 54.00571, 92.12571, Validation accuracy: 53.48141, 91.77977\n",
      "\u001b[32m[06/29 17:08:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.495716, +0.495716, 1\n",
      "+0.624868, -0.625393, 0\n",
      "-0.079115, +0.078796, 1\n",
      "\u001b[32m[06/29 17:08:27 nl.defaults.trainer]: \u001b[0mEpoch 52-23, Train loss: 1.88581, validation loss: 1.89624, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/29 17:08:32 nl.defaults.trainer]: \u001b[0mEpoch 52-53, Train loss: 1.89183, validation loss: 1.97245, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/29 17:08:37 nl.defaults.trainer]: \u001b[0mEpoch 52-83, Train loss: 1.91687, validation loss: 1.90872, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/29 17:08:43 nl.defaults.trainer]: \u001b[0mEpoch 52-113, Train loss: 1.92093, validation loss: 1.86610, learning rate: [0.012246513765648233]\n",
      "\u001b[32m[06/29 17:08:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5079,  0.5079], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6366, -0.6371], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0906,  0.0903], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:08:47 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy (top1, top5): 54.08571, 92.41429, Validation accuracy: 53.47856, 92.04208\n",
      "\u001b[32m[06/29 17:08:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.507877, +0.507877, 1\n",
      "+0.636616, -0.637136, 0\n",
      "-0.090621, +0.090303, 1\n",
      "\u001b[32m[06/29 17:08:48 nl.defaults.trainer]: \u001b[0mEpoch 53-5, Train loss: 1.98895, validation loss: 1.93356, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/29 17:08:53 nl.defaults.trainer]: \u001b[0mEpoch 53-35, Train loss: 1.91182, validation loss: 1.89484, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/29 17:08:58 nl.defaults.trainer]: \u001b[0mEpoch 53-65, Train loss: 1.90671, validation loss: 1.93572, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/29 17:09:03 nl.defaults.trainer]: \u001b[0mEpoch 53-95, Train loss: 1.92008, validation loss: 1.96781, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/29 17:09:08 nl.defaults.trainer]: \u001b[0mEpoch 53-124, Train loss: 1.91071, validation loss: 1.92935, learning rate: [0.01187070024017782]\n",
      "\u001b[32m[06/29 17:09:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5142,  0.5142], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6435, -0.6440], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0958,  0.0955], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:09:10 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy (top1, top5): 54.22000, 92.20286, Validation accuracy: 53.74943, 91.97365\n",
      "\u001b[32m[06/29 17:09:10 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.514154, +0.514154, 1\n",
      "+0.643507, -0.644023, 0\n",
      "-0.095839, +0.095522, 1\n",
      "\u001b[32m[06/29 17:09:13 nl.defaults.trainer]: \u001b[0mEpoch 54-16, Train loss: 1.90689, validation loss: 1.90903, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/29 17:09:18 nl.defaults.trainer]: \u001b[0mEpoch 54-46, Train loss: 1.96138, validation loss: 1.88967, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/29 17:09:23 nl.defaults.trainer]: \u001b[0mEpoch 54-76, Train loss: 1.92324, validation loss: 1.89987, learning rate: [0.01149600119722834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:09:28 nl.defaults.trainer]: \u001b[0mEpoch 54-106, Train loss: 1.92014, validation loss: 1.90585, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/29 17:09:33 nl.defaults.trainer]: \u001b[0mEpoch 54-136, Train loss: 1.89447, validation loss: 1.94558, learning rate: [0.01149600119722834]\n",
      "\u001b[32m[06/29 17:09:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5176,  0.5176], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6484, -0.6489], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0978,  0.0975], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:09:33 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy (top1, top5): 54.05714, 92.20000, Validation accuracy: 54.19708, 92.04208\n",
      "\u001b[32m[06/29 17:09:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.517612, +0.517612, 1\n",
      "+0.648423, -0.648935, 0\n",
      "-0.097802, +0.097486, 1\n",
      "\u001b[32m[06/29 17:09:39 nl.defaults.trainer]: \u001b[0mEpoch 55-29, Train loss: 1.93754, validation loss: 1.94454, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/29 17:09:44 nl.defaults.trainer]: \u001b[0mEpoch 55-59, Train loss: 1.90638, validation loss: 1.87810, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/29 17:09:49 nl.defaults.trainer]: \u001b[0mEpoch 55-89, Train loss: 1.93636, validation loss: 1.94015, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/29 17:09:54 nl.defaults.trainer]: \u001b[0mEpoch 55-119, Train loss: 1.92274, validation loss: 1.88378, learning rate: [0.011122786419517219]\n",
      "\u001b[32m[06/29 17:09:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5315,  0.5315], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6627, -0.6632], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1104,  0.1100], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:09:57 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy (top1, top5): 54.42286, 92.55143, Validation accuracy: 53.90340, 92.03923\n",
      "\u001b[32m[06/29 17:09:57 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.531491, +0.531491, 1\n",
      "+0.662675, -0.663183, 0\n",
      "-0.110359, +0.110045, 1\n",
      "\u001b[32m[06/29 17:09:59 nl.defaults.trainer]: \u001b[0mEpoch 56-11, Train loss: 1.86638, validation loss: 1.90395, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/29 17:10:04 nl.defaults.trainer]: \u001b[0mEpoch 56-41, Train loss: 1.90410, validation loss: 1.95155, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/29 17:10:09 nl.defaults.trainer]: \u001b[0mEpoch 56-71, Train loss: 1.88902, validation loss: 1.94024, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/29 17:10:14 nl.defaults.trainer]: \u001b[0mEpoch 56-101, Train loss: 1.88774, validation loss: 1.94612, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/29 17:10:19 nl.defaults.trainer]: \u001b[0mEpoch 56-131, Train loss: 1.93024, validation loss: 1.93798, learning rate: [0.010751424224971294]\n",
      "\u001b[32m[06/29 17:10:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5378,  0.5378], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6702, -0.6707], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1152,  0.1149], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:10:20 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy (top1, top5): 54.60286, 92.44286, Validation accuracy: 54.13720, 91.91663\n",
      "\u001b[32m[06/29 17:10:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.537834, +0.537834, 1\n",
      "+0.670158, -0.670662, 0\n",
      "-0.115193, +0.114879, 1\n",
      "\u001b[32m[06/29 17:10:24 nl.defaults.trainer]: \u001b[0mEpoch 57-23, Train loss: 1.91407, validation loss: 1.93803, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/29 17:10:30 nl.defaults.trainer]: \u001b[0mEpoch 57-53, Train loss: 1.90078, validation loss: 1.95038, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/29 17:10:35 nl.defaults.trainer]: \u001b[0mEpoch 57-82, Train loss: 1.93882, validation loss: 1.94570, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/29 17:10:40 nl.defaults.trainer]: \u001b[0mEpoch 57-112, Train loss: 1.92595, validation loss: 1.95247, learning rate: [0.010382281103241481]\n",
      "\u001b[32m[06/29 17:10:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5454,  0.5454], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6783, -0.6788], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1216,  0.1213], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:10:44 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy (top1, top5): 54.58571, 92.28571, Validation accuracy: 53.64108, 91.95655\n",
      "\u001b[32m[06/29 17:10:44 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.545445, +0.545445, 1\n",
      "+0.678279, -0.678780, 0\n",
      "-0.121643, +0.121330, 1\n",
      "\u001b[32m[06/29 17:10:45 nl.defaults.trainer]: \u001b[0mEpoch 58-4, Train loss: 1.91688, validation loss: 1.92029, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/29 17:10:50 nl.defaults.trainer]: \u001b[0mEpoch 58-34, Train loss: 1.90816, validation loss: 1.89305, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/29 17:10:55 nl.defaults.trainer]: \u001b[0mEpoch 58-63, Train loss: 1.92152, validation loss: 1.88772, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/29 17:11:00 nl.defaults.trainer]: \u001b[0mEpoch 58-93, Train loss: 1.94914, validation loss: 1.90563, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/29 17:11:05 nl.defaults.trainer]: \u001b[0mEpoch 58-122, Train loss: 1.87609, validation loss: 1.94649, learning rate: [0.01001572135402173]\n",
      "\u001b[32m[06/29 17:11:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5507,  0.5507], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6826, -0.6831], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1268,  0.1265], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:11:07 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy (top1, top5): 54.41714, 92.10571, Validation accuracy: 53.15922, 91.75696\n",
      "\u001b[32m[06/29 17:11:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.550749, +0.550749, 1\n",
      "+0.682642, -0.683139, 0\n",
      "-0.126816, +0.126505, 1\n",
      "\u001b[32m[06/29 17:11:10 nl.defaults.trainer]: \u001b[0mEpoch 59-14, Train loss: 1.89364, validation loss: 1.87511, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/29 17:11:15 nl.defaults.trainer]: \u001b[0mEpoch 59-43, Train loss: 1.87758, validation loss: 1.91364, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/29 17:11:20 nl.defaults.trainer]: \u001b[0mEpoch 59-73, Train loss: 1.90231, validation loss: 1.89215, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/29 17:11:25 nl.defaults.trainer]: \u001b[0mEpoch 59-103, Train loss: 1.94234, validation loss: 1.92147, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/29 17:11:31 nl.defaults.trainer]: \u001b[0mEpoch 59-133, Train loss: 1.95667, validation loss: 1.87348, learning rate: [0.009652106727529239]\n",
      "\u001b[32m[06/29 17:11:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5566,  0.5566], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6884, -0.6889], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1321,  0.1318], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:11:31 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy (top1, top5): 55.13143, 92.40857, Validation accuracy: 53.39587, 91.89097\n",
      "\u001b[32m[06/29 17:11:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.556646, +0.556646, 1\n",
      "+0.688358, -0.688851, 0\n",
      "-0.132087, +0.131776, 1\n",
      "\u001b[32m[06/29 17:11:36 nl.defaults.trainer]: \u001b[0mEpoch 60-25, Train loss: 1.90861, validation loss: 1.93728, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/29 17:11:41 nl.defaults.trainer]: \u001b[0mEpoch 60-55, Train loss: 1.88879, validation loss: 1.93888, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/29 17:11:46 nl.defaults.trainer]: \u001b[0mEpoch 60-85, Train loss: 1.98403, validation loss: 1.91922, learning rate: [0.009291796067500625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:11:51 nl.defaults.trainer]: \u001b[0mEpoch 60-115, Train loss: 1.89199, validation loss: 1.93643, learning rate: [0.009291796067500625]\n",
      "\u001b[32m[06/29 17:11:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5616,  0.5616], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6944, -0.6949], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1356,  0.1353], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:11:55 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy (top1, top5): 55.21429, 92.39714, Validation accuracy: 54.33109, 92.12192\n",
      "\u001b[32m[06/29 17:11:55 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.561574, +0.561574, 1\n",
      "+0.694419, -0.694908, 0\n",
      "-0.135572, +0.135263, 1\n",
      "\u001b[32m[06/29 17:11:56 nl.defaults.trainer]: \u001b[0mEpoch 61-8, Train loss: 1.92079, validation loss: 1.90504, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/29 17:12:01 nl.defaults.trainer]: \u001b[0mEpoch 61-37, Train loss: 1.95817, validation loss: 1.87320, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/29 17:12:07 nl.defaults.trainer]: \u001b[0mEpoch 61-67, Train loss: 2.00725, validation loss: 1.89454, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/29 17:12:12 nl.defaults.trainer]: \u001b[0mEpoch 61-96, Train loss: 1.90258, validation loss: 1.87756, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/29 17:12:17 nl.defaults.trainer]: \u001b[0mEpoch 61-126, Train loss: 1.92554, validation loss: 1.91198, learning rate: [0.008935144957056492]\n",
      "\u001b[32m[06/29 17:12:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5656,  0.5656], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6995, -0.6999], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1383,  0.1380], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:12:18 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy (top1, top5): 55.44000, 92.38571, Validation accuracy: 54.20563, 92.20461\n",
      "\u001b[32m[06/29 17:12:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.565644, +0.565644, 1\n",
      "+0.699458, -0.699943, 0\n",
      "-0.138346, +0.138037, 1\n",
      "\u001b[32m[06/29 17:12:22 nl.defaults.trainer]: \u001b[0mEpoch 62-18, Train loss: 1.91456, validation loss: 1.94253, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/29 17:12:27 nl.defaults.trainer]: \u001b[0mEpoch 62-48, Train loss: 1.95441, validation loss: 1.90447, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/29 17:12:32 nl.defaults.trainer]: \u001b[0mEpoch 62-78, Train loss: 1.95549, validation loss: 1.92730, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/29 17:12:37 nl.defaults.trainer]: \u001b[0mEpoch 62-108, Train loss: 1.91054, validation loss: 1.91661, learning rate: [0.008582505367783856]\n",
      "\u001b[32m[06/29 17:12:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5770,  0.5770], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7115, -0.7120], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1483,  0.1480], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:12:42 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy (top1, top5): 55.22286, 92.29714, Validation accuracy: 54.40522, 92.22172\n",
      "\u001b[32m[06/29 17:12:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.577032, +0.577032, 1\n",
      "+0.711493, -0.711974, 0\n",
      "-0.148337, +0.148030, 1\n",
      "\u001b[32m[06/29 17:12:42 nl.defaults.trainer]: \u001b[0mEpoch 63-1, Train loss: 1.88051, validation loss: 1.95558, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/29 17:12:47 nl.defaults.trainer]: \u001b[0mEpoch 63-30, Train loss: 1.88748, validation loss: 1.94104, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/29 17:12:53 nl.defaults.trainer]: \u001b[0mEpoch 63-60, Train loss: 1.88786, validation loss: 1.84861, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/29 17:12:58 nl.defaults.trainer]: \u001b[0mEpoch 63-89, Train loss: 1.91061, validation loss: 1.95568, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/29 17:13:03 nl.defaults.trainer]: \u001b[0mEpoch 63-119, Train loss: 1.95724, validation loss: 1.91670, learning rate: [0.008234225312382621]\n",
      "\u001b[32m[06/29 17:13:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5825,  0.5825], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7174, -0.7179], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1527,  0.1524], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:13:06 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy (top1, top5): 55.61429, 92.53714, Validation accuracy: 54.62477, 92.35287\n",
      "\u001b[32m[06/29 17:13:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.582517, +0.582517, 1\n",
      "+0.717423, -0.717899, 0\n",
      "-0.152741, +0.152435, 1\n",
      "\u001b[32m[06/29 17:13:08 nl.defaults.trainer]: \u001b[0mEpoch 64-11, Train loss: 1.87064, validation loss: 1.94874, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/29 17:13:13 nl.defaults.trainer]: \u001b[0mEpoch 64-41, Train loss: 1.88686, validation loss: 1.89094, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/29 17:13:18 nl.defaults.trainer]: \u001b[0mEpoch 64-70, Train loss: 1.90475, validation loss: 1.87981, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/29 17:13:23 nl.defaults.trainer]: \u001b[0mEpoch 64-100, Train loss: 1.93290, validation loss: 1.92301, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/29 17:13:28 nl.defaults.trainer]: \u001b[0mEpoch 64-129, Train loss: 1.90397, validation loss: 1.87847, learning rate: [0.007890648501219118]\n",
      "\u001b[32m[06/29 17:13:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.5953,  0.5953], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7299, -0.7304], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1647,  0.1644], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:13:29 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy (top1, top5): 55.90857, 92.51143, Validation accuracy: 54.61622, 92.26734\n",
      "\u001b[32m[06/29 17:13:29 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.595341, +0.595341, 1\n",
      "+0.729885, -0.730357, 0\n",
      "-0.164680, +0.164375, 1\n",
      "\u001b[32m[06/29 17:13:33 nl.defaults.trainer]: \u001b[0mEpoch 65-22, Train loss: 1.88781, validation loss: 1.92276, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/29 17:13:38 nl.defaults.trainer]: \u001b[0mEpoch 65-52, Train loss: 1.91912, validation loss: 1.87882, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/29 17:13:44 nl.defaults.trainer]: \u001b[0mEpoch 65-82, Train loss: 1.87634, validation loss: 1.90508, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/29 17:13:49 nl.defaults.trainer]: \u001b[0mEpoch 65-112, Train loss: 1.88861, validation loss: 1.92399, learning rate: [0.00755211400312543]\n",
      "\u001b[32m[06/29 17:13:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6009,  0.6009], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7364, -0.7368], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1688,  0.1685], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:13:53 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy (top1, top5): 55.64000, 92.61429, Validation accuracy: 54.85573, 92.33006\n",
      "\u001b[32m[06/29 17:13:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.600915, +0.600915, 1\n",
      "+0.736378, -0.736846, 0\n",
      "-0.168805, +0.168501, 1\n",
      "\u001b[32m[06/29 17:13:54 nl.defaults.trainer]: \u001b[0mEpoch 66-4, Train loss: 1.88248, validation loss: 1.92975, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/29 17:13:59 nl.defaults.trainer]: \u001b[0mEpoch 66-34, Train loss: 1.89834, validation loss: 1.89426, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/29 17:14:04 nl.defaults.trainer]: \u001b[0mEpoch 66-63, Train loss: 1.98053, validation loss: 1.85561, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/29 17:14:09 nl.defaults.trainer]: \u001b[0mEpoch 66-93, Train loss: 1.91459, validation loss: 1.93315, learning rate: [0.007218955910779407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:14:14 nl.defaults.trainer]: \u001b[0mEpoch 66-122, Train loss: 1.97629, validation loss: 1.92971, learning rate: [0.007218955910779407]\n",
      "\u001b[32m[06/29 17:14:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6100,  0.6100], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7464, -0.7469], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1764,  0.1761], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:14:16 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy (top1, top5): 55.84000, 92.67429, Validation accuracy: 55.18932, 92.27019\n",
      "\u001b[32m[06/29 17:14:16 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.610038, +0.610038, 1\n",
      "+0.746398, -0.746862, 0\n",
      "-0.176374, +0.176071, 1\n",
      "\u001b[32m[06/29 17:14:19 nl.defaults.trainer]: \u001b[0mEpoch 67-15, Train loss: 1.93341, validation loss: 1.89031, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/29 17:14:24 nl.defaults.trainer]: \u001b[0mEpoch 67-45, Train loss: 1.88267, validation loss: 1.94365, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/29 17:14:29 nl.defaults.trainer]: \u001b[0mEpoch 67-75, Train loss: 1.86900, validation loss: 1.90306, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/29 17:14:34 nl.defaults.trainer]: \u001b[0mEpoch 67-105, Train loss: 1.88094, validation loss: 1.90158, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/29 17:14:40 nl.defaults.trainer]: \u001b[0mEpoch 67-135, Train loss: 2.00095, validation loss: 1.88643, learning rate: [0.006891503010995536]\n",
      "\u001b[32m[06/29 17:14:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6080,  0.6080], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7461, -0.7466], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1726,  0.1723], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:14:40 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy (top1, top5): 55.74857, 92.55714, Validation accuracy: 55.08953, 92.65796\n",
      "\u001b[32m[06/29 17:14:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.607983, +0.607983, 1\n",
      "+0.746114, -0.746574, 0\n",
      "-0.172554, +0.172252, 1\n",
      "\u001b[32m[06/29 17:14:45 nl.defaults.trainer]: \u001b[0mEpoch 68-28, Train loss: 1.88662, validation loss: 1.88596, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/29 17:14:50 nl.defaults.trainer]: \u001b[0mEpoch 68-57, Train loss: 1.91964, validation loss: 1.93278, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/29 17:14:55 nl.defaults.trainer]: \u001b[0mEpoch 68-87, Train loss: 1.85704, validation loss: 1.93847, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/29 17:15:00 nl.defaults.trainer]: \u001b[0mEpoch 68-116, Train loss: 1.85551, validation loss: 1.90860, learning rate: [0.00657007846025203]\n",
      "\u001b[32m[06/29 17:15:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6147,  0.6147], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7541, -0.7546], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1775,  0.1772], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:15:03 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy (top1, top5): 56.04000, 92.54571, Validation accuracy: 55.22354, 92.33292\n",
      "\u001b[32m[06/29 17:15:03 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.614696, +0.614696, 1\n",
      "+0.754106, -0.754563, 0\n",
      "-0.177535, +0.177234, 1\n",
      "\u001b[32m[06/29 17:15:05 nl.defaults.trainer]: \u001b[0mEpoch 69-9, Train loss: 1.91106, validation loss: 1.95230, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/29 17:15:10 nl.defaults.trainer]: \u001b[0mEpoch 69-39, Train loss: 1.94295, validation loss: 1.92815, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/29 17:15:15 nl.defaults.trainer]: \u001b[0mEpoch 69-69, Train loss: 1.90776, validation loss: 1.91397, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/29 17:15:20 nl.defaults.trainer]: \u001b[0mEpoch 69-99, Train loss: 1.87666, validation loss: 1.93899, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/29 17:15:26 nl.defaults.trainer]: \u001b[0mEpoch 69-129, Train loss: 1.91193, validation loss: 1.91988, learning rate: [0.006254999465774425]\n",
      "\u001b[32m[06/29 17:15:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6182,  0.6182], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7582, -0.7587], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1797,  0.1794], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:15:27 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy (top1, top5): 56.22000, 92.50286, Validation accuracy: 54.93271, 92.40420\n",
      "\u001b[32m[06/29 17:15:27 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.618153, +0.618153, 1\n",
      "+0.758234, -0.758687, 0\n",
      "-0.179684, +0.179384, 1\n",
      "\u001b[32m[06/29 17:15:31 nl.defaults.trainer]: \u001b[0mEpoch 70-22, Train loss: 1.92965, validation loss: 1.91076, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/29 17:15:36 nl.defaults.trainer]: \u001b[0mEpoch 70-52, Train loss: 1.90621, validation loss: 1.90590, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/29 17:15:41 nl.defaults.trainer]: \u001b[0mEpoch 70-82, Train loss: 1.92730, validation loss: 1.94617, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/29 17:15:46 nl.defaults.trainer]: \u001b[0mEpoch 70-112, Train loss: 1.89237, validation loss: 1.90846, learning rate: [0.005946576972490318]\n",
      "\u001b[32m[06/29 17:15:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6236,  0.6236], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7652, -0.7657], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1832,  0.1829], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:15:50 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy (top1, top5): 56.03429, 92.90286, Validation accuracy: 55.31193, 92.46407\n",
      "\u001b[32m[06/29 17:15:50 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.623589, +0.623589, 1\n",
      "+0.765232, -0.765681, 0\n",
      "-0.183243, +0.182944, 1\n",
      "\u001b[32m[06/29 17:15:51 nl.defaults.trainer]: \u001b[0mEpoch 71-4, Train loss: 1.89475, validation loss: 1.87990, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/29 17:15:56 nl.defaults.trainer]: \u001b[0mEpoch 71-33, Train loss: 1.89911, validation loss: 1.92685, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/29 17:16:01 nl.defaults.trainer]: \u001b[0mEpoch 71-62, Train loss: 1.91033, validation loss: 1.87731, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/29 17:16:06 nl.defaults.trainer]: \u001b[0mEpoch 71-91, Train loss: 1.82172, validation loss: 1.94550, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/29 17:16:11 nl.defaults.trainer]: \u001b[0mEpoch 71-120, Train loss: 1.84469, validation loss: 1.86854, learning rate: [0.005645115356164275]\n",
      "\u001b[32m[06/29 17:16:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6279,  0.6279], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7702, -0.7706], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1863,  0.1860], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:16:14 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy (top1, top5): 56.47714, 92.81429, Validation accuracy: 55.18932, 92.33862\n",
      "\u001b[32m[06/29 17:16:14 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.627915, +0.627915, 1\n",
      "+0.770161, -0.770605, 0\n",
      "-0.186257, +0.185959, 1\n",
      "\u001b[32m[06/29 17:16:17 nl.defaults.trainer]: \u001b[0mEpoch 72-12, Train loss: 1.90554, validation loss: 1.88685, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/29 17:16:22 nl.defaults.trainer]: \u001b[0mEpoch 72-42, Train loss: 1.85918, validation loss: 1.89707, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/29 17:16:27 nl.defaults.trainer]: \u001b[0mEpoch 72-72, Train loss: 1.87769, validation loss: 1.92610, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/29 17:16:32 nl.defaults.trainer]: \u001b[0mEpoch 72-102, Train loss: 1.83327, validation loss: 1.91938, learning rate: [0.005350912123015718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:16:37 nl.defaults.trainer]: \u001b[0mEpoch 72-132, Train loss: 1.89048, validation loss: 1.90976, learning rate: [0.005350912123015718]\n",
      "\u001b[32m[06/29 17:16:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6309,  0.6309], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7754, -0.7759], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1870,  0.1867], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:16:38 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy (top1, top5): 56.28286, 92.53714, Validation accuracy: 55.53718, 92.40990\n",
      "\u001b[32m[06/29 17:16:38 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.630901, +0.630901, 1\n",
      "+0.775411, -0.775853, 0\n",
      "-0.187028, +0.186730, 1\n",
      "\u001b[32m[06/29 17:16:42 nl.defaults.trainer]: \u001b[0mEpoch 73-24, Train loss: 1.96042, validation loss: 1.94817, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/29 17:16:47 nl.defaults.trainer]: \u001b[0mEpoch 73-53, Train loss: 1.86209, validation loss: 1.91068, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/29 17:16:52 nl.defaults.trainer]: \u001b[0mEpoch 73-82, Train loss: 1.88554, validation loss: 1.96620, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/29 17:16:58 nl.defaults.trainer]: \u001b[0mEpoch 73-112, Train loss: 1.89983, validation loss: 1.90164, learning rate: [0.0050642576161161745]\n",
      "\u001b[32m[06/29 17:17:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6286,  0.6286], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7752, -0.7756], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1827,  0.1824], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:17:02 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy (top1, top5): 56.65429, 92.93714, Validation accuracy: 55.24920, 92.55246\n",
      "\u001b[32m[06/29 17:17:02 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.628587, +0.628587, 1\n",
      "+0.775185, -0.775622, 0\n",
      "-0.182689, +0.182393, 1\n",
      "\u001b[32m[06/29 17:17:03 nl.defaults.trainer]: \u001b[0mEpoch 74-4, Train loss: 1.88525, validation loss: 1.95743, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/29 17:17:08 nl.defaults.trainer]: \u001b[0mEpoch 74-34, Train loss: 1.87220, validation loss: 1.93201, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/29 17:17:13 nl.defaults.trainer]: \u001b[0mEpoch 74-63, Train loss: 1.89607, validation loss: 1.94053, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/29 17:17:18 nl.defaults.trainer]: \u001b[0mEpoch 74-93, Train loss: 1.93100, validation loss: 1.88478, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/29 17:17:23 nl.defaults.trainer]: \u001b[0mEpoch 74-122, Train loss: 1.89815, validation loss: 1.89902, learning rate: [0.004785434728855731]\n",
      "\u001b[32m[06/29 17:17:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6280,  0.6280], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7766, -0.7770], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1800,  0.1797], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:17:25 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy (top1, top5): 56.62571, 92.87429, Validation accuracy: 55.58850, 92.62945\n",
      "\u001b[32m[06/29 17:17:25 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.627965, +0.627965, 1\n",
      "+0.776607, -0.777041, 0\n",
      "-0.180023, +0.179728, 1\n",
      "\u001b[32m[06/29 17:17:28 nl.defaults.trainer]: \u001b[0mEpoch 75-15, Train loss: 1.89112, validation loss: 1.87330, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/29 17:17:33 nl.defaults.trainer]: \u001b[0mEpoch 75-45, Train loss: 1.85646, validation loss: 1.91503, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/29 17:17:39 nl.defaults.trainer]: \u001b[0mEpoch 75-75, Train loss: 1.91703, validation loss: 1.92111, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/29 17:17:44 nl.defaults.trainer]: \u001b[0mEpoch 75-105, Train loss: 1.85378, validation loss: 1.89743, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/29 17:17:49 nl.defaults.trainer]: \u001b[0mEpoch 75-135, Train loss: 1.92896, validation loss: 1.89953, learning rate: [0.004514718625761426]\n",
      "\u001b[32m[06/29 17:17:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6250,  0.6250], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7764, -0.7768], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1746,  0.1743], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:17:49 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy (top1, top5): 56.80000, 92.84857, Validation accuracy: 55.51437, 92.60379\n",
      "\u001b[32m[06/29 17:17:49 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.624957, +0.624957, 1\n",
      "+0.776399, -0.776829, 0\n",
      "-0.174616, +0.174321, 1\n",
      "\u001b[32m[06/29 17:17:54 nl.defaults.trainer]: \u001b[0mEpoch 76-28, Train loss: 1.91366, validation loss: 1.87847, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/29 17:17:59 nl.defaults.trainer]: \u001b[0mEpoch 76-57, Train loss: 1.89958, validation loss: 1.91028, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/29 17:18:04 nl.defaults.trainer]: \u001b[0mEpoch 76-87, Train loss: 1.89199, validation loss: 1.91343, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/29 17:18:09 nl.defaults.trainer]: \u001b[0mEpoch 76-116, Train loss: 1.89402, validation loss: 1.91709, learning rate: [0.00425237647094306]\n",
      "\u001b[32m[06/29 17:18:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6246,  0.6246], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7782, -0.7786], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1722,  0.1719], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:18:13 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy (top1, top5): 56.58000, 92.75714, Validation accuracy: 55.50011, 92.31581\n",
      "\u001b[32m[06/29 17:18:13 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.624624, +0.624624, 1\n",
      "+0.778170, -0.778596, 0\n",
      "-0.172231, +0.171937, 1\n",
      "\u001b[32m[06/29 17:18:14 nl.defaults.trainer]: \u001b[0mEpoch 77-9, Train loss: 1.87276, validation loss: 1.91878, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/29 17:18:19 nl.defaults.trainer]: \u001b[0mEpoch 77-39, Train loss: 1.85917, validation loss: 1.94190, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/29 17:18:25 nl.defaults.trainer]: \u001b[0mEpoch 77-69, Train loss: 1.89034, validation loss: 1.88200, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/29 17:18:30 nl.defaults.trainer]: \u001b[0mEpoch 77-99, Train loss: 1.93205, validation loss: 1.94111, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/29 17:18:35 nl.defaults.trainer]: \u001b[0mEpoch 77-129, Train loss: 1.84371, validation loss: 1.91482, learning rate: [0.003998667164434481]\n",
      "\u001b[32m[06/29 17:18:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6270,  0.6270], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7832, -0.7836], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1723,  0.1720], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:18:36 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy (top1, top5): 56.94286, 92.72286, Validation accuracy: 55.56854, 92.41275\n",
      "\u001b[32m[06/29 17:18:36 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.627049, +0.627049, 1\n",
      "+0.783161, -0.783584, 0\n",
      "-0.172289, +0.171997, 1\n",
      "\u001b[32m[06/29 17:18:40 nl.defaults.trainer]: \u001b[0mEpoch 78-22, Train loss: 1.87915, validation loss: 1.91313, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/29 17:18:45 nl.defaults.trainer]: \u001b[0mEpoch 78-52, Train loss: 1.91876, validation loss: 1.96200, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/29 17:18:51 nl.defaults.trainer]: \u001b[0mEpoch 78-82, Train loss: 1.92464, validation loss: 1.92292, learning rate: [0.0037538410866905267]\n",
      "\u001b[32m[06/29 17:18:56 nl.defaults.trainer]: \u001b[0mEpoch 78-112, Train loss: 1.90729, validation loss: 1.84871, learning rate: [0.0037538410866905267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:19:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6320,  0.6320], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7901, -0.7905], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1751,  0.1748], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:19:00 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy (top1, top5): 56.94000, 92.93429, Validation accuracy: 56.15591, 92.67222\n",
      "\u001b[32m[06/29 17:19:00 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.631996, +0.631996, 1\n",
      "+0.790113, -0.790533, 0\n",
      "-0.175121, +0.174829, 1\n",
      "\u001b[32m[06/29 17:19:01 nl.defaults.trainer]: \u001b[0mEpoch 79-4, Train loss: 1.93377, validation loss: 1.89864, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/29 17:19:06 nl.defaults.trainer]: \u001b[0mEpoch 79-34, Train loss: 1.86696, validation loss: 1.89906, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/29 17:19:11 nl.defaults.trainer]: \u001b[0mEpoch 79-63, Train loss: 1.85538, validation loss: 1.90765, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/29 17:19:16 nl.defaults.trainer]: \u001b[0mEpoch 79-93, Train loss: 1.91351, validation loss: 1.93467, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/29 17:19:21 nl.defaults.trainer]: \u001b[0mEpoch 79-122, Train loss: 1.88541, validation loss: 1.90661, learning rate: [0.0035181398514917118]\n",
      "\u001b[32m[06/29 17:19:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6308,  0.6308], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.7916, -0.7920], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1715,  0.1712], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:19:23 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy (top1, top5): 56.66571, 92.85143, Validation accuracy: 56.31843, 92.56387\n",
      "\u001b[32m[06/29 17:19:23 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.630784, +0.630784, 1\n",
      "+0.791577, -0.791994, 0\n",
      "-0.171504, +0.171213, 1\n",
      "\u001b[32m[06/29 17:19:26 nl.defaults.trainer]: \u001b[0mEpoch 80-15, Train loss: 1.90043, validation loss: 1.89382, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/29 17:19:31 nl.defaults.trainer]: \u001b[0mEpoch 80-44, Train loss: 1.85042, validation loss: 1.88029, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/29 17:19:36 nl.defaults.trainer]: \u001b[0mEpoch 80-74, Train loss: 1.88220, validation loss: 1.87344, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/29 17:19:41 nl.defaults.trainer]: \u001b[0mEpoch 80-103, Train loss: 1.90240, validation loss: 1.92412, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/29 17:19:47 nl.defaults.trainer]: \u001b[0mEpoch 80-133, Train loss: 1.89368, validation loss: 1.86884, learning rate: [0.003291796067500629]\n",
      "\u001b[32m[06/29 17:19:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6429,  0.6429], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8052, -0.8056], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1816,  0.1813], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:19:47 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy (top1, top5): 57.56857, 92.72857, Validation accuracy: 56.44104, 92.85470\n",
      "\u001b[32m[06/29 17:19:47 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.642926, +0.642926, 1\n",
      "+0.805219, -0.805632, 0\n",
      "-0.181632, +0.181342, 1\n",
      "\u001b[32m[06/29 17:19:52 nl.defaults.trainer]: \u001b[0mEpoch 81-25, Train loss: 1.90916, validation loss: 1.94270, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/29 17:19:57 nl.defaults.trainer]: \u001b[0mEpoch 81-55, Train loss: 1.89263, validation loss: 1.91311, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/29 17:20:02 nl.defaults.trainer]: \u001b[0mEpoch 81-84, Train loss: 1.89458, validation loss: 1.88884, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/29 17:20:07 nl.defaults.trainer]: \u001b[0mEpoch 81-114, Train loss: 1.87568, validation loss: 1.95782, learning rate: [0.0030750331087052566]\n",
      "\u001b[32m[06/29 17:20:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6497,  0.6497], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8129, -0.8133], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1868,  0.1865], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:20:11 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy (top1, top5): 57.01429, 92.82857, Validation accuracy: 55.72536, 92.38994\n",
      "\u001b[32m[06/29 17:20:11 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.649744, +0.649744, 1\n",
      "+0.812878, -0.813287, 0\n",
      "-0.186823, +0.186534, 1\n",
      "\u001b[32m[06/29 17:20:12 nl.defaults.trainer]: \u001b[0mEpoch 82-6, Train loss: 1.89553, validation loss: 1.92131, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/29 17:20:17 nl.defaults.trainer]: \u001b[0mEpoch 82-35, Train loss: 1.88422, validation loss: 1.87218, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/29 17:20:22 nl.defaults.trainer]: \u001b[0mEpoch 82-65, Train loss: 1.86603, validation loss: 1.90144, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/29 17:20:27 nl.defaults.trainer]: \u001b[0mEpoch 82-95, Train loss: 1.89287, validation loss: 1.87432, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/29 17:20:33 nl.defaults.trainer]: \u001b[0mEpoch 82-125, Train loss: 1.84334, validation loss: 1.90415, learning rate: [0.002868064893975819]\n",
      "\u001b[32m[06/29 17:20:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6574,  0.6574], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8229, -0.8233], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1920,  0.1917], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:20:34 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy (top1, top5): 57.20857, 92.87714, Validation accuracy: 56.28707, 92.76916\n",
      "\u001b[32m[06/29 17:20:34 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.657430, +0.657430, 1\n",
      "+0.822935, -0.823340, 0\n",
      "-0.191999, +0.191710, 1\n",
      "\u001b[32m[06/29 17:20:38 nl.defaults.trainer]: \u001b[0mEpoch 83-17, Train loss: 1.90535, validation loss: 1.85733, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/29 17:20:43 nl.defaults.trainer]: \u001b[0mEpoch 83-47, Train loss: 1.87255, validation loss: 1.89765, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/29 17:20:48 nl.defaults.trainer]: \u001b[0mEpoch 83-76, Train loss: 1.93660, validation loss: 1.93680, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/29 17:20:53 nl.defaults.trainer]: \u001b[0mEpoch 83-106, Train loss: 1.86231, validation loss: 1.89857, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/29 17:20:58 nl.defaults.trainer]: \u001b[0mEpoch 83-135, Train loss: 1.84287, validation loss: 1.86981, learning rate: [0.0026710956759526724]\n",
      "\u001b[32m[06/29 17:20:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6626,  0.6626], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8300, -0.8304], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1950,  0.1947], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:20:58 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy (top1, top5): 57.30286, 92.86571, Validation accuracy: 56.09888, 92.74920\n",
      "\u001b[32m[06/29 17:20:58 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.662638, +0.662638, 1\n",
      "+0.830016, -0.830418, 0\n",
      "-0.194963, +0.194675, 1\n",
      "\u001b[32m[06/29 17:21:03 nl.defaults.trainer]: \u001b[0mEpoch 84-28, Train loss: 1.90025, validation loss: 1.89635, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/29 17:21:08 nl.defaults.trainer]: \u001b[0mEpoch 84-57, Train loss: 1.90664, validation loss: 1.85844, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/29 17:21:13 nl.defaults.trainer]: \u001b[0mEpoch 84-87, Train loss: 1.92368, validation loss: 1.87918, learning rate: [0.0024843198394736347]\n",
      "\u001b[32m[06/29 17:21:18 nl.defaults.trainer]: \u001b[0mEpoch 84-116, Train loss: 1.86092, validation loss: 1.90149, learning rate: [0.0024843198394736347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:21:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6708,  0.6708], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8405, -0.8409], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2005,  0.2002], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:21:22 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy (top1, top5): 57.38857, 92.92286, Validation accuracy: 56.37260, 92.80908\n",
      "\u001b[32m[06/29 17:21:22 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.670785, +0.670785, 1\n",
      "+0.840490, -0.840888, 0\n",
      "-0.200530, +0.200242, 1\n",
      "\u001b[32m[06/29 17:21:24 nl.defaults.trainer]: \u001b[0mEpoch 85-9, Train loss: 1.92585, validation loss: 1.92219, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/29 17:21:29 nl.defaults.trainer]: \u001b[0mEpoch 85-39, Train loss: 1.90481, validation loss: 1.85242, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/29 17:21:34 nl.defaults.trainer]: \u001b[0mEpoch 85-69, Train loss: 1.88715, validation loss: 1.88183, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/29 17:21:39 nl.defaults.trainer]: \u001b[0mEpoch 85-99, Train loss: 1.90492, validation loss: 1.92471, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/29 17:21:44 nl.defaults.trainer]: \u001b[0mEpoch 85-129, Train loss: 1.87212, validation loss: 1.89098, learning rate: [0.0023079217097395845]\n",
      "\u001b[32m[06/29 17:21:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6705,  0.6705], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8419, -0.8423], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1981,  0.1978], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:21:45 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy (top1, top5): 57.33714, 92.78857, Validation accuracy: 55.96772, 92.84044\n",
      "\u001b[32m[06/29 17:21:45 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.670532, +0.670532, 1\n",
      "+0.841947, -0.842341, 0\n",
      "-0.198083, +0.197796, 1\n",
      "\u001b[32m[06/29 17:21:49 nl.defaults.trainer]: \u001b[0mEpoch 86-22, Train loss: 1.91814, validation loss: 1.88977, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/29 17:21:55 nl.defaults.trainer]: \u001b[0mEpoch 86-52, Train loss: 1.86897, validation loss: 1.84657, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/29 17:22:00 nl.defaults.trainer]: \u001b[0mEpoch 86-82, Train loss: 1.90307, validation loss: 1.89821, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/29 17:22:05 nl.defaults.trainer]: \u001b[0mEpoch 86-112, Train loss: 1.88305, validation loss: 1.90051, learning rate: [0.002142075370407766]\n",
      "\u001b[32m[06/29 17:22:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6777,  0.6777], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8508, -0.8512], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2030,  0.2027], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:22:09 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy (top1, top5): 57.21714, 92.93714, Validation accuracy: 56.43533, 92.68077\n",
      "\u001b[32m[06/29 17:22:09 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.677712, +0.677712, 1\n",
      "+0.850773, -0.851164, 0\n",
      "-0.203009, +0.202723, 1\n",
      "\u001b[32m[06/29 17:22:10 nl.defaults.trainer]: \u001b[0mEpoch 87-4, Train loss: 1.86382, validation loss: 1.88462, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/29 17:22:15 nl.defaults.trainer]: \u001b[0mEpoch 87-34, Train loss: 1.84689, validation loss: 1.87651, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/29 17:22:20 nl.defaults.trainer]: \u001b[0mEpoch 87-63, Train loss: 1.84477, validation loss: 1.89303, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/29 17:22:25 nl.defaults.trainer]: \u001b[0mEpoch 87-93, Train loss: 1.93330, validation loss: 1.86786, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/29 17:22:30 nl.defaults.trainer]: \u001b[0mEpoch 87-122, Train loss: 1.91066, validation loss: 1.90337, learning rate: [0.0019869444917922276]\n",
      "\u001b[32m[06/29 17:22:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6809,  0.6809], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8558, -0.8561], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2039,  0.2036], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:22:33 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy (top1, top5): 57.40571, 93.00571, Validation accuracy: 56.30703, 92.66651\n",
      "\u001b[32m[06/29 17:22:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.680932, +0.680932, 1\n",
      "+0.855758, -0.856145, 0\n",
      "-0.203923, +0.203638, 1\n",
      "\u001b[32m[06/29 17:22:35 nl.defaults.trainer]: \u001b[0mEpoch 88-15, Train loss: 1.82370, validation loss: 1.86704, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/29 17:22:40 nl.defaults.trainer]: \u001b[0mEpoch 88-44, Train loss: 1.88709, validation loss: 1.89211, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/29 17:22:46 nl.defaults.trainer]: \u001b[0mEpoch 88-74, Train loss: 1.91296, validation loss: 1.91733, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/29 17:22:51 nl.defaults.trainer]: \u001b[0mEpoch 88-103, Train loss: 1.90018, validation loss: 1.87634, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/29 17:22:56 nl.defaults.trainer]: \u001b[0mEpoch 88-133, Train loss: 1.85366, validation loss: 1.90309, learning rate: [0.0018426821693409826]\n",
      "\u001b[32m[06/29 17:22:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6816,  0.6816], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8585, -0.8588], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2022,  0.2019], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:22:56 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy (top1, top5): 57.49143, 93.10857, Validation accuracy: 56.33554, 92.59523\n",
      "\u001b[32m[06/29 17:22:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.681605, +0.681605, 1\n",
      "+0.858461, -0.858844, 0\n",
      "-0.202187, +0.201903, 1\n",
      "\u001b[32m[06/29 17:23:01 nl.defaults.trainer]: \u001b[0mEpoch 89-25, Train loss: 1.87068, validation loss: 1.93675, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/29 17:23:06 nl.defaults.trainer]: \u001b[0mEpoch 89-55, Train loss: 1.86691, validation loss: 1.90122, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/29 17:23:11 nl.defaults.trainer]: \u001b[0mEpoch 89-84, Train loss: 1.88323, validation loss: 1.92573, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/29 17:23:16 nl.defaults.trainer]: \u001b[0mEpoch 89-114, Train loss: 1.94615, validation loss: 1.89384, learning rate: [0.0017094307725492937]\n",
      "\u001b[32m[06/29 17:23:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6849,  0.6849], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8638, -0.8641], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.2030,  0.2027], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:23:20 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy (top1, top5): 57.67143, 93.06857, Validation accuracy: 56.57219, 93.00297\n",
      "\u001b[32m[06/29 17:23:20 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.684904, +0.684904, 1\n",
      "+0.863765, -0.864144, 0\n",
      "-0.203024, +0.202741, 1\n",
      "\u001b[32m[06/29 17:23:21 nl.defaults.trainer]: \u001b[0mEpoch 90-6, Train loss: 1.82610, validation loss: 1.89680, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/29 17:23:26 nl.defaults.trainer]: \u001b[0mEpoch 90-35, Train loss: 1.89188, validation loss: 1.90665, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/29 17:23:31 nl.defaults.trainer]: \u001b[0mEpoch 90-65, Train loss: 1.93149, validation loss: 1.88048, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/29 17:23:36 nl.defaults.trainer]: \u001b[0mEpoch 90-95, Train loss: 1.86735, validation loss: 1.97518, learning rate: [0.0015873218044581568]\n",
      "\u001b[32m[06/29 17:23:42 nl.defaults.trainer]: \u001b[0mEpoch 90-125, Train loss: 1.88924, validation loss: 1.96405, learning rate: [0.0015873218044581568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:23:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6836,  0.6836], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8652, -0.8656], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1989,  0.1986], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:23:43 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy (top1, top5): 57.70571, 93.03143, Validation accuracy: 56.75182, 92.67222\n",
      "\u001b[32m[06/29 17:23:43 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.683575, +0.683575, 1\n",
      "+0.865191, -0.865567, 0\n",
      "-0.198873, +0.198591, 1\n",
      "\u001b[32m[06/29 17:23:47 nl.defaults.trainer]: \u001b[0mEpoch 91-17, Train loss: 1.90843, validation loss: 1.89098, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/29 17:23:52 nl.defaults.trainer]: \u001b[0mEpoch 91-47, Train loss: 1.89648, validation loss: 1.93178, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/29 17:23:57 nl.defaults.trainer]: \u001b[0mEpoch 91-76, Train loss: 1.83031, validation loss: 1.85598, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/29 17:24:02 nl.defaults.trainer]: \u001b[0mEpoch 91-106, Train loss: 1.89003, validation loss: 1.91648, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/29 17:24:07 nl.defaults.trainer]: \u001b[0mEpoch 91-135, Train loss: 1.92624, validation loss: 1.91073, learning rate: [0.0014764757718766838]\n",
      "\u001b[32m[06/29 17:24:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6814,  0.6814], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8649, -0.8653], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1943,  0.1940], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:24:07 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy (top1, top5): 57.38571, 93.06000, Validation accuracy: 56.23574, 92.74350\n",
      "\u001b[32m[06/29 17:24:07 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.681367, +0.681367, 1\n",
      "+0.864931, -0.865304, 0\n",
      "-0.194278, +0.193996, 1\n",
      "\u001b[32m[06/29 17:24:12 nl.defaults.trainer]: \u001b[0mEpoch 92-28, Train loss: 1.85772, validation loss: 1.91825, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/29 17:24:17 nl.defaults.trainer]: \u001b[0mEpoch 92-57, Train loss: 1.91624, validation loss: 1.89761, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/29 17:24:22 nl.defaults.trainer]: \u001b[0mEpoch 92-87, Train loss: 1.85792, validation loss: 1.84691, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/29 17:24:27 nl.defaults.trainer]: \u001b[0mEpoch 92-116, Train loss: 1.85053, validation loss: 1.91017, learning rate: [0.0013770020664564278]\n",
      "\u001b[32m[06/29 17:24:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6837,  0.6837], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8688, -0.8692], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1945,  0.1942], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:24:31 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy (top1, top5): 57.82571, 92.90000, Validation accuracy: 56.30132, 92.83189\n",
      "\u001b[32m[06/29 17:24:31 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.683734, +0.683734, 1\n",
      "+0.868781, -0.869150, 0\n",
      "-0.194486, +0.194205, 1\n",
      "\u001b[32m[06/29 17:24:33 nl.defaults.trainer]: \u001b[0mEpoch 93-9, Train loss: 1.93756, validation loss: 1.89117, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/29 17:24:38 nl.defaults.trainer]: \u001b[0mEpoch 93-39, Train loss: 1.85651, validation loss: 1.88864, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/29 17:24:43 nl.defaults.trainer]: \u001b[0mEpoch 93-69, Train loss: 1.90244, validation loss: 1.89957, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/29 17:24:48 nl.defaults.trainer]: \u001b[0mEpoch 93-99, Train loss: 1.85227, validation loss: 1.90326, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/29 17:24:53 nl.defaults.trainer]: \u001b[0mEpoch 93-129, Train loss: 1.84668, validation loss: 1.86670, learning rate: [0.0012889988567350316]\n",
      "\u001b[32m[06/29 17:24:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6816,  0.6816], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8682, -0.8685], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1902,  0.1899], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:24:54 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy (top1, top5): 57.38571, 92.99714, Validation accuracy: 56.13310, 92.57812\n",
      "\u001b[32m[06/29 17:24:54 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.681612, +0.681612, 1\n",
      "+0.868152, -0.868517, 0\n",
      "-0.190216, +0.189935, 1\n",
      "\u001b[32m[06/29 17:24:58 nl.defaults.trainer]: \u001b[0mEpoch 94-22, Train loss: 1.90943, validation loss: 1.83985, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/29 17:25:04 nl.defaults.trainer]: \u001b[0mEpoch 94-52, Train loss: 1.88766, validation loss: 1.87291, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/29 17:25:09 nl.defaults.trainer]: \u001b[0mEpoch 94-82, Train loss: 1.91160, validation loss: 1.85197, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/29 17:25:14 nl.defaults.trainer]: \u001b[0mEpoch 94-111, Train loss: 1.87212, validation loss: 1.87486, learning rate: [0.001212552991255735]\n",
      "\u001b[32m[06/29 17:25:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6826,  0.6826], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8709, -0.8712], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1889,  0.1886], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:25:18 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy (top1, top5): 57.41143, 93.04286, Validation accuracy: 56.45529, 92.82333\n",
      "\u001b[32m[06/29 17:25:18 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.682589, +0.682589, 1\n",
      "+0.870877, -0.871238, 0\n",
      "-0.188902, +0.188622, 1\n",
      "\u001b[32m[06/29 17:25:19 nl.defaults.trainer]: \u001b[0mEpoch 95-3, Train loss: 1.95726, validation loss: 1.88571, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/29 17:25:24 nl.defaults.trainer]: \u001b[0mEpoch 95-32, Train loss: 1.88837, validation loss: 1.88630, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/29 17:25:29 nl.defaults.trainer]: \u001b[0mEpoch 95-61, Train loss: 1.89703, validation loss: 1.86384, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/29 17:25:34 nl.defaults.trainer]: \u001b[0mEpoch 95-90, Train loss: 1.88325, validation loss: 1.92174, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/29 17:25:39 nl.defaults.trainer]: \u001b[0mEpoch 95-119, Train loss: 1.87508, validation loss: 1.85540, learning rate: [0.001147739912858348]\n",
      "\u001b[32m[06/29 17:25:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6834,  0.6834], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8733, -0.8737], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1874,  0.1872], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:25:42 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy (top1, top5): 57.31714, 93.05429, Validation accuracy: 56.82881, 92.67792\n",
      "\u001b[32m[06/29 17:25:42 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.683388, +0.683388, 1\n",
      "+0.873295, -0.873653, 0\n",
      "-0.187448, +0.187169, 1\n",
      "\u001b[32m[06/29 17:25:44 nl.defaults.trainer]: \u001b[0mEpoch 96-11, Train loss: 1.91485, validation loss: 1.91073, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/29 17:25:49 nl.defaults.trainer]: \u001b[0mEpoch 96-40, Train loss: 1.91297, validation loss: 1.91318, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/29 17:25:54 nl.defaults.trainer]: \u001b[0mEpoch 96-69, Train loss: 1.89069, validation loss: 1.91226, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/29 17:26:00 nl.defaults.trainer]: \u001b[0mEpoch 96-99, Train loss: 1.87225, validation loss: 1.89125, learning rate: [0.0010946235842262668]\n",
      "\u001b[32m[06/29 17:26:05 nl.defaults.trainer]: \u001b[0mEpoch 96-129, Train loss: 1.91878, validation loss: 1.88493, learning rate: [0.0010946235842262668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 17:26:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6945,  0.6945], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8845, -0.8849], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1971,  0.1968], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:26:06 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy (top1, top5): 57.56571, 93.01143, Validation accuracy: 56.26426, 92.57242\n",
      "\u001b[32m[06/29 17:26:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.694516, +0.694516, 1\n",
      "+0.884497, -0.884851, 0\n",
      "-0.197071, +0.196793, 1\n",
      "\u001b[32m[06/29 17:26:10 nl.defaults.trainer]: \u001b[0mEpoch 97-22, Train loss: 1.83256, validation loss: 1.86518, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/29 17:26:15 nl.defaults.trainer]: \u001b[0mEpoch 97-52, Train loss: 1.86873, validation loss: 1.89520, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/29 17:26:20 nl.defaults.trainer]: \u001b[0mEpoch 97-82, Train loss: 1.82809, validation loss: 1.91200, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/29 17:26:26 nl.defaults.trainer]: \u001b[0mEpoch 97-112, Train loss: 1.93172, validation loss: 1.89747, learning rate: [0.00105325642476304]\n",
      "\u001b[32m[06/29 17:26:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6909,  0.6909], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8828, -0.8832], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1910,  0.1908], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:26:30 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy (top1, top5): 57.64286, 93.10286, Validation accuracy: 56.56364, 92.50399\n",
      "\u001b[32m[06/29 17:26:30 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.690910, +0.690910, 1\n",
      "+0.882807, -0.883157, 0\n",
      "-0.191039, +0.190762, 1\n",
      "\u001b[32m[06/29 17:26:31 nl.defaults.trainer]: \u001b[0mEpoch 98-4, Train loss: 1.91555, validation loss: 1.94770, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/29 17:26:36 nl.defaults.trainer]: \u001b[0mEpoch 98-34, Train loss: 1.89910, validation loss: 1.90787, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/29 17:26:41 nl.defaults.trainer]: \u001b[0mEpoch 98-63, Train loss: 1.89869, validation loss: 1.79859, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/29 17:26:46 nl.defaults.trainer]: \u001b[0mEpoch 98-93, Train loss: 1.92322, validation loss: 1.93395, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/29 17:26:51 nl.defaults.trainer]: \u001b[0mEpoch 98-122, Train loss: 1.86286, validation loss: 1.87582, learning rate: [0.0010236792588607414]\n",
      "\u001b[32m[06/29 17:26:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6971,  0.6971], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8901, -0.8904], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1952,  0.1949], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:26:53 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy (top1, top5): 57.64571, 93.00857, Validation accuracy: 56.65203, 92.43271\n",
      "\u001b[32m[06/29 17:26:53 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n",
      "-0.697096, +0.697096, 1\n",
      "+0.890064, -0.890411, 0\n",
      "-0.195203, +0.194927, 1\n",
      "\u001b[32m[06/29 17:26:56 nl.defaults.trainer]: \u001b[0mEpoch 99-15, Train loss: 1.86300, validation loss: 1.92735, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/29 17:27:01 nl.defaults.trainer]: \u001b[0mEpoch 99-45, Train loss: 1.86588, validation loss: 1.87650, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/29 17:27:07 nl.defaults.trainer]: \u001b[0mEpoch 99-75, Train loss: 1.90655, validation loss: 1.86657, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/29 17:27:12 nl.defaults.trainer]: \u001b[0mEpoch 99-105, Train loss: 1.88358, validation loss: 1.85408, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/29 17:27:17 nl.defaults.trainer]: \u001b[0mEpoch 99-135, Train loss: 1.88099, validation loss: 1.87180, learning rate: [0.0010059212756112208]\n",
      "\u001b[32m[06/29 17:27:17 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n",
      "tensor([-0.6977,  0.6977], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.8930, -0.8933], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.1931,  0.1929], device='cuda:0', requires_grad=True)]\n",
      "\u001b[32m[06/29 17:27:17 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy (top1, top5): 57.51429, 93.16000, Validation accuracy: 56.64062, 92.86040\n",
      "\u001b[32m[06/29 17:27:17 nl.defaults.trainer]: \u001b[0mTraining finished\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(optimizer, config)\n",
    "trainer.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/29 18:09:42 nl.defaults.trainer]: \u001b[0mStart one-shot evaluation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[06/29 18:09:48 nl.defaults.trainer]: \u001b[0mEvaluation finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56.85999998982747"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate_oneshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NASLib Project",
   "language": "python",
   "name": "naslib_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
